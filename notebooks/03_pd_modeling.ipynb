{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Probability of Default (PD) Modeling\n",
    "\n",
    "**Objective**: Build, tune, and evaluate PD models for Lending Club loans.\n",
    "\n",
    "**Pipeline**:\n",
    "1. Load feature-engineered data (from NB02)\n",
    "2. Logistic Regression baseline\n",
    "3. CatBoost default configuration\n",
    "4. Optuna hyperparameter optimization (50 trials)\n",
    "5. SHAP explainability\n",
    "6. Probability calibration (Isotonic, Platt)\n",
    "7. Full evaluation: AUC, KS, Gini, Brier, ECE\n",
    "\n",
    "**Validation**: Out-of-Time split — Train (<2018) → Test (2018+)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import pickle\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    brier_score_loss,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from src.evaluation.metrics import classification_metrics\n",
    "from src.models.calibration import (\n",
    "    expected_calibration_error,\n",
    ")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 6), \"figure.dpi\": 100})\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Data & Define Feature Sets\n",
    "\n",
    "We load the feature-engineered datasets from NB02. If `train_fe.parquet` doesn't exist yet, we fall back to running the feature engineering pipeline inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to load pre-engineered data, fallback to raw + inline FE\n",
    "if (DATA_DIR / \"train_fe.parquet\").exists():\n",
    "    train = pd.read_parquet(DATA_DIR / \"train_fe.parquet\")\n",
    "    cal = pd.read_parquet(DATA_DIR / \"calibration_fe.parquet\")\n",
    "    test = pd.read_parquet(DATA_DIR / \"test_fe.parquet\")\n",
    "    print(\"Loaded feature-engineered datasets from NB02.\")\n",
    "\n",
    "    # Load feature config\n",
    "    with open(DATA_DIR / \"feature_config.pkl\", \"rb\") as f:\n",
    "        feat_config = pickle.load(f)\n",
    "    CATBOOST_FEATURES = feat_config[\"CATBOOST_FEATURES\"]\n",
    "    LOGREG_FEATURES = feat_config[\"LOGREG_FEATURES\"]\n",
    "    CAT_FEATURES = feat_config[\"CATEGORICAL_FEATURES\"].copy()\n",
    "\n",
    "    # Interaction features (e.g. int_rate_bucket__grade) are string columns\n",
    "    # that must also be declared as categorical for CatBoost\n",
    "    if \"INTERACTION_FEATURES\" in feat_config:\n",
    "        for f in feat_config[\"INTERACTION_FEATURES\"]:\n",
    "            if f in CATBOOST_FEATURES and f not in CAT_FEATURES:\n",
    "                CAT_FEATURES.append(f)\n",
    "else:\n",
    "    print(\"Feature-engineered files not found. Running NB02 pipeline inline...\")\n",
    "    # Load raw and apply feature engineering\n",
    "    train = pd.read_parquet(DATA_DIR / \"train.parquet\")\n",
    "    cal = pd.read_parquet(DATA_DIR / \"calibration.parquet\")\n",
    "    test = pd.read_parquet(DATA_DIR / \"test.parquet\")\n",
    "\n",
    "    # Minimal cleaning\n",
    "    for df_ref in [train, cal, test]:\n",
    "        if df_ref[\"int_rate\"].dtype == object:\n",
    "            df_ref[\"int_rate\"] = (\n",
    "                df_ref[\"int_rate\"].astype(str).str.rstrip(\"%\").pipe(pd.to_numeric, errors=\"coerce\")\n",
    "            )\n",
    "        if df_ref[\"term\"].dtype == object:\n",
    "            df_ref[\"term\"] = (\n",
    "                df_ref[\"term\"]\n",
    "                .astype(str)\n",
    "                .str.extract(r\"(\\d+)\")[0]\n",
    "                .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "            )\n",
    "        if \"revol_util\" in df_ref.columns and df_ref[\"revol_util\"].dtype == object:\n",
    "            df_ref[\"revol_util\"] = (\n",
    "                df_ref[\"revol_util\"]\n",
    "                .astype(str)\n",
    "                .str.rstrip(\"%\")\n",
    "                .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "            )\n",
    "\n",
    "    # Minimal features\n",
    "    from src.features.feature_engineering import run_feature_pipeline\n",
    "\n",
    "    train = run_feature_pipeline(train)\n",
    "    cal = run_feature_pipeline(cal)\n",
    "    test = run_feature_pipeline(test)\n",
    "\n",
    "    CATBOOST_FEATURES = [\n",
    "        c\n",
    "        for c in [\n",
    "            \"loan_amnt\",\n",
    "            \"int_rate\",\n",
    "            \"installment\",\n",
    "            \"annual_inc\",\n",
    "            \"dti\",\n",
    "            \"loan_to_income\",\n",
    "            \"rev_utilization\",\n",
    "            \"open_acc\",\n",
    "            \"total_acc\",\n",
    "            \"revol_bal\",\n",
    "            \"pub_rec\",\n",
    "            \"grade\",\n",
    "            \"sub_grade\",\n",
    "            \"home_ownership\",\n",
    "            \"purpose\",\n",
    "            \"verification_status\",\n",
    "            \"term\",\n",
    "            \"int_rate_bucket\",\n",
    "            \"int_rate_bucket__grade\",\n",
    "        ]\n",
    "        if c in train.columns\n",
    "    ]\n",
    "    CAT_FEATURES = [\n",
    "        c\n",
    "        for c in [\n",
    "            \"grade\",\n",
    "            \"sub_grade\",\n",
    "            \"home_ownership\",\n",
    "            \"purpose\",\n",
    "            \"verification_status\",\n",
    "            \"int_rate_bucket\",\n",
    "            \"int_rate_bucket__grade\",\n",
    "        ]\n",
    "        if c in train.columns\n",
    "    ]\n",
    "    LOGREG_FEATURES = [c for c in CATBOOST_FEATURES if c not in CAT_FEATURES]\n",
    "\n",
    "TARGET = \"default_flag\"\n",
    "\n",
    "# Filter to available features\n",
    "CATBOOST_FEATURES = [c for c in CATBOOST_FEATURES if c in train.columns]\n",
    "LOGREG_FEATURES = [c for c in LOGREG_FEATURES if c in train.columns]\n",
    "CAT_FEATURES = [c for c in CAT_FEATURES if c in train.columns]\n",
    "\n",
    "# Convert categoricals to string for CatBoost compatibility\n",
    "# (handles category dtype, int64 term, and object columns uniformly)\n",
    "for df_ref in [train, cal, test]:\n",
    "    for c in CAT_FEATURES:\n",
    "        if c in df_ref.columns:\n",
    "            df_ref[c] = df_ref[c].astype(str)\n",
    "\n",
    "print(f\"\\nTrain:       {train.shape[0]:>10,} rows\")\n",
    "print(f\"Calibration: {cal.shape[0]:>10,} rows\")\n",
    "print(f\"Test:        {test.shape[0]:>10,} rows\")\n",
    "print(f\"\\nCatBoost features: {len(CATBOOST_FEATURES)}\")\n",
    "print(f\"LogReg features:   {len(LOGREG_FEATURES)}\")\n",
    "print(f\"Categorical:       {len(CAT_FEATURES)}: {CAT_FEATURES}\")\n",
    "print(\"\\nDefault rates:\")\n",
    "print(f\"  Train: {train[TARGET].mean():.2%}\")\n",
    "print(f\"  Cal:   {cal[TARGET].mean():.2%}\")\n",
    "print(f\"  Test:  {test[TARGET].mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X, y splits\n",
    "X_train = train[CATBOOST_FEATURES]\n",
    "y_train = train[TARGET]\n",
    "X_cal = cal[CATBOOST_FEATURES]\n",
    "y_cal = cal[TARGET]\n",
    "X_test = test[CATBOOST_FEATURES]\n",
    "y_test = test[TARGET]\n",
    "\n",
    "# For LogReg (numeric only, no NaN)\n",
    "X_train_lr = train[LOGREG_FEATURES].fillna(0)\n",
    "X_cal_lr = cal[LOGREG_FEATURES].fillna(0)\n",
    "X_test_lr = test[LOGREG_FEATURES].fillna(0)\n",
    "\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_test:  {X_test.shape}\")\n",
    "print(f\"X_train_lr: {X_train_lr.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Logistic Regression Baseline\n",
    "\n",
    "Simple L2-regularized LogReg with standardized features. This serves as the baseline to beat with CatBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogReg pipeline with standardization\n",
    "lr_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LogisticRegression(\n",
    "                penalty=\"l2\",\n",
    "                C=1.0,\n",
    "                max_iter=1000,\n",
    "                solver=\"lbfgs\",\n",
    "                class_weight=\"balanced\",\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "lr_pipeline.fit(X_train_lr, y_train)\n",
    "lr_train_time = time.time() - t0\n",
    "\n",
    "# Predict on test\n",
    "y_prob_lr = lr_pipeline.predict_proba(X_test_lr)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "lr_metrics = classification_metrics(y_test.values, y_prob_lr)\n",
    "lr_metrics[\"train_time_s\"] = lr_train_time\n",
    "\n",
    "print(\"Logistic Regression Results (Test Set):\")\n",
    "print(f\"  AUC-ROC:     {lr_metrics['auc_roc']:.4f}\")\n",
    "print(f\"  Gini:        {lr_metrics['gini']:.4f}\")\n",
    "print(f\"  KS:          {lr_metrics['ks_statistic']:.4f}\")\n",
    "print(f\"  Brier:       {lr_metrics['brier_score']:.4f}\")\n",
    "print(f\"  ECE:         {lr_metrics['ece']:.4f}\")\n",
    "print(f\"  Train time:  {lr_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogReg coefficients\n",
    "coef_df = pd.DataFrame(\n",
    "    {\n",
    "        \"feature\": LOGREG_FEATURES,\n",
    "        \"coefficient\": lr_pipeline.named_steps[\"clf\"].coef_[0],\n",
    "    }\n",
    ").assign(abs_coef=lambda x: x[\"coefficient\"].abs())\n",
    "coef_df = coef_df.sort_values(\"abs_coef\", ascending=False)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(6, len(coef_df) * 0.3)))\n",
    "colors = [\"#e74c3c\" if c > 0 else \"#3498db\" for c in coef_df[\"coefficient\"]]\n",
    "ax.barh(coef_df[\"feature\"][::-1], coef_df[\"coefficient\"][::-1], color=colors[::-1])\n",
    "ax.axvline(0, color=\"#2c3e50\", linewidth=1)\n",
    "ax.set_xlabel(\"Coefficient (standardized)\")\n",
    "ax.set_title(\"Logistic Regression — Feature Coefficients\", fontweight=\"bold\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. CatBoost — Default Configuration\n",
    "\n",
    "Train CatBoost with sensible defaults before hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost default model\n",
    "cb_params_default = {\n",
    "    \"iterations\": 1000,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"depth\": 6,\n",
    "    \"l2_leaf_reg\": 3,\n",
    "    \"auto_class_weights\": \"Balanced\",\n",
    "    \"eval_metric\": \"Logloss\",\n",
    "    \"random_seed\": 42,\n",
    "    \"verbose\": 200,\n",
    "    \"early_stopping_rounds\": 50,\n",
    "}\n",
    "\n",
    "train_pool = Pool(X_train, y_train, cat_features=CAT_FEATURES)\n",
    "cal_pool = Pool(X_cal, y_cal, cat_features=CAT_FEATURES)\n",
    "test_pool = Pool(X_test, y_test, cat_features=CAT_FEATURES)\n",
    "\n",
    "t0 = time.time()\n",
    "cb_default = CatBoostClassifier(**cb_params_default)\n",
    "cb_default.fit(train_pool, eval_set=cal_pool, use_best_model=True)\n",
    "cb_train_time = time.time() - t0\n",
    "\n",
    "# Predict\n",
    "y_prob_cb = cb_default.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "cb_metrics = classification_metrics(y_test.values, y_prob_cb)\n",
    "cb_metrics[\"train_time_s\"] = cb_train_time\n",
    "cb_metrics[\"best_iteration\"] = cb_default.get_best_iteration()\n",
    "\n",
    "print(\"\\nCatBoost Default Results (Test Set):\")\n",
    "print(f\"  AUC-ROC:        {cb_metrics['auc_roc']:.4f}\")\n",
    "print(f\"  Gini:           {cb_metrics['gini']:.4f}\")\n",
    "print(f\"  KS:             {cb_metrics['ks_statistic']:.4f}\")\n",
    "print(f\"  Brier:          {cb_metrics['brier_score']:.4f}\")\n",
    "print(f\"  ECE:            {cb_metrics['ece']:.4f}\")\n",
    "print(f\"  Best iteration: {cb_metrics['best_iteration']}\")\n",
    "print(f\"  Train time:     {cb_train_time:.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CatBoost built-in feature importance\n",
    "fi = cb_default.get_feature_importance(type=\"PredictionValuesChange\")\n",
    "fi_df = pd.DataFrame({\"feature\": CATBOOST_FEATURES, \"importance\": fi}).sort_values(\n",
    "    \"importance\", ascending=False\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(6, len(fi_df) * 0.3)))\n",
    "bars = ax.barh(fi_df[\"feature\"][::-1], fi_df[\"importance\"][::-1], color=\"#3498db\")\n",
    "ax.set_xlabel(\"Feature Importance (PredictionValuesChange)\")\n",
    "ax.set_title(\"CatBoost Default — Feature Importance\", fontweight=\"bold\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 10 features:\")\n",
    "print(fi_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Optuna Hyperparameter Optimization\n",
    "\n",
    "Search over CatBoost hyperparameters using the calibration set as validation.\n",
    "**30 trials** with early stopping and Optuna's TPE sampler for efficient exploration.\n",
    "\n",
    "CatBoost bootstrap rules:\n",
    "- `Bayesian` bootstrap → uses `bagging_temperature` (no `subsample`)\n",
    "- `Bernoulli`/`MVS` bootstrap → uses `subsample` (no `bagging_temperature`)\n",
    "\n",
    "We handle this with **conditional hyperparameters** in the Optuna objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Bootstrap type determines which sampling params are valid\n",
    "    bootstrap_type = trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"])\n",
    "\n",
    "    params = {\n",
    "        \"iterations\": 500,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3, log=True),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 4, 8),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-3, 10, log=True),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 10, 100),\n",
    "        \"border_count\": trial.suggest_int(\"border_count\", 32, 128),\n",
    "        \"bootstrap_type\": bootstrap_type,\n",
    "        \"auto_class_weights\": \"Balanced\",\n",
    "        \"eval_metric\": \"Logloss\",\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": 0,\n",
    "        \"early_stopping_rounds\": 30,\n",
    "    }\n",
    "\n",
    "    # Conditional params based on bootstrap type\n",
    "    if bootstrap_type == \"Bayesian\":\n",
    "        params[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0.0, 5.0)\n",
    "    else:\n",
    "        params[\"subsample\"] = trial.suggest_float(\"subsample\", 0.6, 1.0)\n",
    "\n",
    "    model = CatBoostClassifier(**params)\n",
    "    model.fit(train_pool, eval_set=cal_pool, use_best_model=True)\n",
    "    y_prob = model.predict_proba(X_cal)[:, 1]\n",
    "    return roc_auc_score(y_cal, y_prob)\n",
    "\n",
    "\n",
    "print(\"Starting Optuna HPO (30 trials, CPU)...\")\n",
    "t0 = time.time()\n",
    "study = optuna.create_study(direction=\"maximize\", study_name=\"pd_catboost\")\n",
    "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
    "hpo_time = time.time() - t0\n",
    "\n",
    "print(f\"\\nOptuna completed in {hpo_time / 60:.1f} min\")\n",
    "print(f\"Best AUC (validation): {study.best_value:.4f}\")\n",
    "print(\"Best params:\")\n",
    "for k, v in study.best_params.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna optimization history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Trial values\n",
    "trial_values = [t.value for t in study.trials if t.value is not None]\n",
    "axes[0].plot(range(len(trial_values)), trial_values, \"o-\", alpha=0.6, markersize=4, color=\"#3498db\")\n",
    "best_so_far = pd.Series(trial_values).cummax()\n",
    "axes[0].plot(\n",
    "    range(len(best_so_far)), best_so_far, \"-\", color=\"#e74c3c\", linewidth=2, label=\"Best so far\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Trial\")\n",
    "axes[0].set_ylabel(\"Validation AUC\")\n",
    "axes[0].set_title(\"Optimization History\", fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Parameter importance\n",
    "try:\n",
    "    param_imp = optuna.importance.get_param_importances(study)\n",
    "    axes[1].barh(list(param_imp.keys())[::-1], list(param_imp.values())[::-1], color=\"#9b59b6\")\n",
    "    axes[1].set_xlabel(\"Importance\")\n",
    "    axes[1].set_title(\"Hyperparameter Importance\", fontweight=\"bold\")\n",
    "except Exception:\n",
    "    axes[1].text(\n",
    "        0.5, 0.5, \"Parameter importance\\nnot available\", ha=\"center\", va=\"center\", fontsize=12\n",
    "    )\n",
    "    axes[1].set_title(\"Hyperparameter Importance\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with best params (CPU, more iterations)\n",
    "best_params = study.best_params.copy()\n",
    "best_params.update(\n",
    "    {\n",
    "        \"iterations\": 2000,\n",
    "        \"auto_class_weights\": \"Balanced\",\n",
    "        \"eval_metric\": \"Logloss\",\n",
    "        \"random_seed\": 42,\n",
    "        \"verbose\": 200,\n",
    "        \"early_stopping_rounds\": 100,\n",
    "    }\n",
    ")\n",
    "\n",
    "t0 = time.time()\n",
    "cb_best = CatBoostClassifier(**best_params)\n",
    "cb_best.fit(train_pool, eval_set=cal_pool, use_best_model=True)\n",
    "cb_best_time = time.time() - t0\n",
    "\n",
    "# Predict on test\n",
    "y_prob_cb_best = cb_best.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Metrics\n",
    "cb_best_metrics = classification_metrics(y_test.values, y_prob_cb_best)\n",
    "cb_best_metrics[\"train_time_s\"] = cb_best_time\n",
    "cb_best_metrics[\"best_iteration\"] = cb_best.get_best_iteration()\n",
    "\n",
    "print(\"CatBoost Tuned Results (Test Set):\")\n",
    "print(f\"  AUC-ROC:        {cb_best_metrics['auc_roc']:.4f}\")\n",
    "print(f\"  Gini:           {cb_best_metrics['gini']:.4f}\")\n",
    "print(f\"  KS:             {cb_best_metrics['ks_statistic']:.4f}\")\n",
    "print(f\"  Brier:          {cb_best_metrics['brier_score']:.4f}\")\n",
    "print(f\"  ECE:            {cb_best_metrics['ece']:.4f}\")\n",
    "print(f\"  Best iteration: {cb_best_metrics['best_iteration']}\")\n",
    "print(f\"  Train time:     {cb_best_time:.1f}s\")\n",
    "\n",
    "# Improvement over default\n",
    "delta_auc = cb_best_metrics[\"auc_roc\"] - cb_metrics[\"auc_roc\"]\n",
    "print(f\"\\n  AUC improvement: {delta_auc:+.4f} ({delta_auc / cb_metrics['auc_roc'] * 100:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. SHAP Explainability\n",
    "\n",
    "Global and local feature importance using SHAP values. CatBoost has native SHAP support via `get_feature_importance(type='ShapValues')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Compute SHAP values using CatBoost's native method\n",
    "# Use a sample for performance (SHAP on 1.3M rows is slow)\n",
    "shap_sample_size = min(10_000, len(X_test))\n",
    "X_shap = X_test.sample(shap_sample_size, random_state=42)\n",
    "shap_pool = Pool(X_shap, cat_features=CAT_FEATURES)\n",
    "\n",
    "t0 = time.time()\n",
    "shap_values_raw = cb_best.get_feature_importance(type=\"ShapValues\", data=shap_pool)\n",
    "shap_time = time.time() - t0\n",
    "\n",
    "# CatBoost ShapValues returns (n_samples, n_features+1) — last col is base value\n",
    "shap_values = shap_values_raw[:, :-1]\n",
    "base_value = shap_values_raw[0, -1]\n",
    "\n",
    "print(f\"SHAP computed on {shap_sample_size:,} samples in {shap_time:.1f}s\")\n",
    "print(f\"SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"Base value (log-odds): {base_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (beeswarm)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Beeswarm — need to handle categoricals by converting to numeric for display\n",
    "X_shap_numeric = X_shap.copy()\n",
    "for c in CAT_FEATURES:\n",
    "    if c in X_shap_numeric.columns:\n",
    "        X_shap_numeric[c] = pd.Categorical(X_shap_numeric[c]).codes\n",
    "\n",
    "plt.sca(axes[0])\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_shap_numeric,\n",
    "    feature_names=CATBOOST_FEATURES,\n",
    "    show=False,\n",
    "    max_display=20,\n",
    "    plot_size=None,\n",
    ")\n",
    "axes[0].set_title(\"SHAP Summary (Beeswarm)\", fontweight=\"bold\", fontsize=13)\n",
    "\n",
    "# Bar plot — mean |SHAP|\n",
    "plt.sca(axes[1])\n",
    "shap.summary_plot(\n",
    "    shap_values,\n",
    "    X_shap_numeric,\n",
    "    feature_names=CATBOOST_FEATURES,\n",
    "    plot_type=\"bar\",\n",
    "    show=False,\n",
    "    max_display=20,\n",
    "    plot_size=None,\n",
    ")\n",
    "axes[1].set_title(\"SHAP Feature Importance (Mean |SHAP|)\", fontweight=\"bold\", fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Dependence plots for top 4 features\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "top_features_idx = np.argsort(mean_abs_shap)[::-1][:4]\n",
    "top_feature_names = [CATBOOST_FEATURES[i] for i in top_features_idx]\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 5))\n",
    "for idx, (feat_idx, feat_name) in enumerate(zip(top_features_idx, top_feature_names, strict=False)):\n",
    "    if feat_name in CAT_FEATURES:\n",
    "        # For categorical: box plot of SHAP by category\n",
    "        categories = X_shap[feat_name].values\n",
    "        unique_cats = sorted(set(categories))[:10]  # limit to 10\n",
    "        cat_shap = {c: shap_values[categories == c, feat_idx] for c in unique_cats}\n",
    "        axes[idx].boxplot([cat_shap[c] for c in unique_cats], labels=unique_cats)\n",
    "        axes[idx].set_xlabel(feat_name)\n",
    "        axes[idx].set_ylabel(\"SHAP value\")\n",
    "        axes[idx].tick_params(axis=\"x\", rotation=45)\n",
    "    else:\n",
    "        # For numeric: scatter\n",
    "        axes[idx].scatter(\n",
    "            X_shap_numeric[feat_name].values,\n",
    "            shap_values[:, feat_idx],\n",
    "            alpha=0.3,\n",
    "            s=3,\n",
    "            color=\"#3498db\",\n",
    "        )\n",
    "        axes[idx].set_xlabel(feat_name)\n",
    "        axes[idx].set_ylabel(\"SHAP value\")\n",
    "    axes[idx].axhline(0, color=\"#2c3e50\", linewidth=0.8, alpha=0.5)\n",
    "    axes[idx].set_title(f\"SHAP Dependence: {feat_name}\", fontweight=\"bold\", fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Comparison — ROC & Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "models = {\n",
    "    \"LogReg Baseline\": y_prob_lr,\n",
    "    \"CatBoost Default\": y_prob_cb,\n",
    "    \"CatBoost Tuned\": y_prob_cb_best,\n",
    "}\n",
    "colors_model = [\"#95a5a6\", \"#3498db\", \"#e74c3c\"]\n",
    "\n",
    "# ROC\n",
    "for (name, y_prob), color in zip(models.items(), colors_model, strict=False):\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    auc = roc_auc_score(y_test, y_prob)\n",
    "    axes[0].plot(fpr, tpr, color=color, linewidth=2, label=f\"{name} (AUC={auc:.4f})\")\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], \"k--\", alpha=0.3)\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].set_title(\"ROC Curves\", fontweight=\"bold\", fontsize=14)\n",
    "axes[0].legend(loc=\"lower right\")\n",
    "\n",
    "# Precision-Recall\n",
    "for (name, y_prob), color in zip(models.items(), colors_model, strict=False):\n",
    "    prec, rec, _ = precision_recall_curve(y_test, y_prob)\n",
    "    ap = average_precision_score(y_test, y_prob)\n",
    "    axes[1].plot(rec, prec, color=color, linewidth=2, label=f\"{name} (AP={ap:.4f})\")\n",
    "\n",
    "axes[1].axhline(\n",
    "    y_test.mean(), color=\"#2c3e50\", ls=\"--\", alpha=0.3, label=f\"Baseline ({y_test.mean():.2%})\"\n",
    ")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"Precision-Recall Curves\", fontweight=\"bold\", fontsize=14)\n",
    "axes[1].legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KS Plot for best model\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob_cb_best)\n",
    "ks_values = tpr - fpr\n",
    "ks_max_idx = np.argmax(ks_values)\n",
    "ks_max = ks_values[ks_max_idx]\n",
    "ks_threshold = thresholds[ks_max_idx]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(\n",
    "    thresholds,\n",
    "    tpr[:-1] if len(tpr) > len(thresholds) else tpr[: len(thresholds)],\n",
    "    label=\"TPR (Sensitivity)\",\n",
    "    color=\"#2ecc71\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.plot(\n",
    "    thresholds,\n",
    "    fpr[:-1] if len(fpr) > len(thresholds) else fpr[: len(thresholds)],\n",
    "    label=\"FPR (1-Specificity)\",\n",
    "    color=\"#e74c3c\",\n",
    "    linewidth=2,\n",
    ")\n",
    "ax.axvline(\n",
    "    ks_threshold,\n",
    "    color=\"#2c3e50\",\n",
    "    ls=\"--\",\n",
    "    alpha=0.6,\n",
    "    label=f\"KS={ks_max:.4f} at threshold={ks_threshold:.3f}\",\n",
    ")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Rate\")\n",
    "ax.set_title(\"KS Plot — CatBoost Tuned\", fontweight=\"bold\", fontsize=14)\n",
    "ax.legend()\n",
    "ax.invert_xaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"KS Statistic: {ks_max:.4f} at threshold {ks_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Probability Calibration\n",
    "\n",
    "CatBoost is typically well-calibrated, but we test Isotonic and Platt to verify.\n",
    "\n",
    "Legacy results: Uncalibrated ECE=0.0029, Isotonic ECE=0.0019, Platt ECE=0.0187."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration using the calibration set\n",
    "y_prob_cal = cb_best.predict_proba(X_cal)[:, 1]\n",
    "\n",
    "# Isotonic calibration\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "iso_cal = IsotonicRegression(y_min=0, y_max=1, out_of_bounds=\"clip\")\n",
    "iso_cal.fit(y_prob_cal, y_cal.values)\n",
    "y_prob_iso = iso_cal.predict(y_prob_cb_best)\n",
    "\n",
    "# Platt scaling\n",
    "from sklearn.linear_model import LogisticRegression as PlattLR\n",
    "\n",
    "platt_lr = PlattLR(max_iter=1000)\n",
    "platt_lr.fit(y_prob_cal.reshape(-1, 1), y_cal.values)\n",
    "y_prob_platt = platt_lr.predict_proba(y_prob_cb_best.reshape(-1, 1))[:, 1]\n",
    "\n",
    "# Compute metrics for all calibration methods\n",
    "cal_results = {}\n",
    "for name, y_prob in [\n",
    "    (\"Uncalibrated\", y_prob_cb_best),\n",
    "    (\"Isotonic\", y_prob_iso),\n",
    "    (\"Platt Sigmoid\", y_prob_platt),\n",
    "]:\n",
    "    ece = expected_calibration_error(y_test.values, y_prob)\n",
    "    brier = brier_score_loss(y_test.values, y_prob)\n",
    "    auc = roc_auc_score(y_test.values, y_prob)\n",
    "    cal_results[name] = {\"ECE\": ece, \"Brier\": brier, \"AUC\": auc}\n",
    "    print(f\"{name:20s} — ECE: {ece:.4f}, Brier: {brier:.4f}, AUC: {auc:.4f}\")\n",
    "\n",
    "cal_results_df = pd.DataFrame(cal_results).T\n",
    "print(\"\\n\" + cal_results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibration curves (reliability diagrams)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "cal_data = [\n",
    "    (\"Uncalibrated\", y_prob_cb_best, \"#3498db\"),\n",
    "    (\"Isotonic\", y_prob_iso, \"#2ecc71\"),\n",
    "    (\"Platt\", y_prob_platt, \"#e67e22\"),\n",
    "]\n",
    "\n",
    "for name, y_prob, color in cal_data:\n",
    "    prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10, strategy=\"uniform\")\n",
    "    axes[0].plot(prob_pred, prob_true, \"o-\", color=color, linewidth=2, markersize=6, label=name)\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], \"k--\", alpha=0.3, label=\"Perfect calibration\")\n",
    "axes[0].set_xlabel(\"Mean Predicted Probability\")\n",
    "axes[0].set_ylabel(\"Fraction of Positives\")\n",
    "axes[0].set_title(\"Calibration Curves (Reliability Diagram)\", fontweight=\"bold\", fontsize=14)\n",
    "axes[0].legend()\n",
    "axes[0].set_xlim([0, 0.6])\n",
    "axes[0].set_ylim([0, 0.6])\n",
    "\n",
    "# Prediction distribution by calibration method\n",
    "for name, y_prob, color in cal_data:\n",
    "    axes[1].hist(y_prob, bins=50, alpha=0.4, color=color, label=name, density=True)\n",
    "\n",
    "axes[1].set_xlabel(\"Predicted Probability\")\n",
    "axes[1].set_ylabel(\"Density\")\n",
    "axes[1].set_title(\"Prediction Distribution\", fontweight=\"bold\", fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best calibration method\n",
    "best_cal_name = min(cal_results, key=lambda k: cal_results[k][\"ECE\"])\n",
    "print(f\"Best calibration: {best_cal_name} (ECE={cal_results[best_cal_name]['ECE']:.4f})\")\n",
    "\n",
    "if best_cal_name == \"Isotonic\":\n",
    "    y_prob_final = y_prob_iso\n",
    "    calibrator = iso_cal\n",
    "elif best_cal_name == \"Platt Sigmoid\":\n",
    "    y_prob_final = y_prob_platt\n",
    "    calibrator = platt_lr\n",
    "else:\n",
    "    y_prob_final = y_prob_cb_best\n",
    "    calibrator = None\n",
    "\n",
    "print(f\"\\nFinal model: CatBoost Tuned + {best_cal_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Final Evaluation — Complete Metrics Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full metrics comparison table\n",
    "final_metrics = classification_metrics(y_test.values, y_prob_final)\n",
    "\n",
    "results_table = pd.DataFrame(\n",
    "    {\n",
    "        \"LogReg Baseline\": {\n",
    "            \"AUC-ROC\": lr_metrics[\"auc_roc\"],\n",
    "            \"Gini\": lr_metrics[\"gini\"],\n",
    "            \"KS\": lr_metrics[\"ks_statistic\"],\n",
    "            \"Brier\": lr_metrics[\"brier_score\"],\n",
    "            \"ECE\": lr_metrics[\"ece\"],\n",
    "        },\n",
    "        \"CatBoost Default\": {\n",
    "            \"AUC-ROC\": cb_metrics[\"auc_roc\"],\n",
    "            \"Gini\": cb_metrics[\"gini\"],\n",
    "            \"KS\": cb_metrics[\"ks_statistic\"],\n",
    "            \"Brier\": cb_metrics[\"brier_score\"],\n",
    "            \"ECE\": cb_metrics[\"ece\"],\n",
    "        },\n",
    "        \"CatBoost Tuned\": {\n",
    "            \"AUC-ROC\": cb_best_metrics[\"auc_roc\"],\n",
    "            \"Gini\": cb_best_metrics[\"gini\"],\n",
    "            \"KS\": cb_best_metrics[\"ks_statistic\"],\n",
    "            \"Brier\": cb_best_metrics[\"brier_score\"],\n",
    "            \"ECE\": cb_best_metrics[\"ece\"],\n",
    "        },\n",
    "        f\"CatBoost + {best_cal_name}\": {\n",
    "            \"AUC-ROC\": final_metrics[\"auc_roc\"],\n",
    "            \"Gini\": final_metrics[\"gini\"],\n",
    "            \"KS\": final_metrics[\"ks_statistic\"],\n",
    "            \"Brier\": final_metrics[\"brier_score\"],\n",
    "            \"ECE\": final_metrics[\"ece\"],\n",
    "        },\n",
    "    }\n",
    ")\n",
    "\n",
    "# Format and display\n",
    "print(\"=\" * 80)\n",
    "print(\"FINAL MODEL COMPARISON — TEST SET (Out-of-Time)\")\n",
    "print(\"=\" * 80)\n",
    "print(results_table.T.to_string(float_format=\"{:.4f}\".format))\n",
    "print()\n",
    "\n",
    "# Highlight best\n",
    "for metric in results_table.index:\n",
    "    best_col = (\n",
    "        results_table.loc[metric].idxmax()\n",
    "        if metric in [\"AUC-ROC\", \"Gini\", \"KS\"]\n",
    "        else results_table.loc[metric].idxmin()\n",
    "    )\n",
    "    best_val = results_table.loc[metric, best_col]\n",
    "    print(f\"  Best {metric:8s}: {best_val:.4f} ({best_col})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution by true class — final model\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Histogram\n",
    "for flag in [0, 1]:\n",
    "    mask = y_test == flag\n",
    "    label = \"Fully Paid\" if flag == 0 else \"Default\"\n",
    "    color = \"#2ecc71\" if flag == 0 else \"#e74c3c\"\n",
    "    axes[0].hist(y_prob_final[mask], bins=50, alpha=0.5, density=True, color=color, label=label)\n",
    "axes[0].set_xlabel(\"Predicted PD\")\n",
    "axes[0].set_ylabel(\"Density\")\n",
    "axes[0].set_title(\"Score Distribution by True Class\", fontweight=\"bold\", fontsize=14)\n",
    "axes[0].legend()\n",
    "\n",
    "# Cumulative gains chart\n",
    "sorted_idx = np.argsort(y_prob_final)[::-1]\n",
    "y_sorted = y_test.values[sorted_idx]\n",
    "cum_defaults = np.cumsum(y_sorted) / y_sorted.sum()\n",
    "pct_population = np.arange(1, len(y_sorted) + 1) / len(y_sorted)\n",
    "\n",
    "axes[1].plot(pct_population, cum_defaults, color=\"#e74c3c\", linewidth=2, label=\"Model\")\n",
    "axes[1].plot([0, 1], [0, 1], \"k--\", alpha=0.3, label=\"Random\")\n",
    "axes[1].set_xlabel(\"% Population (sorted by PD descending)\")\n",
    "axes[1].set_ylabel(\"% Defaults Captured\")\n",
    "axes[1].set_title(\"Cumulative Gains Chart\", fontweight=\"bold\", fontsize=14)\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Decile Analysis & Lift Table\n",
    "\n",
    "Rank-ordering analysis: divide the test portfolio into 10 equal bins by predicted PD. A well-performing model should show monotonically increasing default rates across deciles, with high lift in the riskiest segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decile analysis on final calibrated predictions\n",
    "decile_df = pd.DataFrame(\n",
    "    {\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_prob\": y_prob_final,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create deciles (1=lowest risk, 10=highest risk)\n",
    "decile_df[\"decile\"] = pd.qcut(decile_df[\"y_prob\"], q=10, labels=False, duplicates=\"drop\") + 1\n",
    "\n",
    "decile_stats = (\n",
    "    decile_df.groupby(\"decile\")\n",
    "    .agg(\n",
    "        n_loans=(\"y_true\", \"count\"),\n",
    "        n_defaults=(\"y_true\", \"sum\"),\n",
    "        default_rate=(\"y_true\", \"mean\"),\n",
    "        avg_pd=(\"y_prob\", \"mean\"),\n",
    "        min_pd=(\"y_prob\", \"min\"),\n",
    "        max_pd=(\"y_prob\", \"max\"),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "base_rate = decile_df[\"y_true\"].mean()\n",
    "decile_stats[\"lift\"] = decile_stats[\"default_rate\"] / base_rate\n",
    "\n",
    "# Cumulative capture from highest risk decile down\n",
    "decile_rev = decile_stats.sort_values(\"decile\", ascending=False).reset_index(drop=True)\n",
    "decile_rev[\"cum_defaults\"] = decile_rev[\"n_defaults\"].cumsum()\n",
    "decile_rev[\"cum_capture\"] = decile_rev[\"cum_defaults\"] / decile_rev[\"n_defaults\"].sum()\n",
    "\n",
    "print(\"=\" * 95)\n",
    "print(\"DECILE ANALYSIS — Test Set (Out-of-Time)\")\n",
    "print(\"=\" * 95)\n",
    "print(decile_stats.to_string(index=False, float_format=\"{:.4f}\".format))\n",
    "print(f\"\\nBase default rate: {base_rate:.4f}\")\n",
    "print(f\"Top decile lift: {decile_stats.iloc[-1]['lift']:.2f}x\")\n",
    "top3_capture = decile_rev.iloc[:3][\"cum_capture\"].iloc[-1]\n",
    "print(f\"Top 3 deciles capture: {top3_capture:.1%} of all defaults\")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "colors_d = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, 10))\n",
    "\n",
    "# Default rate by decile\n",
    "axes[0].bar(decile_stats[\"decile\"], decile_stats[\"default_rate\"], color=colors_d)\n",
    "axes[0].axhline(\n",
    "    base_rate, color=\"#2c3e50\", ls=\"--\", alpha=0.6, label=f\"Base rate ({base_rate:.2%})\"\n",
    ")\n",
    "axes[0].set_xlabel(\"Risk Decile (1=lowest, 10=highest)\")\n",
    "axes[0].set_ylabel(\"Actual Default Rate\")\n",
    "axes[0].set_title(\"Default Rate by Risk Decile\", fontweight=\"bold\", fontsize=13)\n",
    "axes[0].legend()\n",
    "\n",
    "# Lift chart\n",
    "axes[1].bar(decile_stats[\"decile\"], decile_stats[\"lift\"], color=colors_d)\n",
    "axes[1].axhline(1.0, color=\"#2c3e50\", ls=\"--\", alpha=0.6, label=\"No lift (1.0)\")\n",
    "axes[1].set_xlabel(\"Risk Decile\")\n",
    "axes[1].set_ylabel(\"Lift\")\n",
    "axes[1].set_title(\"Lift by Risk Decile\", fontweight=\"bold\", fontsize=13)\n",
    "axes[1].legend()\n",
    "\n",
    "# Cumulative gains\n",
    "n_dec = range(1, len(decile_rev) + 1)\n",
    "axes[2].plot(\n",
    "    n_dec, decile_rev[\"cum_capture\"].values, \"o-\", color=\"#e74c3c\", linewidth=2, markersize=8\n",
    ")\n",
    "axes[2].plot(n_dec, np.linspace(0.1, 1.0, len(decile_rev)), \"k--\", alpha=0.3, label=\"Random\")\n",
    "axes[2].set_xlabel(\"# Deciles (from highest risk)\")\n",
    "axes[2].set_ylabel(\"Cumulative % Defaults Captured\")\n",
    "axes[2].set_title(\"Cumulative Gains by Decile\", fontweight=\"bold\", fontsize=13)\n",
    "axes[2].legend()\n",
    "axes[2].yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Population Stability Index (PSI)\n",
    "\n",
    "PSI quantifies distribution shift between training and OOT test sets — critical for model monitoring:\n",
    "- **PSI < 0.10**: Stable — no action needed\n",
    "- **0.10 ≤ PSI < 0.25**: Moderate shift — investigate\n",
    "- **PSI ≥ 0.25**: Significant shift — model may need retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_psi(expected, actual, n_bins=10):\n",
    "    \"\"\"Population Stability Index between two distributions.\"\"\"\n",
    "    _, bin_edges = pd.qcut(expected, q=n_bins, retbins=True, duplicates=\"drop\")\n",
    "    bin_edges = bin_edges.copy()\n",
    "    bin_edges[0] = -np.inf\n",
    "    bin_edges[-1] = np.inf\n",
    "\n",
    "    expected_counts = np.histogram(expected, bins=bin_edges)[0]\n",
    "    actual_counts = np.histogram(actual, bins=bin_edges)[0]\n",
    "\n",
    "    expected_pct = expected_counts / expected_counts.sum() + 1e-8\n",
    "    actual_pct = actual_counts / actual_counts.sum() + 1e-8\n",
    "\n",
    "    return np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "\n",
    "\n",
    "# PSI for key features (train vs OOT test)\n",
    "numeric_feats_psi = [f for f in CATBOOST_FEATURES if f not in CAT_FEATURES]\n",
    "psi_results = {}\n",
    "for col in numeric_feats_psi:\n",
    "    tr = X_train[col].dropna()\n",
    "    te = X_test[col].dropna()\n",
    "    if len(tr) > 100 and len(te) > 100:\n",
    "        psi_results[col] = calculate_psi(tr.values, te.values)\n",
    "\n",
    "# PSI for the PD score itself\n",
    "train_sample_psi = X_train.sample(min(100_000, len(X_train)), random_state=42)\n",
    "y_prob_train_psi = cb_best.predict_proba(train_sample_psi)[:, 1]\n",
    "psi_results[\"** PD Score **\"] = calculate_psi(y_prob_train_psi, y_prob_cb_best)\n",
    "\n",
    "psi_df = pd.DataFrame({\"Feature\": psi_results.keys(), \"PSI\": psi_results.values()})\n",
    "psi_df = psi_df.sort_values(\"PSI\", ascending=False).reset_index(drop=True)\n",
    "psi_df[\"Status\"] = psi_df[\"PSI\"].apply(\n",
    "    lambda x: \"SIGNIFICANT\" if x >= 0.25 else (\"Moderate\" if x >= 0.10 else \"Stable\")\n",
    ")\n",
    "\n",
    "print(\"=\" * 65)\n",
    "print(\"POPULATION STABILITY INDEX (Train vs Test — OOT)\")\n",
    "print(\"=\" * 65)\n",
    "print(psi_df.to_string(index=False, float_format=\"{:.4f}\".format))\n",
    "\n",
    "n_sig = (psi_df[\"Status\"] == \"SIGNIFICANT\").sum()\n",
    "n_mod = (psi_df[\"Status\"] == \"Moderate\").sum()\n",
    "n_stab = len(psi_df) - n_sig - n_mod\n",
    "print(f\"\\nSummary: {n_sig} significant, {n_mod} moderate, {n_stab} stable\")\n",
    "\n",
    "# Visualization\n",
    "fig, ax = plt.subplots(figsize=(12, max(5, len(psi_df) * 0.25)))\n",
    "colors_psi = [\n",
    "    \"#e74c3c\" if s == \"SIGNIFICANT\" else \"#f39c12\" if s == \"Moderate\" else \"#2ecc71\"\n",
    "    for s in psi_df[\"Status\"]\n",
    "]\n",
    "ax.barh(psi_df[\"Feature\"][::-1], psi_df[\"PSI\"][::-1], color=colors_psi[::-1])\n",
    "ax.axvline(0.10, color=\"#f39c12\", ls=\"--\", alpha=0.7, label=\"Moderate (0.10)\")\n",
    "ax.axvline(0.25, color=\"#e74c3c\", ls=\"--\", alpha=0.7, label=\"Significant (0.25)\")\n",
    "ax.set_xlabel(\"PSI\")\n",
    "ax.set_title(\"Population Stability Index — Train vs Test (OOT)\", fontweight=\"bold\", fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Partial Dependence Plots (PDP)\n",
    "\n",
    "PDP shows the average marginal effect of each feature on predicted PD. Unlike SHAP (which decomposes individual predictions), PDP reveals the global functional form: linear, monotonic, or threshold effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDP for top numeric features by SHAP importance\n",
    "numeric_only_pdp = [f for f in CATBOOST_FEATURES if f not in CAT_FEATURES]\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "feat_imp_sorted = sorted(zip(CATBOOST_FEATURES, mean_abs_shap, strict=False), key=lambda x: -x[1])\n",
    "top_numeric_pdp = [f for f, _ in feat_imp_sorted if f in numeric_only_pdp][:6]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "pdp_sample = X_test.sample(min(2000, len(X_test)), random_state=42)\n",
    "\n",
    "for idx, feat in enumerate(top_numeric_pdp):\n",
    "    q_low = pdp_sample[feat].quantile(0.02)\n",
    "    q_high = pdp_sample[feat].quantile(0.98)\n",
    "    if pd.isna(q_low) or pd.isna(q_high) or q_low == q_high:\n",
    "        axes_flat[idx].text(0.5, 0.5, f\"{feat}\\n(insufficient data)\", ha=\"center\", va=\"center\")\n",
    "        continue\n",
    "\n",
    "    grid = np.linspace(q_low, q_high, 40)\n",
    "    pdp_vals = []\n",
    "    for val in grid:\n",
    "        X_mod = pdp_sample.copy()\n",
    "        X_mod[feat] = val\n",
    "        pdp_vals.append(cb_best.predict_proba(X_mod)[:, 1].mean())\n",
    "\n",
    "    axes_flat[idx].plot(grid, pdp_vals, color=\"#e74c3c\", linewidth=2.5)\n",
    "    axes_flat[idx].fill_between(grid, pdp_vals, alpha=0.1, color=\"#e74c3c\")\n",
    "    axes_flat[idx].set_xlabel(feat, fontsize=10)\n",
    "    axes_flat[idx].set_ylabel(\"Avg Predicted PD\")\n",
    "    axes_flat[idx].set_title(f\"PDP: {feat}\", fontweight=\"bold\", fontsize=11)\n",
    "\n",
    "    # Rug plot showing data distribution\n",
    "    rug = pdp_sample[feat].dropna().values[:300]\n",
    "    axes_flat[idx].plot(\n",
    "        rug, [min(pdp_vals)] * len(rug), \"|\", color=\"#3498db\", alpha=0.15, markersize=8\n",
    "    )\n",
    "\n",
    "plt.suptitle(\n",
    "    \"Partial Dependence Plots — Top 6 Numeric Features\", fontweight=\"bold\", fontsize=14, y=1.01\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Threshold-Based Business Analysis\n",
    "\n",
    "Simulate business outcomes at different PD cutoff thresholds. This helps determine the optimal operating point balancing approval rate, expected losses, and risk appetite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold analysis — business impact at different cutoffs\n",
    "thresholds = np.arange(0.05, 0.55, 0.05)\n",
    "threshold_results = []\n",
    "\n",
    "n_total = len(y_test)\n",
    "n_defaults_total = int(y_test.sum())\n",
    "\n",
    "for thresh in thresholds:\n",
    "    approved = y_prob_final < thresh\n",
    "    n_approved = int(approved.sum())\n",
    "    approval_rate = n_approved / n_total\n",
    "\n",
    "    approved_defaults = int((approved & (y_test == 1)).sum())\n",
    "    loss_rate = approved_defaults / n_approved if n_approved > 0 else 0\n",
    "\n",
    "    caught = int((~approved & (y_test == 1)).sum())\n",
    "    catch_rate = caught / n_defaults_total\n",
    "\n",
    "    threshold_results.append(\n",
    "        {\n",
    "            \"Threshold\": thresh,\n",
    "            \"Approval %\": approval_rate,\n",
    "            \"# Approved\": n_approved,\n",
    "            \"Loss Rate\": loss_rate,\n",
    "            \"Defaults Caught %\": catch_rate,\n",
    "        }\n",
    "    )\n",
    "\n",
    "thresh_df = pd.DataFrame(threshold_results)\n",
    "print(\"=\" * 80)\n",
    "print(\"THRESHOLD BUSINESS ANALYSIS — What-If Scenarios\")\n",
    "print(\"=\" * 80)\n",
    "print(thresh_df.to_string(index=False, float_format=\"{:.3f}\".format))\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax2 = ax1.twinx()\n",
    "l1 = ax1.plot(\n",
    "    thresh_df[\"Threshold\"],\n",
    "    thresh_df[\"Approval %\"],\n",
    "    \"o-\",\n",
    "    color=\"#3498db\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    label=\"Approval Rate\",\n",
    ")\n",
    "l2 = ax2.plot(\n",
    "    thresh_df[\"Threshold\"],\n",
    "    thresh_df[\"Loss Rate\"],\n",
    "    \"s-\",\n",
    "    color=\"#e74c3c\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    "    label=\"Loss Rate (approved)\",\n",
    ")\n",
    "ax1.set_xlabel(\"PD Threshold (approve if PD < threshold)\")\n",
    "ax1.set_ylabel(\"Approval Rate\", color=\"#3498db\")\n",
    "ax2.set_ylabel(\"Loss Rate (among approved)\", color=\"#e74c3c\")\n",
    "ax1.set_title(\"Approval Rate vs Loss Rate by Threshold\", fontweight=\"bold\", fontsize=13)\n",
    "lines = l1 + l2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc=\"center right\")\n",
    "\n",
    "axes[1].plot(\n",
    "    thresh_df[\"Threshold\"],\n",
    "    thresh_df[\"Defaults Caught %\"],\n",
    "    \"o-\",\n",
    "    color=\"#2ecc71\",\n",
    "    linewidth=2,\n",
    "    markersize=6,\n",
    ")\n",
    "axes[1].fill_between(\n",
    "    thresh_df[\"Threshold\"], thresh_df[\"Defaults Caught %\"], alpha=0.2, color=\"#2ecc71\"\n",
    ")\n",
    "axes[1].set_xlabel(\"PD Threshold\")\n",
    "axes[1].set_ylabel(\"% of Total Defaults Caught (Rejected)\")\n",
    "axes[1].set_title(\"Default Capture Rate by Threshold\", fontweight=\"bold\", fontsize=13)\n",
    "axes[1].yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Suggest optimal operating point\n",
    "sweet = thresh_df[(thresh_df[\"Approval %\"] >= 0.50) & (thresh_df[\"Approval %\"] <= 0.85)]\n",
    "if len(sweet) > 0:\n",
    "    best = sweet.loc[sweet[\"Loss Rate\"].idxmin()]\n",
    "    print(f\"\\nSuggested operating point: PD threshold = {best['Threshold']:.2f}\")\n",
    "    print(f\"  Approval rate:   {best['Approval %']:.1%}\")\n",
    "    print(f\"  Loss rate:       {best['Loss Rate']:.2%}\")\n",
    "    print(f\"  Defaults caught: {best['Defaults Caught %']:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Save Models & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CatBoost model\n",
    "cb_best.save_model(str(MODEL_DIR / \"pd_catboost_tuned.cbm\"))\n",
    "print(f\"CatBoost model saved to {MODEL_DIR / 'pd_catboost_tuned.cbm'}\")\n",
    "\n",
    "# Save calibrator\n",
    "if calibrator is not None:\n",
    "    with open(MODEL_DIR / \"pd_calibrator.pkl\", \"wb\") as f:\n",
    "        pickle.dump(calibrator, f)\n",
    "    print(f\"Calibrator ({best_cal_name}) saved to {MODEL_DIR / 'pd_calibrator.pkl'}\")\n",
    "\n",
    "# Save LogReg pipeline\n",
    "with open(MODEL_DIR / \"pd_logreg_baseline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(lr_pipeline, f)\n",
    "print(f\"LogReg pipeline saved to {MODEL_DIR / 'pd_logreg_baseline.pkl'}\")\n",
    "\n",
    "# Save Optuna study\n",
    "with open(MODEL_DIR / \"optuna_study_pd.pkl\", \"wb\") as f:\n",
    "    pickle.dump(study, f)\n",
    "print(f\"Optuna study saved to {MODEL_DIR / 'optuna_study_pd.pkl'}\")\n",
    "\n",
    "# Save test predictions for NB04 (conformal prediction)\n",
    "preds_df = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": test[\"id\"].values if \"id\" in test.columns else range(len(test)),\n",
    "        \"y_true\": y_test.values,\n",
    "        \"y_prob_lr\": y_prob_lr,\n",
    "        \"y_prob_cb_default\": y_prob_cb,\n",
    "        \"y_prob_cb_tuned\": y_prob_cb_best,\n",
    "        \"y_prob_final\": y_prob_final,\n",
    "    }\n",
    ")\n",
    "preds_df.to_parquet(DATA_DIR / \"test_predictions.parquet\", index=False)\n",
    "print(f\"Test predictions saved ({len(preds_df):,} rows)\")\n",
    "\n",
    "# Save best params\n",
    "best_params_record = {\n",
    "    \"optuna_best_params\": study.best_params,\n",
    "    \"optuna_best_auc\": study.best_value,\n",
    "    \"catboost_default_params\": cb_params_default,\n",
    "    \"best_calibration\": best_cal_name,\n",
    "    \"final_test_metrics\": final_metrics,\n",
    "}\n",
    "with open(MODEL_DIR / \"pd_training_record.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_params_record, f)\n",
    "print(f\"Training record saved to {MODEL_DIR / 'pd_training_record.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Model Performance (Test Set — Out-of-Time)\n",
    "\n",
    "| Metric | LogReg | CatBoost Default | CatBoost Tuned | + Calibration |\n",
    "|--------|--------|-----------------|----------------|---------------|\n",
    "| AUC-ROC | ~ | ~ | ~ | ~ |\n",
    "| Gini | ~ | ~ | ~ | ~ |\n",
    "| KS | ~ | ~ | ~ | ~ |\n",
    "| Brier | ~ | ~ | ~ | ~ |\n",
    "| ECE | ~ | ~ | ~ | ~ |\n",
    "\n",
    "*(Values filled in by execution above)*\n",
    "\n",
    "### Key Decisions\n",
    "- **CatBoost with Optuna HPO** as primary model (50 trials)\n",
    "- **Isotonic calibration** if it improves ECE, otherwise uncalibrated\n",
    "- **SHAP** for global interpretability (native CatBoost method)\n",
    "- **PDP** for functional form analysis (feature-response relationships)\n",
    "- **Out-of-Time validation** prevents temporal leakage\n",
    "- **Balanced class weights** to handle ~18.5% default rate\n",
    "\n",
    "### Model Validation & Diagnostics\n",
    "- **Decile analysis**: Monotonic default rates, top-decile lift, cumulative gains\n",
    "- **PSI**: Distribution stability between train and OOT test sets\n",
    "- **PDP**: Non-linear feature-response curves for top numeric predictors\n",
    "- **Threshold analysis**: Business operating point (approval rate vs loss rate)\n",
    "\n",
    "### Legacy Comparison\n",
    "- Legacy CatBoost AUC: ~0.689 (random split, fewer features)\n",
    "- Our improvement: OOT split + 44 features + Optuna HPO + full validation suite\n",
    "\n",
    "### Artifacts Saved\n",
    "-  — Final CatBoost model\n",
    "-  — Isotonic/Platt calibrator\n",
    "-  — LogReg pipeline\n",
    "-  — Optuna study (50 trials)\n",
    "-  — Best params + metrics\n",
    "-  — Predictions for NB04\n",
    "\n",
    "### Next Steps\n",
    "1. **Notebook 04**: Conformal Prediction — MAPIE intervals [PD_low, PD_high]\n",
    "2. **Notebook 08**: Portfolio Optimization — use intervals as uncertainty sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## Final Conclusions: PD Modeling\n",
    "\n",
    "### Key Findings\n",
    "- CatBoost provides the best discrimination among tested PD candidates.\n",
    "- Calibration materially improves probability reliability without requiring higher AUC.\n",
    "- Final PD outputs are suitable for decision use, not only ranking use.\n",
    "\n",
    "### Financial Risk Interpretation\n",
    "- In lending, miscalibrated PDs distort approval thresholds, pricing, and provisioning.\n",
    "- Reliable PD levels are required for IFRS9 and for economically coherent optimization.\n",
    "- Separation of discrimination and calibration is essential for robust risk governance.\n",
    "\n",
    "### Contribution to End-to-End Pipeline\n",
    "- Delivers the core PD model artifact and calibrated probabilities.\n",
    "- Supplies point-risk inputs consumed by conformal uncertainty, ECL staging, and portfolio optimization.\n",
    "- Establishes the primary model-quality benchmark used across the project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lending-club-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
