{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Feature Engineering for Credit Risk Modeling\n",
    "\n",
    "**Objective**: Transform raw loan data into predictive features for PD (Probability of Default) modeling using credit risk domain knowledge.\n",
    "\n",
    "**Input**: `train.parquet` (1.35M loans, 110 raw columns) + `calibration.parquet` + `test.parquet`\n",
    "\n",
    "**Approach**:\n",
    "1. Clean & parse raw columns (int_rate %, term, emp_length)\n",
    "2. Create financial ratios (loan-to-income, installment burden, etc.)\n",
    "3. Create credit history features (FICO mid, credit age, delinquency flags)\n",
    "4. Create behavioral / account features\n",
    "5. WOE encoding with OptBinning (grade, purpose, home_ownership)\n",
    "6. Information Value (IV) ranking for feature selection\n",
    "7. Validate final feature set with Pandera schemas\n",
    "8. Save modeling-ready datasets\n",
    "\n",
    "**Key Principle**: Only use features available at **loan origination** — no post-loan variables (leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "from src.features.schemas import loan_master_schema\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", font_scale=1.1)\n",
    "plt.rcParams.update({\"figure.figsize\": (12, 6), \"figure.dpi\": 100})\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Raw Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.read_parquet(DATA_DIR / \"train.parquet\")\n",
    "cal_raw = pd.read_parquet(DATA_DIR / \"calibration.parquet\")\n",
    "test_raw = pd.read_parquet(DATA_DIR / \"test.parquet\")\n",
    "\n",
    "print(f\"Train:       {train_raw.shape[0]:>10,} rows x {train_raw.shape[1]} cols\")\n",
    "print(f\"Calibration: {cal_raw.shape[0]:>10,} rows x {cal_raw.shape[1]} cols\")\n",
    "print(f\"Test:        {test_raw.shape[0]:>10,} rows x {test_raw.shape[1]} cols\")\n",
    "print(\n",
    "    f\"\\nDefault rates: Train={train_raw['default_flag'].mean():.2%}  \"\n",
    "    f\"Cal={cal_raw['default_flag'].mean():.2%}  \"\n",
    "    f\"Test={test_raw['default_flag'].mean():.2%}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Clean & Parse Raw Columns\n",
    "\n",
    "Several columns arrive as strings and need numeric conversion:\n",
    "- `int_rate`: \"13.56%\" → 13.56\n",
    "- `term`: \" 36 months\" → 36\n",
    "- `revol_util`: \"55.3%\" → 55.3\n",
    "- `emp_length`: \"10+ years\" → 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_raw_columns(df):\n",
    "    \"\"\"Parse string columns to numeric types.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # int_rate: \"13.56%\" -> 13.56\n",
    "    if \"int_rate\" in df.columns and df[\"int_rate\"].dtype == object:\n",
    "        df[\"int_rate\"] = (\n",
    "            df[\"int_rate\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.rstrip(\"%\")\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # term: \" 36 months\" -> 36\n",
    "    if \"term\" in df.columns and df[\"term\"].dtype == object:\n",
    "        df[\"term\"] = (\n",
    "            df[\"term\"].astype(str).str.extract(r\"(\\d+)\")[0].pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # revol_util: \"55.3%\" -> 55.3\n",
    "    if \"revol_util\" in df.columns and df[\"revol_util\"].dtype == object:\n",
    "        df[\"revol_util\"] = (\n",
    "            df[\"revol_util\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.rstrip(\"%\")\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "        )\n",
    "\n",
    "    # emp_length: parse to numeric years\n",
    "    if \"emp_length\" in df.columns and df[\"emp_length\"].dtype == object:\n",
    "        emp_map = {\n",
    "            \"< 1 year\": 0,\n",
    "            \"1 year\": 1,\n",
    "            \"2 years\": 2,\n",
    "            \"3 years\": 3,\n",
    "            \"4 years\": 4,\n",
    "            \"5 years\": 5,\n",
    "            \"6 years\": 6,\n",
    "            \"7 years\": 7,\n",
    "            \"8 years\": 8,\n",
    "            \"9 years\": 9,\n",
    "            \"10+ years\": 10,\n",
    "        }\n",
    "        df[\"emp_length_num\"] = df[\"emp_length\"].map(emp_map)\n",
    "\n",
    "    logger.info(f\"Cleaned raw columns: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train = clean_raw_columns(train_raw)\n",
    "cal = clean_raw_columns(cal_raw)\n",
    "test = clean_raw_columns(test_raw)\n",
    "\n",
    "# Verify int_rate is now numeric\n",
    "print(\n",
    "    f\"int_rate dtype: {train['int_rate'].dtype}, range: [{train['int_rate'].min():.2f}, {train['int_rate'].max():.2f}]\"\n",
    ")\n",
    "print(f\"term dtype: {train['term'].dtype}, values: {sorted(train['term'].dropna().unique())}\")\n",
    "print(f\"emp_length_num: {train['emp_length_num'].describe().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Financial Ratio Features\n",
    "\n",
    "These ratios capture the borrower's financial burden and leverage — key drivers of default risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_financial_ratios(df):\n",
    "    \"\"\"Create financial ratio features from raw loan data.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Loan-to-income ratio\n",
    "    df[\"loan_to_income\"] = np.where(\n",
    "        df[\"annual_inc\"] > 0, df[\"loan_amnt\"] / df[\"annual_inc\"], np.nan\n",
    "    )\n",
    "\n",
    "    # Installment burden: monthly payment as % of monthly income\n",
    "    df[\"installment_burden\"] = np.where(\n",
    "        df[\"annual_inc\"] > 0, (df[\"installment\"] / (df[\"annual_inc\"] / 12)) * 100, np.nan\n",
    "    )\n",
    "\n",
    "    # Revolving utilization (fraction 0-1)\n",
    "    if \"revol_util\" in df.columns:\n",
    "        df[\"rev_utilization\"] = df[\"revol_util\"] / 100.0\n",
    "    else:\n",
    "        df[\"rev_utilization\"] = np.nan\n",
    "\n",
    "    # Revolving balance to income\n",
    "    if \"revol_bal\" in df.columns:\n",
    "        df[\"revol_bal_to_income\"] = np.where(\n",
    "            df[\"annual_inc\"] > 0, df[\"revol_bal\"] / df[\"annual_inc\"], np.nan\n",
    "        )\n",
    "\n",
    "    # Total balance to high credit limit ratio (credit usage intensity)\n",
    "    if \"tot_cur_bal\" in df.columns and \"tot_hi_cred_lim\" in df.columns:\n",
    "        df[\"bal_to_limit\"] = np.where(\n",
    "            df[\"tot_hi_cred_lim\"] > 0, df[\"tot_cur_bal\"] / df[\"tot_hi_cred_lim\"], np.nan\n",
    "        )\n",
    "\n",
    "    # Open accounts ratio (active / total accounts)\n",
    "    if \"open_acc\" in df.columns and \"total_acc\" in df.columns:\n",
    "        df[\"open_acc_ratio\"] = np.where(\n",
    "            df[\"total_acc\"] > 0, df[\"open_acc\"] / df[\"total_acc\"], np.nan\n",
    "        )\n",
    "\n",
    "    # BC utilization (bankcard-specific)\n",
    "    if \"bc_util\" in df.columns:\n",
    "        df[\"bc_utilization\"] = df[\"bc_util\"] / 100.0\n",
    "\n",
    "    # BC open to buy as fraction of total BC limit\n",
    "    if \"bc_open_to_buy\" in df.columns and \"total_bc_limit\" in df.columns:\n",
    "        df[\"bc_available_ratio\"] = np.where(\n",
    "            df[\"total_bc_limit\"] > 0, df[\"bc_open_to_buy\"] / df[\"total_bc_limit\"], np.nan\n",
    "        )\n",
    "\n",
    "    logger.info(\"Created financial ratios\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train = create_financial_ratios(train)\n",
    "cal = create_financial_ratios(cal)\n",
    "test = create_financial_ratios(test)\n",
    "\n",
    "# Show ratio distributions\n",
    "ratio_cols = [\n",
    "    \"loan_to_income\",\n",
    "    \"installment_burden\",\n",
    "    \"rev_utilization\",\n",
    "    \"revol_bal_to_income\",\n",
    "    \"open_acc_ratio\",\n",
    "]\n",
    "train[ratio_cols].describe().T.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ratio features by default status\n",
    "fig, axes = plt.subplots(1, 5, figsize=(25, 5))\n",
    "for i, col in enumerate(ratio_cols):\n",
    "    data = train[[col, \"default_flag\"]].dropna()\n",
    "    q99 = data[col].quantile(0.99)\n",
    "    data = data[data[col] <= q99]\n",
    "    for flag in [0, 1]:\n",
    "        subset = data[data[\"default_flag\"] == flag][col]\n",
    "        axes[i].hist(\n",
    "            subset,\n",
    "            bins=40,\n",
    "            alpha=0.5,\n",
    "            density=True,\n",
    "            color=[\"#2ecc71\", \"#e74c3c\"][flag],\n",
    "            label=[\"Fully Paid\", \"Default\"][flag],\n",
    "        )\n",
    "    axes[i].set_title(col, fontweight=\"bold\", fontsize=11)\n",
    "    axes[i].legend(fontsize=8)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Financial Ratios: Distribution by Default Status\", fontweight=\"bold\", fontsize=14, y=1.02\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Credit History Features\n",
    "\n",
    "Derived from credit bureau data — these capture the borrower's credit track record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_credit_features(df):\n",
    "    \"\"\"Create credit history and bureau-derived features.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # FICO mid-point (average of range)\n",
    "    if \"fico_range_low\" in df.columns and \"fico_range_high\" in df.columns:\n",
    "        df[\"fico_score\"] = (df[\"fico_range_low\"] + df[\"fico_range_high\"]) / 2\n",
    "\n",
    "    # Credit history length in months\n",
    "    if \"earliest_cr_line\" in df.columns and \"issue_d\" in df.columns:\n",
    "        df[\"credit_history_months\"] = (\n",
    "            (pd.to_datetime(df[\"issue_d\"]) - pd.to_datetime(df[\"earliest_cr_line\"])).dt.days / 30.44\n",
    "        ).clip(lower=0)\n",
    "\n",
    "    # Credit age in years (more interpretable)\n",
    "    if \"credit_history_months\" in df.columns:\n",
    "        df[\"credit_age_years\"] = df[\"credit_history_months\"] / 12\n",
    "\n",
    "    # Delinquency flags\n",
    "    if \"delinq_2yrs\" in df.columns:\n",
    "        df[\"has_delinq_2yrs\"] = (df[\"delinq_2yrs\"] > 0).astype(int)\n",
    "\n",
    "    if \"pub_rec\" in df.columns:\n",
    "        df[\"has_pub_rec\"] = (df[\"pub_rec\"] > 0).astype(int)\n",
    "\n",
    "    if \"pub_rec_bankruptcies\" in df.columns:\n",
    "        df[\"has_bankruptcy\"] = (df[\"pub_rec_bankruptcies\"] > 0).astype(int)\n",
    "\n",
    "    # Days since last delinquency (rename for clarity)\n",
    "    if \"mths_since_last_delinq\" in df.columns:\n",
    "        df[\"mths_since_delinq\"] = df[\"mths_since_last_delinq\"]\n",
    "        df[\"delinq_recency\"] = df[\"mths_since_delinq\"].fillna(999)  # Never delinquent = far away\n",
    "\n",
    "    # Recent inquiries (credit-seeking behavior)\n",
    "    if \"inq_last_6mths\" in df.columns:\n",
    "        df[\"has_recent_inq\"] = (df[\"inq_last_6mths\"] > 0).astype(int)\n",
    "        df[\"high_inq_flag\"] = (df[\"inq_last_6mths\"] >= 3).astype(int)\n",
    "\n",
    "    # Delinquency severity score\n",
    "    delinq_cols = [\"num_tl_30dpd\", \"num_tl_90g_dpd_24m\", \"num_accts_ever_120_pd\"]\n",
    "    available = [c for c in delinq_cols if c in df.columns]\n",
    "    if available:\n",
    "        df[\"delinq_severity\"] = df[available].fillna(0).sum(axis=1)\n",
    "\n",
    "    # Percent of accounts never delinquent (credit reliability)\n",
    "    if \"pct_tl_nvr_dlq\" in df.columns:\n",
    "        df[\"pct_never_delinq\"] = df[\"pct_tl_nvr_dlq\"] / 100.0\n",
    "\n",
    "    logger.info(\"Created credit history features\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train = create_credit_features(train)\n",
    "cal = create_credit_features(cal)\n",
    "test = create_credit_features(test)\n",
    "\n",
    "credit_cols = [\n",
    "    \"fico_score\",\n",
    "    \"credit_age_years\",\n",
    "    \"has_delinq_2yrs\",\n",
    "    \"has_pub_rec\",\n",
    "    \"has_bankruptcy\",\n",
    "    \"delinq_severity\",\n",
    "]\n",
    "train[credit_cols].describe().T.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FICO score by default status\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# FICO distribution\n",
    "for flag in [0, 1]:\n",
    "    subset = train[train[\"default_flag\"] == flag][\"fico_score\"]\n",
    "    axes[0].hist(\n",
    "        subset,\n",
    "        bins=40,\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "        color=[\"#2ecc71\", \"#e74c3c\"][flag],\n",
    "        label=[\"Fully Paid\", \"Default\"][flag],\n",
    "    )\n",
    "axes[0].set_title(\"FICO Score Distribution\", fontweight=\"bold\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Credit age\n",
    "for flag in [0, 1]:\n",
    "    subset = train[train[\"default_flag\"] == flag][\"credit_age_years\"].dropna()\n",
    "    subset = subset[subset <= 40]\n",
    "    axes[1].hist(\n",
    "        subset,\n",
    "        bins=40,\n",
    "        alpha=0.5,\n",
    "        density=True,\n",
    "        color=[\"#2ecc71\", \"#e74c3c\"][flag],\n",
    "        label=[\"Fully Paid\", \"Default\"][flag],\n",
    "    )\n",
    "axes[1].set_title(\"Credit Age (Years)\", fontweight=\"bold\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Delinquency flags vs default rate\n",
    "delinq_flags = [\n",
    "    \"has_delinq_2yrs\",\n",
    "    \"has_pub_rec\",\n",
    "    \"has_bankruptcy\",\n",
    "    \"has_recent_inq\",\n",
    "    \"high_inq_flag\",\n",
    "]\n",
    "flag_rates = []\n",
    "for col in delinq_flags:\n",
    "    for val in [0, 1]:\n",
    "        rate = train[train[col] == val][\"default_flag\"].mean()\n",
    "        flag_rates.append({\"feature\": col, \"value\": val, \"default_rate\": rate})\n",
    "flag_df = pd.DataFrame(flag_rates)\n",
    "\n",
    "x = range(len(delinq_flags))\n",
    "width = 0.35\n",
    "r0 = flag_df[flag_df[\"value\"] == 0][\"default_rate\"].values\n",
    "r1 = flag_df[flag_df[\"value\"] == 1][\"default_rate\"].values\n",
    "axes[2].bar([i - width / 2 for i in x], r0, width, label=\"Flag=0\", color=\"#2ecc71\", alpha=0.8)\n",
    "axes[2].bar([i + width / 2 for i in x], r1, width, label=\"Flag=1\", color=\"#e74c3c\", alpha=0.8)\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels([c.replace(\"has_\", \"\") for c in delinq_flags], rotation=30, ha=\"right\")\n",
    "axes[2].set_ylabel(\"Default Rate\")\n",
    "axes[2].yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "axes[2].set_title(\"Default Rate by Risk Flag\", fontweight=\"bold\")\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Behavioral & Account Features\n",
    "\n",
    "Features derived from borrower's existing account behavior — number and type of credit lines, recent account activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_account_features(df):\n",
    "    \"\"\"Create account-level behavioral features.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Installment vs revolving account mix\n",
    "    if \"num_il_tl\" in df.columns and \"num_rev_accts\" in df.columns:\n",
    "        total_tl = df[\"num_il_tl\"].fillna(0) + df[\"num_rev_accts\"].fillna(0)\n",
    "        df[\"il_ratio\"] = np.where(total_tl > 0, df[\"num_il_tl\"].fillna(0) / total_tl, 0.5)\n",
    "\n",
    "    # Active revolving accounts as fraction of total\n",
    "    if \"num_actv_rev_tl\" in df.columns and \"num_rev_accts\" in df.columns:\n",
    "        df[\"active_rev_ratio\"] = np.where(\n",
    "            df[\"num_rev_accts\"] > 0, df[\"num_actv_rev_tl\"] / df[\"num_rev_accts\"], np.nan\n",
    "        )\n",
    "\n",
    "    # Recent credit line openings (risk signal: too many = credit seeking)\n",
    "    if \"num_tl_op_past_12m\" in df.columns:\n",
    "        df[\"many_recent_opens\"] = (df[\"num_tl_op_past_12m\"] >= 3).astype(int)\n",
    "\n",
    "    # Mortgage account flag (stability indicator)\n",
    "    if \"mort_acc\" in df.columns:\n",
    "        df[\"has_mortgage\"] = (df[\"mort_acc\"] > 0).astype(int)\n",
    "\n",
    "    # Percent of accounts with balance over 75% limit\n",
    "    if \"percent_bc_gt_75\" in df.columns:\n",
    "        df[\"high_util_pct\"] = df[\"percent_bc_gt_75\"] / 100.0\n",
    "\n",
    "    # Collections/chargeoff within 12 months (recent negative events)\n",
    "    if \"chargeoff_within_12_mths\" in df.columns:\n",
    "        df[\"recent_chargeoff\"] = (df[\"chargeoff_within_12_mths\"] > 0).astype(int)\n",
    "\n",
    "    if \"collections_12_mths_ex_med\" in df.columns:\n",
    "        df[\"recent_collections\"] = (df[\"collections_12_mths_ex_med\"] > 0).astype(int)\n",
    "\n",
    "    logger.info(\"Created account features\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train = create_account_features(train)\n",
    "cal = create_account_features(cal)\n",
    "test = create_account_features(test)\n",
    "\n",
    "acct_cols = [\"il_ratio\", \"active_rev_ratio\", \"many_recent_opens\", \"has_mortgage\", \"high_util_pct\"]\n",
    "train[acct_cols].describe().T.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Bucketing & Interaction Features\n",
    "\n",
    "Create interpretable buckets and interaction terms for credit risk segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_buckets(df):\n",
    "    \"\"\"Create bucketed features for risk segmentation.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Interest rate buckets\n",
    "    if \"int_rate\" in df.columns:\n",
    "        df[\"int_rate_bucket\"] = pd.cut(\n",
    "            df[\"int_rate\"],\n",
    "            bins=[0, 8, 12, 16, 20, 100],\n",
    "            labels=[\"very_low\", \"low\", \"medium\", \"high\", \"very_high\"],\n",
    "        )\n",
    "\n",
    "    # DTI buckets\n",
    "    if \"dti\" in df.columns:\n",
    "        df[\"dti_bucket\"] = pd.cut(\n",
    "            df[\"dti\"],\n",
    "            bins=[-1, 10, 20, 30, 40, 100],\n",
    "            labels=[\"low\", \"moderate\", \"high\", \"very_high\", \"extreme\"],\n",
    "        )\n",
    "\n",
    "    # FICO buckets (industry standard)\n",
    "    if \"fico_score\" in df.columns:\n",
    "        df[\"fico_bucket\"] = pd.cut(\n",
    "            df[\"fico_score\"],\n",
    "            bins=[0, 660, 700, 740, 780, 900],\n",
    "            labels=[\"subprime\", \"near_prime\", \"prime\", \"prime_plus\", \"super_prime\"],\n",
    "        )\n",
    "\n",
    "    # Loan amount buckets\n",
    "    if \"loan_amnt\" in df.columns:\n",
    "        df[\"loan_amnt_bucket\"] = pd.cut(\n",
    "            df[\"loan_amnt\"],\n",
    "            bins=[0, 5000, 10000, 20000, 30000, 50000],\n",
    "            labels=[\"very_small\", \"small\", \"medium\", \"large\", \"very_large\"],\n",
    "        )\n",
    "\n",
    "    # Credit age buckets\n",
    "    if \"credit_age_years\" in df.columns:\n",
    "        df[\"credit_age_bucket\"] = pd.cut(\n",
    "            df[\"credit_age_years\"],\n",
    "            bins=[0, 5, 10, 15, 20, 100],\n",
    "            labels=[\"very_young\", \"young\", \"established\", \"mature\", \"seasoned\"],\n",
    "        )\n",
    "\n",
    "    logger.info(\"Created bucket features\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_all_interactions(df):\n",
    "    \"\"\"Create interaction features for risk segmentation.\"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Grade x Term (key risk interaction)\n",
    "    if \"int_rate_bucket\" in df.columns and \"grade\" in df.columns:\n",
    "        df[\"int_rate_bucket__grade\"] = (\n",
    "            df[\"int_rate_bucket\"].astype(str) + \"__\" + df[\"grade\"].astype(str)\n",
    "        )\n",
    "\n",
    "    # Polynomial: loan_to_income squared\n",
    "    if \"loan_to_income\" in df.columns:\n",
    "        df[\"loan_to_income_sq\"] = df[\"loan_to_income\"] ** 2\n",
    "\n",
    "    # FICO x DTI interaction (combines credit quality with burden)\n",
    "    if \"fico_score\" in df.columns and \"dti\" in df.columns:\n",
    "        df[\"fico_x_dti\"] = df[\"fico_score\"] * df[\"dti\"]\n",
    "\n",
    "    # Log transforms for skewed features\n",
    "    for col in [\"annual_inc\", \"revol_bal\", \"loan_amnt\"]:\n",
    "        if col in df.columns:\n",
    "            df[f\"log_{col}\"] = np.log1p(df[col].clip(lower=0))\n",
    "\n",
    "    logger.info(\"Created interaction features\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train = create_all_buckets(train)\n",
    "train = create_all_interactions(train)\n",
    "cal = create_all_buckets(cal)\n",
    "cal = create_all_interactions(cal)\n",
    "test = create_all_buckets(test)\n",
    "test = create_all_interactions(test)\n",
    "\n",
    "print(\"Bucket distributions:\")\n",
    "for col in [\"int_rate_bucket\", \"dti_bucket\", \"fico_bucket\", \"loan_amnt_bucket\"]:\n",
    "    if col in train.columns:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(train[col].value_counts().sort_index().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default rate by FICO bucket and DTI bucket — heatmap\n",
    "fico_dti_pivot = train.dropna(subset=[\"fico_bucket\", \"dti_bucket\"]).pivot_table(\n",
    "    values=\"default_flag\", index=\"fico_bucket\", columns=\"dti_bucket\", aggfunc=\"mean\"\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "sns.heatmap(\n",
    "    fico_dti_pivot,\n",
    "    annot=True,\n",
    "    fmt=\".1%\",\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    linewidths=0.5,\n",
    "    ax=axes[0],\n",
    "    vmin=0,\n",
    "    vmax=0.5,\n",
    ")\n",
    "axes[0].set_title(\"Default Rate: FICO Bucket x DTI Bucket\", fontweight=\"bold\")\n",
    "\n",
    "# Grade x Term already done in EDA, now add FICO x loan amount\n",
    "fico_loan_pivot = train.dropna(subset=[\"fico_bucket\", \"loan_amnt_bucket\"]).pivot_table(\n",
    "    values=\"default_flag\", index=\"fico_bucket\", columns=\"loan_amnt_bucket\", aggfunc=\"mean\"\n",
    ")\n",
    "sns.heatmap(\n",
    "    fico_loan_pivot,\n",
    "    annot=True,\n",
    "    fmt=\".1%\",\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    linewidths=0.5,\n",
    "    ax=axes[1],\n",
    "    vmin=0,\n",
    "    vmax=0.5,\n",
    ")\n",
    "axes[1].set_title(\"Default Rate: FICO Bucket x Loan Amount Bucket\", fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. WOE Encoding with OptBinning\n",
    "\n",
    "**Weight of Evidence (WOE)** transforms categorical and continuous variables into risk-ordered numeric scores. Key benefits for credit scoring:\n",
    "- Handles non-linear relationships automatically\n",
    "- Monotonic WOE ensures regulatory compliance\n",
    "- **Information Value (IV)** provides built-in feature importance\n",
    "\n",
    "IV interpretation:\n",
    "| IV Range | Predictive Power |\n",
    "|----------|-----------------|\n",
    "| < 0.02 | Useless |\n",
    "| 0.02 - 0.1 | Weak |\n",
    "| 0.1 - 0.3 | Medium |\n",
    "| 0.3 - 0.5 | Strong |\n",
    "| > 0.5 | Suspicious (check for overfit/leakage) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optbinning import OptimalBinning\n",
    "\n",
    "# Features to WOE-encode\n",
    "woe_categorical = [\"grade\", \"sub_grade\", \"home_ownership\", \"purpose\", \"verification_status\", \"term\"]\n",
    "woe_numerical = [\n",
    "    \"int_rate\",\n",
    "    \"dti\",\n",
    "    \"annual_inc\",\n",
    "    \"loan_amnt\",\n",
    "    \"fico_score\",\n",
    "    \"installment_burden\",\n",
    "    \"rev_utilization\",\n",
    "    \"credit_age_years\",\n",
    "    \"open_acc\",\n",
    "    \"total_acc\",\n",
    "    \"revol_bal\",\n",
    "    \"pub_rec\",\n",
    "    \"inq_last_6mths\",\n",
    "    \"emp_length_num\",\n",
    "]\n",
    "\n",
    "# Fit WOE on training data only (avoid leakage)\n",
    "woe_results = {}\n",
    "iv_scores = {}\n",
    "train_target = train[\"default_flag\"].values\n",
    "\n",
    "print(\"Computing WOE encodings...\\n\")\n",
    "print(f\"{'Feature':30s} {'Type':12s} {'IV':>8s}  {'Bins':>5s}  Status\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for col in woe_categorical + woe_numerical:\n",
    "    if col not in train.columns:\n",
    "        print(f\"{col:30s} {'skip':12s} {'N/A':>8s}  {'N/A':>5s}  Column not found\")\n",
    "        continue\n",
    "\n",
    "    dtype = \"categorical\" if col in woe_categorical else \"numerical\"\n",
    "    try:\n",
    "        optb = OptimalBinning(\n",
    "            name=col,\n",
    "            dtype=dtype,\n",
    "            solver=\"cp\",\n",
    "            monotonic_trend=\"auto\",\n",
    "            min_bin_size=0.02,\n",
    "        )\n",
    "\n",
    "        x_vals = train[col].values\n",
    "        if dtype == \"numerical\":\n",
    "            x_vals = pd.to_numeric(pd.Series(x_vals), errors=\"coerce\").values\n",
    "\n",
    "        optb.fit(x_vals, train_target)\n",
    "\n",
    "        # Get IV\n",
    "        bt = optb.binning_table.build()\n",
    "        iv = bt[\"IV\"].values\n",
    "        total_iv = iv[:-2].sum() if len(iv) > 2 else iv.sum()\n",
    "        n_bins = len(bt) - 2  # exclude Totals and Special\n",
    "\n",
    "        woe_results[col] = optb\n",
    "        iv_scores[col] = total_iv\n",
    "\n",
    "        status = \"OK\"\n",
    "        if total_iv > 0.5:\n",
    "            status = \"WARNING: high IV\"\n",
    "        elif total_iv < 0.02:\n",
    "            status = \"Weak predictor\"\n",
    "\n",
    "        print(f\"{col:30s} {dtype:12s} {total_iv:8.4f}  {n_bins:5d}  {status}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"{col:30s} {dtype:12s} {'ERROR':>8s}  {'---':>5s}  {str(e)[:40]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show binning tables for top 3 WOE features\n",
    "top_woe = sorted(iv_scores, key=iv_scores.get, reverse=True)[:3]\n",
    "\n",
    "for col in top_woe:\n",
    "    optb = woe_results[col]\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Binning Table: {col} (IV = {iv_scores[col]:.4f})\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "    bt = optb.binning_table.build()\n",
    "    print(bt.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize binning tables for top features\n",
    "top6 = sorted(iv_scores, key=iv_scores.get, reverse=True)[:6]\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(top6):\n",
    "    optb = woe_results[col]\n",
    "    try:\n",
    "        optb.binning_table.plot(metric=\"woe\", ax=axes[i])\n",
    "        axes[i].set_title(f\"{col}\\n(IV={iv_scores[col]:.4f})\", fontweight=\"bold\")\n",
    "    except Exception:\n",
    "        bt = optb.binning_table.build()\n",
    "        woe_vals = bt[\"WoE\"].values[:-2]\n",
    "        axes[i].barh(range(len(woe_vals)), woe_vals, color=\"#3498db\")\n",
    "        axes[i].set_title(f\"{col} (IV={iv_scores[col]:.4f})\", fontweight=\"bold\")\n",
    "        axes[i].set_xlabel(\"WoE\")\n",
    "\n",
    "plt.suptitle(\"WOE Binning — Top 6 Features by IV\", fontweight=\"bold\", fontsize=15, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply WOE transformation to all datasets\n",
    "def apply_woe_transform(df, woe_results):\n",
    "    \"\"\"Apply fitted WOE encoders to a DataFrame.\"\"\"\n",
    "    df = df.copy()\n",
    "    for col, optb in woe_results.items():\n",
    "        try:\n",
    "            dtype = \"categorical\" if optb.dtype == \"categorical\" else \"numerical\"\n",
    "            x_vals = df[col].values\n",
    "            if dtype == \"numerical\":\n",
    "                x_vals = pd.to_numeric(pd.Series(x_vals), errors=\"coerce\").values\n",
    "            df[f\"{col}_woe\"] = optb.transform(x_vals, metric=\"woe\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"WOE transform failed for {col}: {e}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "train = apply_woe_transform(train, woe_results)\n",
    "cal = apply_woe_transform(cal, woe_results)\n",
    "test = apply_woe_transform(test, woe_results)\n",
    "\n",
    "woe_cols = [c for c in train.columns if c.endswith(\"_woe\")]\n",
    "print(f\"WOE columns created: {len(woe_cols)}\")\n",
    "print(woe_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Information Value (IV) Ranking\n",
    "\n",
    "IV summarizes the predictive power of each feature. We use it to select the most discriminative features for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IV ranking visualization\n",
    "iv_df = pd.DataFrame.from_dict(iv_scores, orient=\"index\", columns=[\"IV\"]).sort_values(\n",
    "    \"IV\", ascending=True\n",
    ")\n",
    "\n",
    "# Categorize IV strength\n",
    "iv_df[\"strength\"] = pd.cut(\n",
    "    iv_df[\"IV\"],\n",
    "    bins=[-np.inf, 0.02, 0.1, 0.3, 0.5, np.inf],\n",
    "    labels=[\"Useless\", \"Weak\", \"Medium\", \"Strong\", \"Suspicious\"],\n",
    ")\n",
    "\n",
    "color_map = {\n",
    "    \"Useless\": \"#95a5a6\",\n",
    "    \"Weak\": \"#f39c12\",\n",
    "    \"Medium\": \"#2ecc71\",\n",
    "    \"Strong\": \"#3498db\",\n",
    "    \"Suspicious\": \"#e74c3c\",\n",
    "}\n",
    "colors = [color_map[s] for s in iv_df[\"strength\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, max(8, len(iv_df) * 0.35)))\n",
    "bars = ax.barh(iv_df.index, iv_df[\"IV\"], color=colors, edgecolor=\"white\")\n",
    "\n",
    "for bar, iv_val in zip(bars, iv_df[\"IV\"], strict=False):\n",
    "    ax.text(\n",
    "        bar.get_width() + 0.005,\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f\"{iv_val:.4f}\",\n",
    "        va=\"center\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "# Threshold lines\n",
    "ax.axvline(0.02, color=\"#f39c12\", ls=\"--\", alpha=0.5, label=\"Weak (0.02)\")\n",
    "ax.axvline(0.1, color=\"#2ecc71\", ls=\"--\", alpha=0.5, label=\"Medium (0.1)\")\n",
    "ax.axvline(0.3, color=\"#3498db\", ls=\"--\", alpha=0.5, label=\"Strong (0.3)\")\n",
    "ax.axvline(0.5, color=\"#e74c3c\", ls=\"--\", alpha=0.5, label=\"Suspicious (0.5)\")\n",
    "\n",
    "ax.set_xlabel(\"Information Value (IV)\")\n",
    "ax.set_title(\"Feature Predictive Power — Information Value Ranking\", fontweight=\"bold\", fontsize=14)\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table\n",
    "print(\"\\nIV Summary by Strength Category:\")\n",
    "print(\n",
    "    iv_df.groupby(\"strength\", observed=True)\n",
    "    .agg(count=(\"IV\", \"size\"), mean_iv=(\"IV\", \"mean\"), max_iv=(\"IV\", \"max\"))\n",
    "    .to_string()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Feature Selection — Final Modeling Set\n",
    "\n",
    "Select features based on:\n",
    "1. IV > 0.02 (at least weak predictive power)\n",
    "2. No data leakage (only pre-origination variables)\n",
    "3. Manageable null rates (< 50% or handled by CatBoost)\n",
    "4. No redundant features (remove high-correlation pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define final feature sets\n",
    "# Numeric features (for CatBoost — handles NaN natively)\n",
    "NUMERIC_FEATURES = [\n",
    "    # Core financial\n",
    "    \"loan_amnt\",\n",
    "    \"int_rate\",\n",
    "    \"installment\",\n",
    "    \"annual_inc\",\n",
    "    \"dti\",\n",
    "    # Ratios\n",
    "    \"loan_to_income\",\n",
    "    \"installment_burden\",\n",
    "    \"rev_utilization\",\n",
    "    \"revol_bal_to_income\",\n",
    "    \"open_acc_ratio\",\n",
    "    # Credit history\n",
    "    \"fico_score\",\n",
    "    \"credit_age_years\",\n",
    "    \"emp_length_num\",\n",
    "    # Bureau counts\n",
    "    \"open_acc\",\n",
    "    \"total_acc\",\n",
    "    \"revol_bal\",\n",
    "    \"pub_rec\",\n",
    "    \"inq_last_6mths\",\n",
    "    \"mort_acc\",\n",
    "    # Delinquency\n",
    "    \"delinq_severity\",\n",
    "    \"delinq_recency\",\n",
    "    # Account behavior\n",
    "    \"il_ratio\",\n",
    "    \"high_util_pct\",\n",
    "    # Transforms\n",
    "    \"log_annual_inc\",\n",
    "    \"log_revol_bal\",\n",
    "    \"loan_to_income_sq\",\n",
    "    \"fico_x_dti\",\n",
    "]\n",
    "\n",
    "# Binary flags\n",
    "FLAG_FEATURES = [\n",
    "    \"has_delinq_2yrs\",\n",
    "    \"has_pub_rec\",\n",
    "    \"has_bankruptcy\",\n",
    "    \"has_recent_inq\",\n",
    "    \"has_mortgage\",\n",
    "    \"many_recent_opens\",\n",
    "    \"recent_chargeoff\",\n",
    "]\n",
    "\n",
    "# Categorical features (for CatBoost cat_features)\n",
    "CATEGORICAL_FEATURES = [\n",
    "    \"grade\",\n",
    "    \"sub_grade\",\n",
    "    \"home_ownership\",\n",
    "    \"purpose\",\n",
    "    \"verification_status\",\n",
    "    \"term\",\n",
    "    \"int_rate_bucket\",\n",
    "    \"dti_bucket\",\n",
    "    \"fico_bucket\",\n",
    "]\n",
    "\n",
    "# WOE features (for LogReg — no categoricals needed)\n",
    "WOE_FEATURES = [c for c in woe_cols if iv_scores.get(c.replace(\"_woe\", \"\"), 0) > 0.02]\n",
    "\n",
    "# Interaction features\n",
    "INTERACTION_FEATURES = [\"int_rate_bucket__grade\"]\n",
    "\n",
    "\n",
    "# Filter to only features that exist\n",
    "def filter_existing(cols, df):\n",
    "    return [c for c in cols if c in df.columns]\n",
    "\n",
    "\n",
    "num_feat = filter_existing(NUMERIC_FEATURES, train)\n",
    "flag_feat = filter_existing(FLAG_FEATURES, train)\n",
    "cat_feat = filter_existing(CATEGORICAL_FEATURES, train)\n",
    "woe_feat = filter_existing(WOE_FEATURES, train)\n",
    "int_feat = filter_existing(INTERACTION_FEATURES, train)\n",
    "\n",
    "print(\"Feature set sizes:\")\n",
    "print(f\"  Numeric:      {len(num_feat)}\")\n",
    "print(f\"  Flags:        {len(flag_feat)}\")\n",
    "print(f\"  Categorical:  {len(cat_feat)}\")\n",
    "print(f\"  WOE:          {len(woe_feat)}\")\n",
    "print(f\"  Interactions: {len(int_feat)}\")\n",
    "\n",
    "# CatBoost model features\n",
    "CATBOOST_FEATURES = num_feat + flag_feat + cat_feat + int_feat\n",
    "# LogReg model features (WOE-encoded, no categoricals)\n",
    "LOGREG_FEATURES = num_feat + flag_feat + woe_feat\n",
    "\n",
    "print(f\"\\nCatBoost total: {len(CATBOOST_FEATURES)} features\")\n",
    "print(f\"LogReg total:   {len(LOGREG_FEATURES)} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check null rates in final feature set\n",
    "null_analysis = []\n",
    "for col in CATBOOST_FEATURES:\n",
    "    null_pct = train[col].isnull().mean() * 100\n",
    "    null_analysis.append(\n",
    "        {\n",
    "            \"feature\": col,\n",
    "            \"null_pct\": null_pct,\n",
    "            \"type\": \"categorical\" if col in cat_feat else \"numeric\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "null_df = pd.DataFrame(null_analysis).sort_values(\"null_pct\", ascending=False)\n",
    "print(\"Features with >1% nulls (CatBoost handles natively):\")\n",
    "print(null_df[null_df[\"null_pct\"] > 1].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation check for multicollinearity among numeric features\n",
    "corr = train[num_feat].corr().abs()\n",
    "# Find highly correlated pairs (>0.85)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr.columns)):\n",
    "    for j in range(i + 1, len(corr.columns)):\n",
    "        if corr.iloc[i, j] > 0.85:\n",
    "            high_corr_pairs.append(\n",
    "                {\n",
    "                    \"feature_1\": corr.columns[i],\n",
    "                    \"feature_2\": corr.columns[j],\n",
    "                    \"correlation\": corr.iloc[i, j],\n",
    "                }\n",
    "            )\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"Highly correlated feature pairs (|r| > 0.85):\")\n",
    "    for pair in sorted(high_corr_pairs, key=lambda x: x[\"correlation\"], reverse=True):\n",
    "        print(f\"  {pair['feature_1']:25s} <-> {pair['feature_2']:25s}  r={pair['correlation']:.3f}\")\n",
    "    print(\n",
    "        \"\\nNote: CatBoost is robust to multicollinearity. For LogReg, consider dropping one from each pair.\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No feature pairs with |r| > 0.85 found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Save Modeling-Ready Datasets\n",
    "\n",
    "Save feature-engineered datasets with the selected feature columns + target + metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to keep for modeling\n",
    "keep_cols = list(\n",
    "    set(CATBOOST_FEATURES + LOGREG_FEATURES + [\"default_flag\", \"issue_d\", \"loan_status\", \"id\"])\n",
    ")\n",
    "keep_cols = [c for c in keep_cols if c in train.columns]\n",
    "\n",
    "train_fe = train[keep_cols].copy()\n",
    "cal_fe = cal[keep_cols].copy()\n",
    "test_fe = test[keep_cols].copy()\n",
    "\n",
    "print(\"Feature-engineered datasets:\")\n",
    "print(f\"  Train:       {train_fe.shape}\")\n",
    "print(f\"  Calibration: {cal_fe.shape}\")\n",
    "print(f\"  Test:        {test_fe.shape}\")\n",
    "\n",
    "# Save\n",
    "output_dir = DATA_DIR\n",
    "train_fe.to_parquet(output_dir / \"train_fe.parquet\", index=False)\n",
    "cal_fe.to_parquet(output_dir / \"calibration_fe.parquet\", index=False)\n",
    "test_fe.to_parquet(output_dir / \"test_fe.parquet\", index=False)\n",
    "\n",
    "print(f\"\\nSaved to {output_dir}:\")\n",
    "print(f\"  train_fe.parquet        ({train_fe.shape[0]:,} x {train_fe.shape[1]})\")\n",
    "print(f\"  calibration_fe.parquet  ({cal_fe.shape[0]:,} x {cal_fe.shape[1]})\")\n",
    "print(f\"  test_fe.parquet         ({test_fe.shape[0]:,} x {test_fe.shape[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature lists and WOE encoders for later use\n",
    "import pickle\n",
    "\n",
    "feature_config = {\n",
    "    \"NUMERIC_FEATURES\": num_feat,\n",
    "    \"FLAG_FEATURES\": flag_feat,\n",
    "    \"CATEGORICAL_FEATURES\": cat_feat,\n",
    "    \"WOE_FEATURES\": woe_feat,\n",
    "    \"INTERACTION_FEATURES\": int_feat,\n",
    "    \"CATBOOST_FEATURES\": CATBOOST_FEATURES,\n",
    "    \"LOGREG_FEATURES\": LOGREG_FEATURES,\n",
    "    \"iv_scores\": iv_scores,\n",
    "}\n",
    "\n",
    "# Save feature config\n",
    "config_path = output_dir / \"feature_config.pkl\"\n",
    "with open(config_path, \"wb\") as f:\n",
    "    pickle.dump(feature_config, f)\n",
    "print(f\"Feature config saved to {config_path}\")\n",
    "\n",
    "# Save WOE encoders\n",
    "woe_path = output_dir / \"woe_encoders.pkl\"\n",
    "with open(woe_path, \"wb\") as f:\n",
    "    pickle.dump(woe_results, f)\n",
    "print(f\"WOE encoders saved to {woe_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate with Pandera schema\n",
    "\n",
    "try:\n",
    "    loan_master_schema.validate(train_fe, lazy=True)\n",
    "    print(\"Pandera validation PASSED for train_fe\")\n",
    "except Exception as e:\n",
    "    print(f\"Pandera validation issues (non-blocking):\\n{e}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n{'=' * 60}\")\n",
    "print(\"FEATURE ENGINEERING COMPLETE\")\n",
    "print(f\"{'=' * 60}\")\n",
    "print(f\"Total features: {len(keep_cols) - 3} (excluding id, issue_d, loan_status)\")\n",
    "print(f\"WOE features: {len(woe_feat)} (IV > 0.02)\")\n",
    "print(f\"IV range: [{min(iv_scores.values()):.4f}, {max(iv_scores.values()):.4f}]\")\n",
    "print(\"Top 5 features by IV:\")\n",
    "for col in sorted(iv_scores, key=iv_scores.get, reverse=True)[:5]:\n",
    "    print(f\"  {col:30s} IV={iv_scores[col]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary\n",
    "\n",
    "### Features Created\n",
    "\n",
    "| Category | Count | Examples |\n",
    "|----------|-------|---------|\n",
    "| Financial Ratios | 6+ | loan_to_income, installment_burden, rev_utilization, revol_bal_to_income |\n",
    "| Credit History | 8+ | fico_score, credit_age_years, delinq_severity, delinq_recency |\n",
    "| Binary Flags | 7 | has_delinq_2yrs, has_bankruptcy, has_mortgage, high_inq_flag |\n",
    "| Account Behavior | 5+ | il_ratio, active_rev_ratio, high_util_pct |\n",
    "| Buckets | 5 | int_rate_bucket, dti_bucket, fico_bucket, loan_amnt_bucket, credit_age_bucket |\n",
    "| WOE Encoded | 14+ | grade_woe, purpose_woe, int_rate_woe, fico_score_woe |\n",
    "| Interactions | 3 | int_rate_bucket__grade, loan_to_income_sq, fico_x_dti |\n",
    "| Log Transforms | 3 | log_annual_inc, log_revol_bal, log_loan_amnt |\n",
    "\n",
    "### Key Decisions\n",
    "- **WOE fitted on training set only** — prevents data leakage\n",
    "- **Two feature sets**: CatBoost (raw categoricals) vs LogReg (WOE-encoded)\n",
    "- **No imputation for CatBoost** — handles NaN natively\n",
    "- **emp_length parsed to numeric** — enables continuous modeling\n",
    "- **FICO mid-point** — combines low/high range for single score\n",
    "\n",
    "### Artifacts Saved\n",
    "- `train_fe.parquet`, `calibration_fe.parquet`, `test_fe.parquet` — modeling-ready\n",
    "- `feature_config.pkl` — feature lists and IV scores\n",
    "- `woe_encoders.pkl` — fitted OptBinning objects for production scoring\n",
    "\n",
    "### Next Steps\n",
    "1. **Notebook 03**: PD Modeling — LogReg baseline + CatBoost + Optuna HPO\n",
    "2. **Notebook 04**: Conformal Prediction — MAPIE intervals with coverage validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": [
    "## Final Conclusions: Feature Engineering\n",
    "\n",
    "### Key Findings\n",
    "- Engineered features add economically meaningful structure (ratios, buckets, interactions, temporal proxies).\n",
    "- WOE-style transformations improve monotonic interpretability for risk-driven variables.\n",
    "- Feature set design balances predictive signal with governance-friendly explainability.\n",
    "\n",
    "### Financial Risk Interpretation\n",
    "- Borrower affordability, utilization, and delinquency history act as consistent risk amplifiers.\n",
    "- Structured bins and interactions convert noisy raw fields into stable risk gradients.\n",
    "- Feature transparency supports policy setting, auditability, and model-risk review.\n",
    "\n",
    "### Contribution to End-to-End Pipeline\n",
    "- Produces `train_fe`, `calibration_fe`, and `test_fe`, the canonical modeling inputs.\n",
    "- Enables stable PD training, calibration, conformal uncertainty estimation, and optimization constraints.\n",
    "- Anchors reproducibility through `feature_config.pkl` so every stage uses the same contract."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lending-club-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
