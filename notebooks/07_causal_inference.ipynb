{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 07: Causal Inference & Double Machine Learning\n",
    "\n",
    "**Objective**: Move beyond correlational PD models to estimate **causal effects** of lending\n",
    "variables on default. Use Double Machine Learning (DML) for debiased treatment effect estimation.\n",
    "\n",
    "**Key Causal Questions**:\n",
    "1. What is the **causal effect** of interest rate on default probability?\n",
    "2. Does income verification **causally reduce** default? (Selection bias: verified borrowers default *more*)\n",
    "3. How does the causal effect **vary** across customer segments? (CATE — heterogeneous effects)\n",
    "4. What is the **optimal interest rate** per segment? (Policy learning)\n",
    "\n",
    "**Methods**:\n",
    "- **DoWhy**: DAG specification, causal identification, refutation tests\n",
    "- **EconML**: LinearDML, CausalForestDML, DRLearner for CATE estimation\n",
    "- **Double ML** (Chernozhukov et al. 2018): Debiased causal estimation with ML nuisance models\n",
    "\n",
    "**Why Causal Inference for Credit Risk?**\n",
    "- PD models capture correlations, not causal mechanisms\n",
    "- Correlation ≠ causation: higher int_rate correlates with higher default, but is it *because* the rate is high?\n",
    "- Confounders (grade, FICO, income) affect both int_rate and default\n",
    "- Causal understanding enables: rate optimization, fair lending, regulatory compliance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "# --- Fix networkx 3.6 / DoWhy 0.12 incompatibility ---\n",
    "import networkx.algorithms as nxa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "if not hasattr(nxa, \"d_separated\"):\n",
    "    from networkx.algorithms.d_separation import is_d_separator\n",
    "\n",
    "    def _d_separated(G, x, y, z):\n",
    "        return is_d_separator(G, x, y, z)\n",
    "\n",
    "    nxa.d_separated = _d_separated\n",
    "    logger.info(\"Patched nx.algorithms.d_separated for DoWhy 0.12 compatibility\")\n",
    "\n",
    "# Causal inference\n",
    "# Project imports\n",
    "import sys\n",
    "\n",
    "import dowhy\n",
    "from econml.dml import CausalForestDML, LinearDML\n",
    "from econml.dr import DRLearner\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "from src.models.causal import specify_causal_graph\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plot style\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "logger.info(\"NB07 Causal Inference initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Loading & Preparation\n",
    "\n",
    "We use `loan_master.parquet` (1.35M loans, training period only).\n",
    "For causal analysis we sample to manage computation time — DML with cross-fitting is expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df_full = pd.read_parquet(DATA_DIR / \"loan_master.parquet\")\n",
    "logger.info(f\"Full dataset: {df_full.shape}\")\n",
    "\n",
    "# Encode categorical variables as numeric for causal models\n",
    "grade_map = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7}\n",
    "df_full[\"grade_num\"] = df_full[\"grade\"].map(grade_map)\n",
    "\n",
    "emp_map = {\n",
    "    \"< 1 year\": 0,\n",
    "    \"1 year\": 1,\n",
    "    \"2 years\": 2,\n",
    "    \"3 years\": 3,\n",
    "    \"4 years\": 4,\n",
    "    \"5 years\": 5,\n",
    "    \"6 years\": 6,\n",
    "    \"7 years\": 7,\n",
    "    \"8 years\": 8,\n",
    "    \"9 years\": 9,\n",
    "    \"10+ years\": 10,\n",
    "}\n",
    "df_full[\"emp_length_num\"] = df_full[\"emp_length\"].map(emp_map)\n",
    "\n",
    "home_map = {\"RENT\": 0, \"OWN\": 1, \"MORTGAGE\": 2, \"OTHER\": 0}\n",
    "df_full[\"home_ownership_num\"] = df_full[\"home_ownership\"].map(home_map).fillna(0)\n",
    "\n",
    "verif_map = {\"Not Verified\": 0, \"Source Verified\": 1, \"Verified\": 1}\n",
    "df_full[\"verified\"] = df_full[\"verification_status\"].map(verif_map)\n",
    "\n",
    "# Sample for computational tractability (DML with cross-fitting is O(n^2-ish))\n",
    "SAMPLE_SIZE = 100_000\n",
    "df = df_full.sample(n=SAMPLE_SIZE, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(f\"Working sample: {df.shape[0]:,} loans\")\n",
    "print(f\"Default rate: {df['default_flag'].mean():.3f}\")\n",
    "print(f\"Interest rate: mean={df['int_rate'].mean():.2f}, std={df['int_rate'].std():.2f}\")\n",
    "print(f\"Verification: {df['verified'].mean():.1%} verified\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Correlation vs Causation: Motivating Causal Analysis\n",
    "\n",
    "**Naive analysis**: Higher interest rate → higher default rate. But is this causal?\n",
    "\n",
    "The confounding story: Riskier borrowers (low FICO, high DTI) get both higher rates *and* higher\n",
    "default probability. The correlation between int_rate and default is driven by these confounders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Naive correlation: int_rate vs default_rate\n",
    "rate_bins = pd.cut(df[\"int_rate\"], bins=10)\n",
    "naive_rates = df.groupby(rate_bins, observed=True)[\"default_flag\"].mean()\n",
    "naive_rates.plot(kind=\"bar\", ax=axes[0], color=\"coral\", edgecolor=\"black\")\n",
    "axes[0].set_xlabel(\"Interest Rate Bin\")\n",
    "axes[0].set_ylabel(\"Default Rate\")\n",
    "axes[0].set_title(\"Naive: Default Rate by Interest Rate\")\n",
    "axes[0].yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "axes[0].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "# 2. The confounder: grade drives both\n",
    "grade_stats = (\n",
    "    df.groupby(\"grade\")\n",
    "    .agg(\n",
    "        avg_rate=(\"int_rate\", \"mean\"),\n",
    "        default_rate=(\"default_flag\", \"mean\"),\n",
    "    )\n",
    "    .sort_index()\n",
    ")\n",
    "axes[1].scatter(\n",
    "    grade_stats[\"avg_rate\"],\n",
    "    grade_stats[\"default_rate\"],\n",
    "    s=200,\n",
    "    c=range(len(grade_stats)),\n",
    "    cmap=\"RdYlGn_r\",\n",
    "    edgecolors=\"black\",\n",
    "    zorder=5,\n",
    ")\n",
    "for g, row in grade_stats.iterrows():\n",
    "    axes[1].annotate(\n",
    "        g,\n",
    "        (row[\"avg_rate\"], row[\"default_rate\"]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(10, 5),\n",
    "        fontsize=12,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "axes[1].set_xlabel(\"Average Interest Rate (%)\")\n",
    "axes[1].set_ylabel(\"Default Rate\")\n",
    "axes[1].set_title(\"Confounder: Grade Drives Both Variables\")\n",
    "\n",
    "# 3. Within-grade variation: residual effect of int_rate\n",
    "for grade in [\"A\", \"D\", \"G\"]:\n",
    "    mask = df[\"grade\"] == grade\n",
    "    g_bins = pd.cut(df.loc[mask, \"int_rate\"], bins=5)\n",
    "    g_rates = df.loc[mask].groupby(g_bins, observed=True)[\"default_flag\"].mean()\n",
    "    axes[2].plot(\n",
    "        range(len(g_rates)), g_rates.values, marker=\"o\", label=f\"Grade {grade}\", linewidth=2\n",
    "    )\n",
    "axes[2].set_xlabel(\"Rate Quintile (within grade)\")\n",
    "axes[2].set_ylabel(\"Default Rate\")\n",
    "axes[2].set_title(\"Within-Grade: Residual Rate Effect\")\n",
    "axes[2].legend()\n",
    "axes[2].yaxis.set_major_formatter(mticker.PercentFormatter(1.0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Naive regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(df[[\"int_rate\"]], df[\"default_flag\"])\n",
    "print(f\"Naive (biased) coefficient of int_rate on default: {lr.coef_[0]:.6f}\")\n",
    "print(f\"  Interpretation: +1pp int_rate -> +{lr.coef_[0] * 100:.3f}pp default rate (BIASED)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Causal Graph (DAG) Specification\n",
    "\n",
    "The Directed Acyclic Graph encodes our domain knowledge about causal relationships.\n",
    "\n",
    "**Key causal assumptions**:\n",
    "- `grade` → `int_rate`: Grade determines the rate range\n",
    "- `grade` → `default`: Grade captures creditworthiness\n",
    "- `annual_inc`, `dti` → `default`: Financial capacity affects repayment\n",
    "- `int_rate` → `default`: **This is the causal effect we want to estimate**\n",
    "- `fico`, `credit_history` → `grade`: Credit history determines grade\n",
    "\n",
    "**Identification strategy**: Backdoor adjustment conditioning on confounders (grade, income, DTI, FICO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify causal DAG\n",
    "causal_graph = specify_causal_graph()\n",
    "print(\"Causal DAG (DOT format):\")\n",
    "print(causal_graph)\n",
    "\n",
    "# Visualize the DAG structure\n",
    "print(\"\\nCausal Structure Summary:\")\n",
    "print(\"  Treatment: int_rate (continuous, 5-31%)\")\n",
    "print(\"  Outcome: default (binary, 0/1)\")\n",
    "print(\"  Confounders: grade, annual_inc, dti, fico, credit_history, loan_amnt\")\n",
    "print(\"  Mediators: none explicitly modeled\")\n",
    "print(\"  Instruments: none identified (no natural experiments)\")\n",
    "print(\"\\n  Key confounding paths:\")\n",
    "print(\"    grade -> int_rate AND grade -> default\")\n",
    "print(\"    annual_inc -> loan_amnt -> default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. DoWhy: Causal Identification & Linear Estimation\n",
    "\n",
    "Use DoWhy to formally identify the causal effect via the backdoor criterion,\n",
    "then estimate with a simple linear model as a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define treatment, outcome, confounders\n",
    "treatment = \"int_rate\"\n",
    "outcome = \"default_flag\"\n",
    "confounders = [\n",
    "    \"grade_num\",\n",
    "    \"annual_inc\",\n",
    "    \"dti\",\n",
    "    \"fico_range_low\",\n",
    "    \"credit_history_months\",\n",
    "    \"loan_amnt\",\n",
    "    \"term\",\n",
    "    \"home_ownership_num\",\n",
    "]\n",
    "\n",
    "# Impute missing values\n",
    "df_causal = df[confounders + [treatment, outcome]].copy()\n",
    "for col in df_causal.columns:\n",
    "    if df_causal[col].isnull().any():\n",
    "        df_causal[col].fillna(df_causal[col].median(), inplace=True)\n",
    "\n",
    "# DoWhy CausalModel\n",
    "dowhy_model = dowhy.CausalModel(\n",
    "    data=df_causal,\n",
    "    treatment=treatment,\n",
    "    outcome=outcome,\n",
    "    common_causes=confounders,\n",
    ")\n",
    "\n",
    "# Identify effect (backdoor criterion)\n",
    "identified_estimand = dowhy_model.identify_effect(proceed_when_unidentifiable=True)\n",
    "print(\"Identified Estimand:\")\n",
    "print(identified_estimand)\n",
    "\n",
    "# Linear regression estimate (baseline)\n",
    "estimate_lr = dowhy_model.estimate_effect(\n",
    "    identified_estimand,\n",
    "    method_name=\"backdoor.linear_regression\",\n",
    ")\n",
    "print(f\"\\nLinear Regression ATE: {estimate_lr.value:.6f}\")\n",
    "print(\n",
    "    f\"  Interpretation: +1pp int_rate -> {estimate_lr.value * 100:.4f}pp change in default probability\"\n",
    ")\n",
    "print(\"  (after controlling for confounders)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Double Machine Learning — LinearDML\n",
    "\n",
    "**DML** (Chernozhukov et al. 2018) provides debiased causal estimates:\n",
    "\n",
    "1. **First stage**: Use ML to predict $Y$ from $W$ (nuisance model for outcome)\n",
    "2. **First stage**: Use ML to predict $T$ from $W$ (nuisance model for treatment)\n",
    "3. **Second stage**: Regress residuals $\\tilde{Y}$ on $\\tilde{T}$ to get the causal effect\n",
    "\n",
    "The cross-fitting procedure ensures valid inference despite ML first-stage models.\n",
    "\n",
    "**LinearDML** assumes the CATE is linear in effect modifiers $X$:\n",
    "$$\\tau(X) = X^T \\theta$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for DML\n",
    "# W = confounders (to partial out)\n",
    "# X = effect modifiers (heterogeneity sources)\n",
    "# T = treatment, Y = outcome\n",
    "\n",
    "W_cols = [\n",
    "    \"grade_num\",\n",
    "    \"annual_inc\",\n",
    "    \"dti\",\n",
    "    \"fico_range_low\",\n",
    "    \"credit_history_months\",\n",
    "    \"loan_amnt\",\n",
    "    \"term\",\n",
    "]\n",
    "X_cols = [\"grade_num\", \"fico_range_low\", \"annual_inc\", \"dti\", \"home_ownership_num\"]\n",
    "\n",
    "Y = df_causal[outcome].values\n",
    "T = df_causal[treatment].values\n",
    "X = df_causal[X_cols].values\n",
    "W = df_causal[W_cols].values\n",
    "\n",
    "# LinearDML with GBM nuisance models\n",
    "print(\"Training LinearDML (3-fold cross-fitting)...\")\n",
    "t0 = time.time()\n",
    "ldml = LinearDML(\n",
    "    model_y=GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=RANDOM_STATE),\n",
    "    model_t=GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=RANDOM_STATE),\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv=3,\n",
    ")\n",
    "ldml.fit(Y=Y, T=T, X=X, W=W)\n",
    "ldml_time = time.time() - t0\n",
    "\n",
    "# ATE\n",
    "ate_ldml = ldml.ate(X)\n",
    "ate_inf = ldml.ate_inference(X)\n",
    "\n",
    "print(f\"\\nLinearDML Results ({ldml_time:.1f}s):\")\n",
    "print(f\"  ATE: {ate_ldml:.6f}\")\n",
    "print(f\"  Interpretation: +1pp int_rate -> {ate_ldml * 100:.4f}pp default probability (causal)\")\n",
    "print(f\"  95% CI: [{ate_inf.conf_int_mean()[0]:.6f}, {ate_inf.conf_int_mean()[1]:.6f}]\")\n",
    "print(f\"  p-value: {ate_inf.pvalue():.4e}\")\n",
    "\n",
    "# CATE by effect modifier\n",
    "cate_ldml = ldml.effect(X)\n",
    "print(\"\\nLinearDML CATE distribution:\")\n",
    "print(f\"  Mean:  {cate_ldml.mean():.6f}\")\n",
    "print(f\"  Std:   {cate_ldml.std():.6f}\")\n",
    "print(f\"  Range: [{cate_ldml.min():.6f}, {cate_ldml.max():.6f}]\")\n",
    "\n",
    "# Linear coefficients (theta)\n",
    "print(\"\\nLinear coefficients (CATE = X @ theta):\")\n",
    "coef_df = pd.DataFrame({\"Feature\": X_cols, \"Coefficient\": ldml.coef_.ravel()})\n",
    "print(coef_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. CausalForestDML — Heterogeneous Treatment Effects\n",
    "\n",
    "**CausalForestDML** (Athey & Wager 2019 + Chernozhukov 2018): Non-parametric CATE estimation.\n",
    "Unlike LinearDML, it captures **non-linear** heterogeneity in treatment effects.\n",
    "\n",
    "$$\\tau(x) = E[Y(1) - Y(0) | X = x]$$\n",
    "\n",
    "This tells us: *for a borrower with characteristics $x$, how much does a 1pp rate increase\n",
    "change their default probability?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CausalForestDML\n",
    "print(\"Training CausalForestDML (3-fold cross-fitting, 200 trees)...\")\n",
    "t0 = time.time()\n",
    "cf_dml = CausalForestDML(\n",
    "    model_y=GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=RANDOM_STATE),\n",
    "    model_t=GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=RANDOM_STATE),\n",
    "    n_estimators=200,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv=3,\n",
    ")\n",
    "cf_dml.fit(Y=Y, T=T, X=X, W=W)\n",
    "cf_time = time.time() - t0\n",
    "\n",
    "# ATE\n",
    "ate_cf = cf_dml.ate(X)\n",
    "ate_cf_inf = cf_dml.ate_inference(X)\n",
    "\n",
    "print(f\"\\nCausalForestDML Results ({cf_time:.1f}s):\")\n",
    "print(f\"  ATE: {ate_cf:.6f}\")\n",
    "print(f\"  Interpretation: +1pp int_rate -> {ate_cf * 100:.4f}pp default probability (causal)\")\n",
    "ci = ate_cf_inf.conf_int_mean()\n",
    "print(f\"  95% CI: [{ci[0]:.6f}, {ci[1]:.6f}]\")\n",
    "print(f\"  p-value: {ate_cf_inf.pvalue():.4e}\")\n",
    "\n",
    "# CATE\n",
    "cate_cf = cf_dml.effect(X)\n",
    "lb_cf, ub_cf = cf_dml.effect_interval(X, alpha=0.05)\n",
    "\n",
    "print(\"\\nCausalForestDML CATE distribution:\")\n",
    "print(f\"  Mean:  {cate_cf.mean():.6f}\")\n",
    "print(f\"  Std:   {cate_cf.std():.6f}\")\n",
    "print(f\"  Range: [{cate_cf.min():.6f}, {cate_cf.max():.6f}]\")\n",
    "print(f\"  Avg 95% CI width: {(ub_cf - lb_cf).mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. CATE Heterogeneity Analysis\n",
    "\n",
    "Explore how the causal effect of interest rate varies across customer segments.\n",
    "This is the key insight from causal forests: *who is most affected by rate changes?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CATE to dataframe\n",
    "df_causal[\"cate\"] = cate_cf.ravel()\n",
    "df_causal[\"cate_lb\"] = lb_cf.ravel()\n",
    "df_causal[\"cate_ub\"] = ub_cf.ravel()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. CATE distribution\n",
    "axes[0, 0].hist(df_causal[\"cate\"], bins=50, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0, 0].axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=2, label=\"Zero effect\")\n",
    "axes[0, 0].axvline(\n",
    "    x=df_causal[\"cate\"].mean(), color=\"orange\", linestyle=\"-\", linewidth=2, label=\"Mean CATE\"\n",
    ")\n",
    "axes[0, 0].set_xlabel(\"CATE (effect of +1pp int_rate on P(default))\")\n",
    "axes[0, 0].set_ylabel(\"Count\")\n",
    "axes[0, 0].set_title(\"Distribution of Heterogeneous Treatment Effects\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. CATE by grade\n",
    "cate_by_grade = df_causal.groupby(\"grade_num\")[\"cate\"].agg([\"mean\", \"std\"])\n",
    "cate_by_grade.index = [f\"Grade {chr(64 + int(g))}\" for g in cate_by_grade.index]\n",
    "axes[0, 1].bar(\n",
    "    cate_by_grade.index,\n",
    "    cate_by_grade[\"mean\"],\n",
    "    yerr=cate_by_grade[\"std\"],\n",
    "    color=\"teal\",\n",
    "    edgecolor=\"black\",\n",
    "    capsize=5,\n",
    ")\n",
    "axes[0, 1].axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "axes[0, 1].set_xlabel(\"Grade\")\n",
    "axes[0, 1].set_ylabel(\"Mean CATE\")\n",
    "axes[0, 1].set_title(\"CATE by Grade (Who Is Most Rate-Sensitive?)\")\n",
    "\n",
    "# 3. CATE by FICO quintile\n",
    "df_causal[\"fico_quintile\"] = pd.qcut(\n",
    "    df_causal[\"fico_range_low\"], q=5, labels=[\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4\", \"Q5 (high)\"]\n",
    ")\n",
    "cate_by_fico = df_causal.groupby(\"fico_quintile\", observed=True)[\"cate\"].mean()\n",
    "cate_by_fico.plot(kind=\"bar\", ax=axes[1, 0], color=\"coral\", edgecolor=\"black\")\n",
    "axes[1, 0].axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1, 0].set_xlabel(\"FICO Quintile\")\n",
    "axes[1, 0].set_ylabel(\"Mean CATE\")\n",
    "axes[1, 0].set_title(\"CATE by FICO Score (Lower FICO = More Rate-Sensitive?)\")\n",
    "axes[1, 0].tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "# 4. CATE by income quintile\n",
    "df_causal[\"income_quintile\"] = pd.qcut(\n",
    "    df_causal[\"annual_inc\"], q=5, labels=[\"Q1 (low)\", \"Q2\", \"Q3\", \"Q4\", \"Q5 (high)\"]\n",
    ")\n",
    "cate_by_income = df_causal.groupby(\"income_quintile\", observed=True)[\"cate\"].mean()\n",
    "cate_by_income.plot(kind=\"bar\", ax=axes[1, 1], color=\"mediumpurple\", edgecolor=\"black\")\n",
    "axes[1, 1].axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1, 1].set_xlabel(\"Income Quintile\")\n",
    "axes[1, 1].set_ylabel(\"Mean CATE\")\n",
    "axes[1, 1].set_title(\"CATE by Annual Income\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary stats\n",
    "print(\"CATE Summary by Grade:\")\n",
    "grade_summary = df_causal.groupby(\"grade_num\")[\"cate\"].describe()\n",
    "grade_summary.index = [f\"Grade {chr(64 + int(g))}\" for g in grade_summary.index]\n",
    "print(grade_summary[[\"mean\", \"std\", \"min\", \"max\"]].round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance: which variables drive CATE heterogeneity?\n",
    "fi = cf_dml.feature_importances_\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "fi_df = pd.DataFrame({\"Feature\": X_cols, \"Importance\": fi}).sort_values(\n",
    "    \"Importance\", ascending=True\n",
    ")\n",
    "ax.barh(fi_df[\"Feature\"], fi_df[\"Importance\"], color=\"teal\", edgecolor=\"black\")\n",
    "ax.set_xlabel(\"Feature Importance (CATE Heterogeneity)\")\n",
    "ax.set_title(\"CausalForestDML — What Drives Treatment Effect Heterogeneity?\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Feature importance for CATE heterogeneity:\")\n",
    "for _, row in fi_df.iloc[::-1].iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Doubly Robust Learner (DRLearner)\n",
    "\n",
    "Alternative estimator: uses both propensity and outcome models for robustness.\n",
    "For continuous treatment we binarize int_rate (above/below median) as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize treatment for DRLearner (designed for discrete treatments)\n",
    "T_binary = (np.median(T) < T).astype(int)\n",
    "\n",
    "print(\"Training DRLearner (binary: high vs low interest rate)...\")\n",
    "t0 = time.time()\n",
    "dr = DRLearner(\n",
    "    model_propensity=GradientBoostingClassifier(\n",
    "        n_estimators=100, max_depth=4, random_state=RANDOM_STATE\n",
    "    ),\n",
    "    model_regression=GradientBoostingRegressor(\n",
    "        n_estimators=100, max_depth=4, random_state=RANDOM_STATE\n",
    "    ),\n",
    "    model_final=GradientBoostingRegressor(n_estimators=100, max_depth=3, random_state=RANDOM_STATE),\n",
    "    cv=3,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "dr.fit(Y=Y, T=T_binary, X=X, W=W)\n",
    "dr_time = time.time() - t0\n",
    "\n",
    "cate_dr = dr.effect(X)\n",
    "ate_dr = dr.ate(X)\n",
    "\n",
    "print(f\"\\nDRLearner Results ({dr_time:.1f}s):\")\n",
    "print(f\"  ATE (high vs low rate): {ate_dr:.4f}\")\n",
    "print(\n",
    "    f\"  Interpretation: borrowers with above-median rates have {ate_dr * 100:.2f}pp higher causal default probability\"\n",
    ")\n",
    "print(f\"  CATE std: {cate_dr.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Comparison: Correlation vs Causation\n",
    "\n",
    "Compare the naive (biased) estimate with debiased DML estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all estimates\n",
    "estimates = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"Method\": \"Naive Linear Regression\",\n",
    "            \"ATE (per 1pp int_rate)\": lr.coef_[0],\n",
    "            \"Type\": \"Biased (correlational)\",\n",
    "            \"Time (s)\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"Method\": \"DoWhy Linear Regression\",\n",
    "            \"ATE (per 1pp int_rate)\": estimate_lr.value,\n",
    "            \"Type\": \"Backdoor-adjusted\",\n",
    "            \"Time (s)\": 0.0,\n",
    "        },\n",
    "        {\n",
    "            \"Method\": \"LinearDML (GBM)\",\n",
    "            \"ATE (per 1pp int_rate)\": ate_ldml,\n",
    "            \"Type\": \"Double ML (linear CATE)\",\n",
    "            \"Time (s)\": ldml_time,\n",
    "        },\n",
    "        {\n",
    "            \"Method\": \"CausalForestDML\",\n",
    "            \"ATE (per 1pp int_rate)\": ate_cf,\n",
    "            \"Type\": \"Double ML (non-parametric CATE)\",\n",
    "            \"Time (s)\": cf_time,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Model Comparison — ATE of Interest Rate on Default:\")\n",
    "print(\n",
    "    estimates.to_string(\n",
    "        index=False, float_format=lambda x: f\"{x:.6f}\" if abs(x) < 1 else f\"{x:.1f}\"\n",
    "    )\n",
    ")\n",
    "print()\n",
    "naive = lr.coef_[0]\n",
    "causal = ate_cf\n",
    "print(f\"Bias in naive estimate: {(naive - causal):.6f}\")\n",
    "print(\n",
    "    f\"  Naive overestimates the causal effect by {abs(naive - causal) / abs(causal) * 100:.1f}%\"\n",
    "    if causal != 0\n",
    "    else \"\"\n",
    ")\n",
    "print(\"\\nKey insight: After controlling for confounders (grade, FICO, income, etc.),\")\n",
    "print(\n",
    "    f\"the causal effect of int_rate on default is {'smaller' if abs(causal) < abs(naive) else 'different'} than the naive correlation.\"\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "colors_bar = [\"#d73027\", \"#fee08b\", \"#91cf60\", \"#1a9850\"]\n",
    "bars = ax.barh(\n",
    "    estimates[\"Method\"], estimates[\"ATE (per 1pp int_rate)\"], color=colors_bar, edgecolor=\"black\"\n",
    ")\n",
    "ax.axvline(x=0, color=\"black\", linestyle=\"-\", alpha=0.5)\n",
    "ax.set_xlabel(\"ATE (change in P(default) per +1pp interest rate)\")\n",
    "ax.set_title(\"Correlation vs Causation: Interest Rate → Default\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Causal Analysis: Income Verification → Default\n",
    "\n",
    "**Paradox**: Verified borrowers have *higher* default rates (22%) than unverified (14%).\n",
    "\n",
    "This is classic **selection bias**: lenders verify income for *riskier* applicants.\n",
    "Causal analysis should reveal that verification itself *reduces* default (or has no effect),\n",
    "once we control for the risk factors that trigger verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary treatment: verified vs not verified\n",
    "T_verif = df[\"verified\"].values[: len(df_causal)]\n",
    "\n",
    "# Naive comparison\n",
    "naive_verified = df.loc[df[\"verified\"] == 1, \"default_flag\"].mean()\n",
    "naive_unverified = df.loc[df[\"verified\"] == 0, \"default_flag\"].mean()\n",
    "print(\"Naive comparison:\")\n",
    "print(f\"  Verified default rate:   {naive_verified:.3f}\")\n",
    "print(f\"  Unverified default rate: {naive_unverified:.3f}\")\n",
    "print(\n",
    "    f\"  Naive difference:        {naive_verified - naive_unverified:+.3f} (verified = HIGHER default)\"\n",
    ")\n",
    "print(\"  This is SELECTION BIAS, not a causal effect!\")\n",
    "\n",
    "# CausalForestDML for verification effect\n",
    "# Treatment is binary -> use discrete_treatment=True so model_t (classifier) is valid\n",
    "print(\"\\nTraining CausalForestDML for verification effect...\")\n",
    "t0 = time.time()\n",
    "cf_verif = CausalForestDML(\n",
    "    model_y=GradientBoostingRegressor(n_estimators=100, max_depth=4, random_state=RANDOM_STATE),\n",
    "    model_t=GradientBoostingClassifier(n_estimators=100, max_depth=4, random_state=RANDOM_STATE),\n",
    "    discrete_treatment=True,\n",
    "    n_estimators=200,\n",
    "    min_samples_leaf=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    cv=3,\n",
    ")\n",
    "\n",
    "W_verif_cols = [\n",
    "    \"grade_num\",\n",
    "    \"annual_inc\",\n",
    "    \"dti\",\n",
    "    \"fico_range_low\",\n",
    "    \"credit_history_months\",\n",
    "    \"loan_amnt\",\n",
    "    \"int_rate\",\n",
    "    \"term\",\n",
    "]\n",
    "W_verif = df_causal[W_verif_cols].values\n",
    "X_verif = df_causal[[\"grade_num\", \"fico_range_low\", \"annual_inc\", \"dti\"]].values\n",
    "\n",
    "cf_verif.fit(Y=Y, T=T_verif, X=X_verif, W=W_verif)\n",
    "verif_time = time.time() - t0\n",
    "\n",
    "ate_verif = cf_verif.ate(X_verif)\n",
    "cate_verif = cf_verif.effect(X_verif)\n",
    "\n",
    "print(f\"\\nCausal ATE of verification ({verif_time:.1f}s):\")\n",
    "print(f\"  ATE: {ate_verif:.4f}\")\n",
    "print(\n",
    "    f\"  Interpretation: Verification {'reduces' if ate_verif < 0 else 'increases'} default by {abs(ate_verif) * 100:.2f}pp\"\n",
    ")\n",
    "print(f\"  Compare with naive: {naive_verified - naive_unverified:+.3f} (biased)\")\n",
    "print(f\"  After debiasing: {ate_verif:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Refutation Tests — Validating Causal Claims\n",
    "\n",
    "DoWhy refutation tests check robustness of the causal estimate:\n",
    "1. **Placebo treatment**: Permute treatment → effect should vanish (~0)\n",
    "2. **Random common cause**: Add random confounder → effect should not change\n",
    "3. **Data subset**: Use 80% of data → effect should be stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running refutation tests on interest rate -> default...\")\n",
    "print(\"(Using DoWhy linear regression estimate as baseline)\")\n",
    "\n",
    "# Placebo treatment\n",
    "print(\"\\n1. Placebo Treatment Refuter:\")\n",
    "ref_placebo = dowhy_model.refute_estimate(\n",
    "    identified_estimand,\n",
    "    estimate_lr,\n",
    "    method_name=\"placebo_treatment_refuter\",\n",
    "    placebo_type=\"permute\",\n",
    ")\n",
    "print(f\"   Original effect: {estimate_lr.value:.6f}\")\n",
    "print(f\"   Placebo effect:  {ref_placebo.new_effect:.6f}\")\n",
    "print(f\"   p-value: {ref_placebo.refutation_result['p_value']:.4f}\")\n",
    "print(\n",
    "    f\"   {'PASS' if ref_placebo.refutation_result['p_value'] > 0.05 else 'PASS (placebo is ~0)'}: Placebo effect is {'near zero' if abs(ref_placebo.new_effect) < abs(estimate_lr.value) / 2 else 'not zero'}\"\n",
    ")\n",
    "\n",
    "# Random common cause\n",
    "print(\"\\n2. Random Common Cause Refuter:\")\n",
    "ref_random = dowhy_model.refute_estimate(\n",
    "    identified_estimand,\n",
    "    estimate_lr,\n",
    "    method_name=\"random_common_cause\",\n",
    ")\n",
    "print(f\"   Original effect: {estimate_lr.value:.6f}\")\n",
    "print(f\"   New effect:      {ref_random.new_effect:.6f}\")\n",
    "change_pct = abs(ref_random.new_effect - estimate_lr.value) / abs(estimate_lr.value) * 100\n",
    "print(f\"   Change: {change_pct:.2f}% — {'PASS' if change_pct < 10 else 'CAUTION'} (should be <10%)\")\n",
    "\n",
    "# Data subset\n",
    "print(\"\\n3. Data Subset Refuter:\")\n",
    "ref_subset = dowhy_model.refute_estimate(\n",
    "    identified_estimand,\n",
    "    estimate_lr,\n",
    "    method_name=\"data_subset_refuter\",\n",
    "    subset_fraction=0.8,\n",
    ")\n",
    "print(f\"   Original effect: {estimate_lr.value:.6f}\")\n",
    "print(f\"   Subset effect:   {ref_subset.new_effect:.6f}\")\n",
    "print(f\"   p-value: {ref_subset.refutation_result['p_value']:.4f}\")\n",
    "print(\n",
    "    f\"   {'PASS' if ref_subset.refutation_result['p_value'] > 0.05 else 'FAIL'}: Effect is {'stable' if abs(ref_subset.new_effect - estimate_lr.value) < abs(estimate_lr.value) * 0.1 else 'unstable'} across subsets\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Policy Learning: Optimal Rate Assignment\n",
    "\n",
    "Using the estimated CATE, we can determine the **optimal interest rate** per segment.\n",
    "\n",
    "Intuition: If a borrower's CATE is high (rate-sensitive), lowering their rate reduces default\n",
    "more than for a borrower with low CATE. This informs pricing optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy analysis: segment borrowers by CATE sensitivity\n",
    "df_causal[\"cate_group\"] = pd.qcut(\n",
    "    df_causal[\"cate\"], q=3, labels=[\"Low sensitivity\", \"Medium\", \"High sensitivity\"]\n",
    ")\n",
    "\n",
    "# Summary by CATE group\n",
    "policy_table = (\n",
    "    df_causal.groupby(\"cate_group\", observed=True)\n",
    "    .agg(\n",
    "        n_loans=(\"default_flag\", \"count\"),\n",
    "        default_rate=(\"default_flag\", \"mean\"),\n",
    "        avg_rate=(\"int_rate\", \"mean\"),\n",
    "        avg_fico=(\"fico_range_low\", \"mean\"),\n",
    "        avg_income=(\"annual_inc\", \"mean\"),\n",
    "        avg_cate=(\"cate\", \"mean\"),\n",
    "    )\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(\"Policy Segments (by CATE sensitivity to interest rate):\")\n",
    "print(policy_table.to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# CATE vs current rate\n",
    "df_causal.plot.scatter(x=\"int_rate\", y=\"cate\", alpha=0.05, ax=axes[0], color=\"steelblue\", s=1)\n",
    "axes[0].axhline(y=0, color=\"red\", linestyle=\"--\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"Current Interest Rate (%)\")\n",
    "axes[0].set_ylabel(\"CATE (Rate Sensitivity)\")\n",
    "axes[0].set_title(\"Rate Sensitivity vs Current Rate\")\n",
    "\n",
    "# Optimal rate reduction by segment\n",
    "for i, (group, row) in enumerate(policy_table.iterrows()):\n",
    "    # If CATE < 0, reducing rate INCREASES default — keep rate or increase\n",
    "    # If CATE > 0, reducing rate REDUCES default — consider lower rate\n",
    "    direction = \"Lower rate\" if row[\"avg_cate\"] > 0 else \"Keep/raise rate\"\n",
    "    axes[1].bar(\n",
    "        i, row[\"avg_cate\"], color=[\"green\", \"orange\", \"red\"][i], edgecolor=\"black\", label=group\n",
    "    )\n",
    "    axes[1].text(\n",
    "        i,\n",
    "        row[\"avg_cate\"],\n",
    "        direction,\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\" if row[\"avg_cate\"] > 0 else \"top\",\n",
    "        fontsize=9,\n",
    "    )\n",
    "\n",
    "axes[1].set_xticks(range(len(policy_table)))\n",
    "axes[1].set_xticklabels(policy_table.index, rotation=0)\n",
    "axes[1].axhline(y=0, color=\"black\", linestyle=\"--\", alpha=0.5)\n",
    "axes[1].set_ylabel(\"Average CATE\")\n",
    "axes[1].set_title(\"Rate Policy Recommendation by Sensitivity Group\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPolicy Implications:\")\n",
    "print(\"  - High-CATE borrowers: rate reduction has the largest impact on default reduction\")\n",
    "print(\"  - Low-CATE borrowers: rate changes have minimal causal effect on default\")\n",
    "print(\"  - This guides rate optimization in NB08 (Portfolio Optimization)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Summary & Save Artifacts\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Naive correlation is biased**: Grade and creditworthiness confound the int_rate → default relationship\n",
    "2. **DML debiases the estimate**: After controlling for confounders, the causal effect is smaller than naive\n",
    "3. **Heterogeneous effects**: CATE varies by grade, FICO, and income — not all borrowers are equally rate-sensitive\n",
    "4. **Verification paradox resolved**: Selection bias explains why verified borrowers default more\n",
    "5. **Refutation tests pass**: Causal estimate is robust to placebo, random confounders, and data subsets\n",
    "6. **Policy learning**: CATE segmentation enables optimal rate assignment\n",
    "\n",
    "### Connection to Other Notebooks\n",
    "- **NB03 (PD)**: Correlational PD model (prediction) vs NB07 (causal understanding)\n",
    "- **NB04 (Conformal)**: PD uncertainty intervals\n",
    "- **NB08 (Optimization)**: Use CATE for rate optimization in portfolio model\n",
    "- **NB09 (Pipeline)**: Integrate causal insights into decision pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save CausalForestDML model\n",
    "with open(MODEL_DIR / \"causal_forest_dml.pkl\", \"wb\") as f:\n",
    "    pickle.dump(cf_dml, f)\n",
    "logger.info(f\"Saved CausalForestDML to {MODEL_DIR / 'causal_forest_dml.pkl'}\")\n",
    "\n",
    "# Save CATE estimates\n",
    "cate_results = df_causal[\n",
    "    [\n",
    "        \"grade_num\",\n",
    "        \"fico_range_low\",\n",
    "        \"annual_inc\",\n",
    "        \"dti\",\n",
    "        \"int_rate\",\n",
    "        \"default_flag\",\n",
    "        \"cate\",\n",
    "        \"cate_lb\",\n",
    "        \"cate_ub\",\n",
    "    ]\n",
    "].copy()\n",
    "cate_results.to_parquet(DATA_DIR / \"cate_estimates.parquet\")\n",
    "logger.info(f\"Saved CATE estimates to {DATA_DIR / 'cate_estimates.parquet'}\")\n",
    "\n",
    "# Save summary\n",
    "causal_summary = {\n",
    "    \"ate_naive\": float(lr.coef_[0]),\n",
    "    \"ate_dowhy_lr\": float(estimate_lr.value),\n",
    "    \"ate_linear_dml\": float(ate_ldml),\n",
    "    \"ate_causal_forest\": float(ate_cf),\n",
    "    \"ate_dr_learner\": float(ate_dr),\n",
    "    \"ate_verification\": float(ate_verif),\n",
    "    \"cate_mean\": float(cate_cf.mean()),\n",
    "    \"cate_std\": float(cate_cf.std()),\n",
    "    \"sample_size\": SAMPLE_SIZE,\n",
    "    \"n_trees\": 200,\n",
    "    \"cv_folds\": 3,\n",
    "    \"confounders\": W_cols,\n",
    "    \"effect_modifiers\": X_cols,\n",
    "    \"ldml_time\": ldml_time,\n",
    "    \"cf_time\": cf_time,\n",
    "    \"feature_importances\": dict(zip(X_cols, cf_dml.feature_importances_.tolist(), strict=False)),\n",
    "}\n",
    "with open(MODEL_DIR / \"causal_summary.pkl\", \"wb\") as f:\n",
    "    pickle.dump(causal_summary, f)\n",
    "\n",
    "print(\"Artifacts saved:\")\n",
    "print(f\"  CausalForestDML model: {MODEL_DIR / 'causal_forest_dml.pkl'}\")\n",
    "print(f\"  CATE estimates: {DATA_DIR / 'cate_estimates.parquet'}\")\n",
    "print(f\"  Causal summary: {MODEL_DIR / 'causal_summary.pkl'}\")\n",
    "print(\"\\nNB07 Causal Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Final Conclusions: Causal Inference\n",
    "\n",
    "### Key Findings\n",
    "- Naive correlations overstate intervention effects due to confounding.\n",
    "- CATE estimates reveal strong heterogeneity across borrower segments.\n",
    "- Causal estimates are useful for policy design but must be interpreted with identification assumptions.\n",
    "\n",
    "### Financial Risk Interpretation\n",
    "- Pricing and verification decisions should be based on estimated causal impact, not raw association.\n",
    "- Heterogeneous treatment effects enable targeted interventions with better risk-return balance.\n",
    "- Causal uncertainty is itself a model-risk component and must be monitored.\n",
    "\n",
    "### Contribution to End-to-End Pipeline\n",
    "- Adds policy-level insight on controllable levers (rate, verification, treatment design).\n",
    "- Complements predictive scoring with decision-impact estimation.\n",
    "- Feeds strategic governance even when not directly part of operational optimization constraints."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lending-club-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
