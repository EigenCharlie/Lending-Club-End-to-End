{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Notebook 09: End-to-End Pipeline — Conformal Predict-then-Optimize\n",
    "\n",
    "**Objective**: Demonstrate the complete thesis pipeline on a batch of new loan applications,\n",
    "from raw data through feature engineering, PD prediction, conformal intervals, portfolio\n",
    "optimization, and IFRS 9 ECL calculation.\n",
    "\n",
    "**Pipeline**:\n",
    "```\n",
    "Raw Loan Data\n",
    "  -> Feature Engineering (WOE, interactions, buckets)\n",
    "    -> PD Model (CatBoost tuned)\n",
    "      -> Probability Calibration (Isotonic)\n",
    "        -> Conformal Prediction (MAPIE) -> [PD_low, PD_high]\n",
    "          -> IFRS 9 Staging (standard + CP-enhanced SICR)\n",
    "            -> ECL Calculation (PD x LGD x EAD)\n",
    "              -> Portfolio Optimization (Pyomo + HiGHS)\n",
    "                -> Robust Allocation (conformal uncertainty sets)\n",
    "```\n",
    "\n",
    "**Key Innovation**: Conformal prediction intervals provide **distribution-free coverage guarantees**\n",
    "that propagate through staging (enhanced SICR detection), ECL (provision ranges), and optimization\n",
    "(robust portfolio allocation).\n",
    "\n",
    "**This notebook uses pre-trained artifacts from NB01-NB08.** If artifacts are missing,\n",
    "fallback logic trains lightweight models inline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from loguru import logger\n",
    "\n",
    "sys.path.insert(0, str(Path(\"..\").resolve()))\n",
    "\n",
    "from src.evaluation.ifrs9 import assign_stage, compute_ecl, ecl_with_conformal_range\n",
    "from src.optimization.portfolio_model import build_portfolio_model, solve_portfolio\n",
    "from src.optimization.robust_opt import (\n",
    "    scenario_analysis,\n",
    ")\n",
    "\n",
    "DATA_DIR = Path(\"../data/processed\")\n",
    "MODEL_DIR = Path(\"../models\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.1)\n",
    "plt.rcParams[\"figure.figsize\"] = (14, 6)\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "logger.info(\"NB09 End-to-End Pipeline initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load Pre-Trained Artifacts\n",
    "\n",
    "Load the CatBoost PD model, calibrator, conformal intervals, and feature configuration\n",
    "from previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load CatBoost model ---\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cb_model = CatBoostClassifier()\n",
    "cb_model.load_model(str(MODEL_DIR / \"pd_catboost_tuned.cbm\"))\n",
    "logger.info(f\"Loaded CatBoost model: {cb_model.tree_count_} trees\")\n",
    "\n",
    "# --- Load calibrator ---\n",
    "calibrator = None\n",
    "cal_path = MODEL_DIR / \"pd_calibrator.pkl\"\n",
    "if cal_path.exists():\n",
    "    try:\n",
    "        with open(cal_path, \"rb\") as f:\n",
    "            calibrator = pickle.load(f)\n",
    "    except Exception:\n",
    "        import joblib\n",
    "\n",
    "        calibrator = joblib.load(cal_path)\n",
    "    logger.info(f\"Loaded calibrator: {calibrator.__class__.__name__}\")\n",
    "else:\n",
    "    logger.warning(\"No calibrator found ? using raw CatBoost probabilities\")\n",
    "\n",
    "# --- Load feature config ---\n",
    "with open(DATA_DIR / \"feature_config.pkl\", \"rb\") as f:\n",
    "    feat_config = pickle.load(f)\n",
    "features = feat_config[\"CATBOOST_FEATURES\"]\n",
    "cat_features = feat_config[\"CATEGORICAL_FEATURES\"] + feat_config.get(\"INTERACTION_FEATURES\", [])\n",
    "cat_features = [c for c in cat_features if c in features]\n",
    "\n",
    "# --- Load conformal results ---\n",
    "conformal_results = None\n",
    "cr_path = MODEL_DIR / \"conformal_results.pkl\"\n",
    "if cr_path.exists():\n",
    "    with open(cr_path, \"rb\") as f:\n",
    "        conformal_results = pickle.load(f)\n",
    "    logger.info(\"Loaded conformal results\")\n",
    "\n",
    "# --- Load test data as our 'new applications' ---\n",
    "df_test = pd.read_parquet(DATA_DIR / \"test_fe.parquet\")\n",
    "\n",
    "print(\"Artifacts loaded:\")\n",
    "print(f\"  CatBoost model: {cb_model.tree_count_} trees\")\n",
    "print(f\"  Calibrator: {calibrator.__class__.__name__ if calibrator else 'None'}\")\n",
    "print(f\"  Features: {len(features)} ({len(cat_features)} categorical)\")\n",
    "print(f\"  Test data (new applications): {df_test.shape[0]:,} loans\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Simulate New Loan Applications\n",
    "\n",
    "We take a representative sample from the test set to simulate a batch of new applications\n",
    "arriving at the credit desk. In production, this would come from the loan origination system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a batch of new loan applications\n",
    "BATCH_SIZE = 10_000\n",
    "df_apps = df_test.sample(n=min(BATCH_SIZE, len(df_test)), random_state=RANDOM_STATE).reset_index(\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "# Convert categoricals for CatBoost\n",
    "for col in cat_features:\n",
    "    if col in df_apps.columns:\n",
    "        df_apps[col] = df_apps[col].astype(str)\n",
    "\n",
    "print(f\"New Application Batch: {len(df_apps):,} loans\")\n",
    "print(f\"  Total requested: ${df_apps['loan_amnt'].sum():,.0f}\")\n",
    "print(f\"  Avg loan amount: ${df_apps['loan_amnt'].mean():,.0f}\")\n",
    "print(\"  Grade distribution:\")\n",
    "if \"grade\" in df_apps.columns:\n",
    "    print(df_apps[\"grade\"].value_counts().sort_index().to_string())\n",
    "print(f\"  Actual default rate: {df_apps['default_flag'].mean():.3f} (unknown at decision time)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Step 1 — PD Prediction & Calibration\n",
    "\n",
    "Generate point PD estimates using the tuned CatBoost model, then apply Isotonic calibration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: PD Prediction\n",
    "t0 = time.time()\n",
    "pd_raw = cb_model.predict_proba(df_apps[features])[:, 1]\n",
    "\n",
    "\n",
    "def apply_calibrator(cal_obj, scores):\n",
    "    scores = np.asarray(scores, dtype=float)\n",
    "    if cal_obj is None:\n",
    "        return np.clip(scores, 0.001, 0.999)\n",
    "\n",
    "    # IsotonicRegression\n",
    "    if hasattr(cal_obj, \"transform\"):\n",
    "        out = cal_obj.transform(scores)\n",
    "        return np.clip(np.asarray(out, dtype=float), 0.001, 0.999)\n",
    "\n",
    "    # LogisticRegression / similar sklearn classifier\n",
    "    if hasattr(cal_obj, \"predict_proba\"):\n",
    "        out = cal_obj.predict_proba(scores.reshape(-1, 1))[:, 1]\n",
    "        return np.clip(np.asarray(out, dtype=float), 0.001, 0.999)\n",
    "\n",
    "    # Generic fallback\n",
    "    try:\n",
    "        out = cal_obj.predict(scores)\n",
    "        out = np.asarray(out, dtype=float)\n",
    "        if out.shape[0] != scores.shape[0]:\n",
    "            out = np.asarray(cal_obj.predict(scores.reshape(-1, 1)), dtype=float)\n",
    "    except Exception:\n",
    "        out = np.asarray(cal_obj.predict(scores.reshape(-1, 1)), dtype=float)\n",
    "\n",
    "    return np.clip(out, 0.001, 0.999)\n",
    "\n",
    "\n",
    "pd_calibrated = apply_calibrator(calibrator, pd_raw)\n",
    "pd_time = time.time() - t0\n",
    "\n",
    "print(f\"Step 1: PD Prediction ({pd_time:.2f}s)\")\n",
    "print(f\"  Raw PD:        mean={pd_raw.mean():.4f}, median={np.median(pd_raw):.4f}\")\n",
    "print(f\"  Calibrated PD: mean={pd_calibrated.mean():.4f}, median={np.median(pd_calibrated):.4f}\")\n",
    "print(f\"  Range: [{pd_calibrated.min():.4f}, {pd_calibrated.max():.4f}]\")\n",
    "\n",
    "# Quick check: AUC on known labels (would not exist in production)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "auc = roc_auc_score(df_apps[\"default_flag\"], pd_calibrated)\n",
    "print(f\"  AUC (on known labels): {auc:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].hist(pd_calibrated, bins=50, color=\"steelblue\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[0].set_xlabel(\"Calibrated PD\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Distribution of PD Predictions\")\n",
    "axes[0].axvline(\n",
    "    pd_calibrated.mean(), color=\"red\", linestyle=\"--\", label=f\"Mean={pd_calibrated.mean():.3f}\"\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "# PD by grade\n",
    "if \"grade\" in df_apps.columns:\n",
    "    pd_grade = pd.DataFrame({\"grade\": df_apps[\"grade\"].values, \"pd\": pd_calibrated})\n",
    "    pd_grade.boxplot(column=\"pd\", by=\"grade\", ax=axes[1])\n",
    "    axes[1].set_xlabel(\"Grade\")\n",
    "    axes[1].set_ylabel(\"Calibrated PD\")\n",
    "    axes[1].set_title(\"PD by Grade\")\n",
    "    plt.suptitle(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Step 2 — Conformal Prediction Intervals\n",
    "\n",
    "Generate prediction intervals $[PD_{low}, PD_{high}]$ with **90% coverage guarantee**.\n",
    "\n",
    "These intervals quantify the **epistemic uncertainty** in each PD estimate. Wider intervals\n",
    "mean less model confidence, which feeds into robust optimization and enhanced SICR detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Conformal Prediction Intervals\n",
    "# Use pre-computed intervals from NB04 if available, otherwise simulate\n",
    "t0 = time.time()\n",
    "\n",
    "intervals_path = DATA_DIR / \"conformal_intervals_mondrian.parquet\"\n",
    "if intervals_path.exists():\n",
    "    all_intervals = pd.read_parquet(intervals_path)\n",
    "    # Map column names\n",
    "    col_point = \"y_pred\" if \"y_pred\" in all_intervals.columns else \"pd_point\"\n",
    "    col_low = \"pd_low_90\" if \"pd_low_90\" in all_intervals.columns else \"pd_low\"\n",
    "    col_high = \"pd_high_90\" if \"pd_high_90\" in all_intervals.columns else \"pd_high\"\n",
    "\n",
    "    # Sample intervals matching batch size\n",
    "    interval_sample = all_intervals.sample(n=len(df_apps), random_state=RANDOM_STATE).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    pd_low = interval_sample[col_low].values\n",
    "    pd_high = interval_sample[col_high].values\n",
    "    # Use our calibrated PD as point estimate, but clip within conformal bounds\n",
    "    pd_point = pd_calibrated\n",
    "    logger.info(\"Loaded real conformal intervals from NB04\")\n",
    "else:\n",
    "    # Simulate conformal intervals based on uncertainty heuristic\n",
    "    # Width proportional to PD*(1-PD) — higher uncertainty in middle range\n",
    "    base_width = 0.10  # 10% average width for 90% coverage\n",
    "    heteroscedastic_width = base_width * (1 + 2 * pd_calibrated * (1 - pd_calibrated))\n",
    "    pd_low = np.maximum(pd_calibrated - heteroscedastic_width / 2, 0.001)\n",
    "    pd_high = np.minimum(pd_calibrated + heteroscedastic_width / 2, 0.999)\n",
    "    pd_point = pd_calibrated\n",
    "    logger.warning(\"Using simulated conformal intervals (NB04 artifacts not found)\")\n",
    "\n",
    "cp_time = time.time() - t0\n",
    "interval_width = pd_high - pd_low\n",
    "\n",
    "print(f\"Step 2: Conformal Prediction ({cp_time:.2f}s)\")\n",
    "print(\"  Coverage target: 90%\")\n",
    "print(f\"  PD point:  mean={pd_point.mean():.4f}\")\n",
    "print(f\"  PD low:    mean={pd_low.mean():.4f}\")\n",
    "print(f\"  PD high:   mean={pd_high.mean():.4f}\")\n",
    "print(f\"  Width:     mean={interval_width.mean():.4f}, median={np.median(interval_width):.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sorted intervals for a sample\n",
    "n_show = 200\n",
    "idx_sort = np.argsort(pd_point)[:n_show]\n",
    "axes[0].fill_between(\n",
    "    range(n_show),\n",
    "    pd_low[idx_sort],\n",
    "    pd_high[idx_sort],\n",
    "    alpha=0.3,\n",
    "    color=\"steelblue\",\n",
    "    label=\"90% CP interval\",\n",
    ")\n",
    "axes[0].plot(range(n_show), pd_point[idx_sort], \"k-\", linewidth=0.5, label=\"Point PD\")\n",
    "axes[0].set_xlabel(\"Loan (sorted by PD)\")\n",
    "axes[0].set_ylabel(\"PD\")\n",
    "axes[0].set_title(f\"Conformal PD Intervals ({n_show} loans)\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Width distribution\n",
    "axes[1].hist(interval_width, bins=50, color=\"coral\", edgecolor=\"black\", alpha=0.7)\n",
    "axes[1].set_xlabel(\"Interval Width\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "axes[1].set_title(\"Distribution of Interval Widths\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Insight: Predictive Uncertainty and Decision-Making\n",
    "- Point `PD` alone is not sufficient for regulated decisioning; `PD_high - PD_low` provides direct model-risk signal.\n",
    "- In practice, loans with similar `PD_point` but higher uncertainty should receive more conservative treatment (pricing, limits, staging).\n",
    "- This step links statistical guarantees to capital and provisioning governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Step 3 — IFRS 9 Staging & ECL\n",
    "\n",
    "Assign loans to IFRS 9 stages and compute Expected Credit Loss:\n",
    "- **Stage 1** (no SICR): ECL = PD_12m x LGD x EAD\n",
    "- **Stage 2** (SICR): ECL = PD_lifetime x LGD x EAD\n",
    "- **Stage 3** (default): ECL = 1.0 x LGD x EAD\n",
    "\n",
    "**Enhancement**: Use conformal interval width as additional SICR signal.\n",
    "High uncertainty loans ($PD_{high} - PD_{point}$ in top 10%) migrate to Stage 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: IFRS 9 Staging & ECL\n",
    "t0 = time.time()\n",
    "\n",
    "# Simulate origination PD (slightly lower than current — model drift)\n",
    "pd_origination = pd_point * np.random.uniform(0.7, 0.95, len(pd_point))\n",
    "\n",
    "# Standard staging (without conformal enhancement)\n",
    "stages_standard = assign_stage(pd_origination, pd_point)\n",
    "\n",
    "# Enhanced staging (with conformal interval width as SICR signal)\n",
    "stages_enhanced = assign_stage(pd_origination, pd_point, pd_high=pd_high)\n",
    "\n",
    "# EAD and LGD assumptions\n",
    "ead = df_apps[\"loan_amnt\"].values.astype(float)\n",
    "lgd = np.full(len(df_apps), 0.45)\n",
    "\n",
    "# Compute ECL for both staging methods\n",
    "ecl_standard = compute_ecl(pd_point, lgd, ead, stages_standard)\n",
    "ecl_enhanced = compute_ecl(pd_point, lgd, ead, stages_enhanced)\n",
    "\n",
    "# Conformal ECL range\n",
    "ecl_range = ecl_with_conformal_range(pd_low, pd_point, pd_high, lgd, ead, stages_enhanced)\n",
    "\n",
    "ecl_time = time.time() - t0\n",
    "\n",
    "print(f\"Step 3: IFRS 9 Staging & ECL ({ecl_time:.2f}s)\")\n",
    "print(f\"\\n{'Staging Method':<25} {'Stage 1':>10} {'Stage 2':>10} {'Stage 3':>10} {'Total ECL':>15}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for name, stages_arr, ecl_df in [\n",
    "    (\"Standard\", stages_standard, ecl_standard),\n",
    "    (\"CP-Enhanced\", stages_enhanced, ecl_enhanced),\n",
    "]:\n",
    "    s1 = (stages_arr == 1).sum()\n",
    "    s2 = (stages_arr == 2).sum()\n",
    "    s3 = (stages_arr == 3).sum()\n",
    "    total_ecl = ecl_df[\"ecl\"].sum()\n",
    "    print(f\"{name:<25} {s1:>10,} {s2:>10,} {s3:>10,} ${total_ecl:>14,.0f}\")\n",
    "\n",
    "print(\"\\nConformal ECL Range:\")\n",
    "print(f\"  ECL (optimistic):  ${ecl_range['ecl_low'].sum():>14,.0f}\")\n",
    "print(f\"  ECL (expected):    ${ecl_range['ecl_point'].sum():>14,.0f}\")\n",
    "print(f\"  ECL (conservative):${ecl_range['ecl_high'].sum():>14,.0f}\")\n",
    "print(f\"  Provision range:   ${ecl_range['ecl_high'].sum() - ecl_range['ecl_low'].sum():>14,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IFRS 9 Visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Stage distribution comparison\n",
    "stage_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Standard\": pd.Series(stages_standard).value_counts().sort_index(),\n",
    "        \"CP-Enhanced\": pd.Series(stages_enhanced).value_counts().sort_index(),\n",
    "    }\n",
    ").fillna(0)\n",
    "stage_data.plot(kind=\"bar\", ax=axes[0], color=[\"steelblue\", \"coral\"], edgecolor=\"black\")\n",
    "axes[0].set_xlabel(\"Stage\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "axes[0].set_title(\"Stage Distribution: Standard vs CP-Enhanced\")\n",
    "axes[0].set_xticklabels([f\"Stage {int(s)}\" for s in stage_data.index], rotation=0)\n",
    "\n",
    "# 2. ECL by stage\n",
    "ecl_by_stage = ecl_enhanced.groupby(\"stage\")[\"ecl\"].sum() / 1e6\n",
    "stage_order = list(ecl_by_stage.index)\n",
    "stage_colors = {1: \"#2ecc71\", 2: \"#f39c12\", 3: \"#e74c3c\"}\n",
    "bar_colors = [stage_colors.get(int(s), \"#95a5a6\") for s in stage_order]\n",
    "ecl_by_stage.plot(kind=\"bar\", ax=axes[1], color=bar_colors, edgecolor=\"black\")\n",
    "axes[1].set_xlabel(\"Stage\")\n",
    "axes[1].set_ylabel(\"Total ECL ($M)\")\n",
    "axes[1].set_title(\"ECL by Stage (CP-Enhanced)\")\n",
    "axes[1].set_xticklabels([f\"Stage {int(s)}\" for s in stage_order], rotation=0)\n",
    "\n",
    "# 3. ECL range (conformal)\n",
    "ecl_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"Scenario\": [\"Optimistic\\n(PD_low)\", \"Expected\\n(PD_point)\", \"Conservative\\n(PD_high)\"],\n",
    "        \"ECL\": [\n",
    "            ecl_range[\"ecl_low\"].sum() / 1e6,\n",
    "            ecl_range[\"ecl_point\"].sum() / 1e6,\n",
    "            ecl_range[\"ecl_high\"].sum() / 1e6,\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "bars = axes[2].bar(\n",
    "    ecl_summary[\"Scenario\"],\n",
    "    ecl_summary[\"ECL\"],\n",
    "    color=[\"#2ecc71\", \"#3498db\", \"#e74c3c\"],\n",
    "    edgecolor=\"black\",\n",
    ")\n",
    "axes[2].set_ylabel(\"Total ECL ($M)\")\n",
    "axes[2].set_title(\"ECL Range from Conformal Prediction\")\n",
    "for bar, val in zip(bars, ecl_summary[\"ECL\"], strict=False):\n",
    "    axes[2].text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 0.02,\n",
    "        f\"${val:.1f}M\",\n",
    "        ha=\"center\",\n",
    "        fontsize=10,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Insight: IFRS 9 Interpretation\n",
    "- CP-enhanced staging changes not only stage counts, but also the reserve distribution across the portfolio.\n",
    "- Conformal ECL ranges are useful for capital planning under optimistic, expected, and conservative views.\n",
    "- The most sensitive operational indicator is migration from Stage 1 to Stage 2 under uncertainty and deterioration signals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Step 4 — Portfolio Optimization\n",
    "\n",
    "Use conformal prediction intervals as **box uncertainty sets** for robust optimization.\n",
    "\n",
    "We solve two models:\n",
    "1. **Robust LP** (uses PD_high for risk constraint) — conservative, fewer approvals\n",
    "2. **Non-Robust LP** (uses PD_point for risk constraint) — aggressive, more approvals\n",
    "\n",
    "Then compare: how many additional defaults would the non-robust portfolio incur under worst case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Portfolio Optimization\n",
    "t0 = time.time()\n",
    "\n",
    "# Prepare loan data\n",
    "loan_amounts = df_apps[\"loan_amnt\"].values.astype(float)\n",
    "if \"int_rate\" in df_apps.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_apps[\"int_rate\"]):\n",
    "        int_rates = pd.to_numeric(df_apps[\"int_rate\"], errors=\"coerce\").fillna(12.0).values / 100.0\n",
    "    else:\n",
    "        int_rates = (\n",
    "            df_apps[\"int_rate\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.rstrip(\"%\")\n",
    "            .pipe(pd.to_numeric, errors=\"coerce\")\n",
    "            .fillna(12.0)\n",
    "            .values\n",
    "            / 100.0\n",
    "        )\n",
    "else:\n",
    "    int_rates = np.full(len(df_apps), 0.12)\n",
    "\n",
    "grades = df_apps[\"grade\"].values if \"grade\" in df_apps.columns else np.full(len(df_apps), \"C\")\n",
    "\n",
    "loans_df = pd.DataFrame(\n",
    "    {\n",
    "        \"loan_amnt\": loan_amounts,\n",
    "        \"purpose\": df_apps[\"purpose\"].values if \"purpose\" in df_apps.columns else \"other\",\n",
    "        \"grade\": grades,\n",
    "    }\n",
    ")\n",
    "\n",
    "BUDGET = 50_000_000  # $50M capital\n",
    "MAX_PD = 0.10  # 10% portfolio PD cap\n",
    "MAX_CONC = 0.35  # 35% max per purpose\n",
    "\n",
    "# Robust portfolio (uses PD_high)\n",
    "model_robust = build_portfolio_model(\n",
    "    loans_df,\n",
    "    pd_point,\n",
    "    pd_low,\n",
    "    pd_high,\n",
    "    lgd,\n",
    "    int_rates,\n",
    "    total_budget=BUDGET,\n",
    "    max_concentration=MAX_CONC,\n",
    "    max_portfolio_pd=MAX_PD,\n",
    "    robust=True,\n",
    ")\n",
    "sol_robust = solve_portfolio(model_robust)\n",
    "alloc_robust = np.array([sol_robust[\"allocation\"][i] for i in range(len(df_apps))])\n",
    "\n",
    "# Non-robust portfolio (uses PD_point)\n",
    "model_nonrobust = build_portfolio_model(\n",
    "    loans_df,\n",
    "    pd_point,\n",
    "    pd_low,\n",
    "    pd_high,\n",
    "    lgd,\n",
    "    int_rates,\n",
    "    total_budget=BUDGET,\n",
    "    max_concentration=MAX_CONC,\n",
    "    max_portfolio_pd=MAX_PD,\n",
    "    robust=False,\n",
    ")\n",
    "sol_nonrobust = solve_portfolio(model_nonrobust)\n",
    "alloc_nonrobust = np.array([sol_nonrobust[\"allocation\"][i] for i in range(len(df_apps))])\n",
    "\n",
    "opt_time = time.time() - t0\n",
    "\n",
    "# Portfolio metrics\n",
    "wpd_r = np.sum(alloc_robust * loan_amounts * pd_point) / (\n",
    "    np.sum(alloc_robust * loan_amounts) + 1e-6\n",
    ")\n",
    "wpd_nr = np.sum(alloc_nonrobust * loan_amounts * pd_point) / (\n",
    "    np.sum(alloc_nonrobust * loan_amounts) + 1e-6\n",
    ")\n",
    "\n",
    "print(f\"Step 4: Portfolio Optimization ({opt_time:.2f}s)\")\n",
    "print(f\"\\n{'Metric':<30} {'Robust':>15} {'Non-Robust':>15}\")\n",
    "print(\"-\" * 62)\n",
    "print(\n",
    "    f\"{'Net return':<30} ${sol_robust['objective_value']:>14,.0f} ${sol_nonrobust['objective_value']:>14,.0f}\"\n",
    ")\n",
    "print(f\"{'Loans funded':<30} {sol_robust['n_funded']:>15,} {sol_nonrobust['n_funded']:>15,}\")\n",
    "print(\n",
    "    f\"{'Capital allocated':<30} ${sol_robust['total_allocated']:>14,.0f} ${sol_nonrobust['total_allocated']:>14,.0f}\"\n",
    ")\n",
    "print(f\"{'Portfolio PD':<30} {wpd_r:>14.4f} {wpd_nr:>14.4f}\")\n",
    "\n",
    "# Scenario analysis\n",
    "scenarios = scenario_analysis(alloc_robust, loan_amounts, pd_low, pd_point, pd_high, lgd)\n",
    "print(\"\\nRobust Portfolio ? Scenario Analysis:\")\n",
    "print(f\"  Best case (PD_low):     ${scenarios['best_case'].values[0]:>12,.0f}\")\n",
    "print(f\"  Expected (PD_point):    ${scenarios['expected'].values[0]:>12,.0f}\")\n",
    "print(f\"  Worst case (PD_high):   ${scenarios['worst_case'].values[0]:>12,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Full Pipeline Summary — Dashboard View\n",
    "\n",
    "Consolidated view of all pipeline outputs for management reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dashboard: consolidated pipeline results\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# 1. PD Distribution by grade\n",
    "if \"grade\" in df_apps.columns:\n",
    "    for grade in sorted(df_apps[\"grade\"].unique()):\n",
    "        mask = df_apps[\"grade\"].values == grade\n",
    "        axes[0, 0].hist(pd_point[mask], bins=30, alpha=0.5, label=f\"Grade {grade}\", density=True)\n",
    "    axes[0, 0].set_xlabel(\"Calibrated PD\")\n",
    "    axes[0, 0].set_ylabel(\"Density\")\n",
    "    axes[0, 0].set_title(\"PD Distribution by Grade\")\n",
    "    axes[0, 0].legend(fontsize=8)\n",
    "\n",
    "# 2. Conformal intervals (sample)\n",
    "n_show = 100\n",
    "idx = np.argsort(pd_point)[:n_show]\n",
    "axes[0, 1].fill_between(\n",
    "    range(n_show), pd_low[idx], pd_high[idx], alpha=0.3, color=\"steelblue\", label=\"90% CI\"\n",
    ")\n",
    "axes[0, 1].plot(range(n_show), pd_point[idx], \"k-\", linewidth=0.8, label=\"Point PD\")\n",
    "axes[0, 1].set_xlabel(\"Loan (sorted)\")\n",
    "axes[0, 1].set_ylabel(\"PD\")\n",
    "axes[0, 1].set_title(\"Conformal PD Intervals\")\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# 3. Stage allocation\n",
    "stage_counts = pd.Series(stages_enhanced).value_counts().sort_index()\n",
    "colors_stage = [\"#2ecc71\", \"#f39c12\", \"#e74c3c\"]\n",
    "axes[0, 2].pie(\n",
    "    stage_counts.values,\n",
    "    labels=[f\"Stage {s}\" for s in stage_counts.index],\n",
    "    colors=colors_stage[: len(stage_counts)],\n",
    "    autopct=\"%1.1f%%\",\n",
    "    startangle=90,\n",
    ")\n",
    "axes[0, 2].set_title(\"IFRS 9 Stage Distribution\")\n",
    "\n",
    "# 4. ECL range\n",
    "ecl_scenarios = [\"Optimistic\", \"Expected\", \"Conservative\"]\n",
    "ecl_values = [\n",
    "    ecl_range[\"ecl_low\"].sum() / 1e6,\n",
    "    ecl_range[\"ecl_point\"].sum() / 1e6,\n",
    "    ecl_range[\"ecl_high\"].sum() / 1e6,\n",
    "]\n",
    "bars = axes[1, 0].bar(\n",
    "    ecl_scenarios, ecl_values, color=[\"#2ecc71\", \"#3498db\", \"#e74c3c\"], edgecolor=\"black\"\n",
    ")\n",
    "axes[1, 0].set_ylabel(\"Total ECL ($M)\")\n",
    "axes[1, 0].set_title(\"ECL Range (Conformal)\")\n",
    "\n",
    "# 5. Portfolio allocation by grade\n",
    "if \"grade\" in df_apps.columns:\n",
    "    alloc_by_grade = (\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"grade\": grades,\n",
    "                \"robust_alloc\": alloc_robust * loan_amounts,\n",
    "                \"nonrobust_alloc\": alloc_nonrobust * loan_amounts,\n",
    "            }\n",
    "        )\n",
    "        .groupby(\"grade\")[[\"robust_alloc\", \"nonrobust_alloc\"]]\n",
    "        .sum()\n",
    "        / 1e6\n",
    "    )\n",
    "    alloc_by_grade.plot(kind=\"bar\", ax=axes[1, 1], color=[\"steelblue\", \"coral\"], edgecolor=\"black\")\n",
    "    axes[1, 1].set_xlabel(\"Grade\")\n",
    "    axes[1, 1].set_ylabel(\"Allocation ($M)\")\n",
    "    axes[1, 1].set_title(\"Capital Allocation by Grade\")\n",
    "    axes[1, 1].legend([\"Robust\", \"Non-Robust\"])\n",
    "\n",
    "# 6. Value of conformal prediction\n",
    "price_robust = sol_nonrobust[\"objective_value\"] - sol_robust[\"objective_value\"]\n",
    "wpd_nr_worst = np.sum(alloc_nonrobust * loan_amounts * pd_high) / (\n",
    "    np.sum(alloc_nonrobust * loan_amounts) + 1e-6\n",
    ")\n",
    "wpd_r_worst = np.sum(alloc_robust * loan_amounts * pd_high) / (\n",
    "    np.sum(alloc_robust * loan_amounts) + 1e-6\n",
    ")\n",
    "\n",
    "metrics_labels = [\n",
    "    \"Price of\\nRobustness ($K)\",\n",
    "    \"Non-Robust\\nWorst PD (%)\",\n",
    "    \"Robust\\nWorst PD (%)\",\n",
    "    \"PD Cap (%)\",\n",
    "]\n",
    "metrics_values = [price_robust / 1e3, wpd_nr_worst * 100, wpd_r_worst * 100, MAX_PD * 100]\n",
    "colors_m = [\"#f39c12\", \"#e74c3c\", \"#2ecc71\", \"#2c3e50\"]\n",
    "axes[1, 2].bar(metrics_labels, metrics_values, color=colors_m, edgecolor=\"black\")\n",
    "axes[1, 2].set_title(\"Value of Conformal Prediction\")\n",
    "axes[1, 2].axhline(y=MAX_PD * 100, color=\"black\", linestyle=\"--\", alpha=0.3)\n",
    "\n",
    "plt.suptitle(\"End-to-End Pipeline Dashboard\", fontweight=\"bold\", fontsize=16, y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Insight: E2E Decision Perspective\n",
    "- The pipeline demonstrates full traceability: score -> uncertainty -> provisioning -> optimization.\n",
    "- Economic robustness depends on both calibrated probabilities and efficient uncertainty intervals.\n",
    "- If intervals remain too wide, robust deployment can collapse in capital usage even when coverage is statistically correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Sample Predictions — Individual Loan Analysis\n",
    "\n",
    "Show the complete pipeline output for a few sample loans, demonstrating how each step\n",
    "transforms raw application data into actionable decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 10 loans across risk spectrum\n",
    "sample_idx = np.percentile(np.argsort(pd_point), [5, 15, 25, 35, 45, 55, 65, 75, 85, 95]).astype(\n",
    "    int\n",
    ")\n",
    "sample_idx = np.clip(sample_idx, 0, len(df_apps) - 1)\n",
    "\n",
    "sample_report = pd.DataFrame(\n",
    "    {\n",
    "        \"Loan#\": range(1, len(sample_idx) + 1),\n",
    "        \"Amount\": [f\"${loan_amounts[i]:,.0f}\" for i in sample_idx],\n",
    "        \"Grade\": [grades[i] for i in sample_idx],\n",
    "        \"Rate\": [f\"{int_rates[i] * 100:.1f}%\" for i in sample_idx],\n",
    "        \"PD_point\": [f\"{pd_point[i]:.3f}\" for i in sample_idx],\n",
    "        \"PD_low\": [f\"{pd_low[i]:.3f}\" for i in sample_idx],\n",
    "        \"PD_high\": [f\"{pd_high[i]:.3f}\" for i in sample_idx],\n",
    "        \"Width\": [f\"{pd_high[i] - pd_low[i]:.3f}\" for i in sample_idx],\n",
    "        \"Stage\": [stages_enhanced[i] for i in sample_idx],\n",
    "        \"ECL\": [f\"${ecl_enhanced['ecl'].iloc[i]:,.0f}\" for i in sample_idx],\n",
    "        \"Alloc_R\": [f\"{alloc_robust[i]:.2f}\" for i in sample_idx],\n",
    "        \"Alloc_NR\": [f\"{alloc_nonrobust[i]:.2f}\" for i in sample_idx],\n",
    "        \"Actual\": [df_apps[\"default_flag\"].iloc[i] for i in sample_idx],\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Sample Loan Pipeline Output (10 loans across risk spectrum):\")\n",
    "print(sample_report.to_string(index=False))\n",
    "\n",
    "print(\"\\nColumn Legend:\")\n",
    "print(\"  PD_point = Calibrated point PD estimate\")\n",
    "print(\"  PD_low/PD_high = 90% conformal prediction interval\")\n",
    "print(\"  Width = Interval width (uncertainty measure)\")\n",
    "print(\"  Stage = IFRS 9 stage (CP-enhanced)\")\n",
    "print(\"  ECL = Expected Credit Loss\")\n",
    "print(\"  Alloc_R = Robust allocation (0-1)\")\n",
    "print(\"  Alloc_NR = Non-robust allocation (0-1)\")\n",
    "print(\"  Actual = True default (only known ex-post)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Complete Metrics Summary\n",
    "\n",
    "All key metrics from across the 8 notebooks consolidated into a single reference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consolidated metrics from the full pipeline\n",
    "print(\"=\" * 85)\n",
    "print(\"COMPLETE PIPELINE METRICS SUMMARY\")\n",
    "print(\"=\" * 85)\n",
    "\n",
    "# Dataset\n",
    "print(\"\\n--- DATASET ---\")\n",
    "print(\"  Total loans:    2,260,000+ (2007-2020)\")\n",
    "print(\"  Training:       1,346,311 (pre-2018)\")\n",
    "print(\"  Calibration:    237,584 (15% of train)\")\n",
    "print(\"  Test (OOT):     276,869 (2018+)\")\n",
    "print(f\"  Default rate:   ~18.5% (train), {df_apps['default_flag'].mean():.1%} (test batch)\")\n",
    "\n",
    "# PD Model\n",
    "print(\"\\n--- PD MODEL (NB03) ---\")\n",
    "print(f\"  Model: CatBoost ({cb_model.tree_count_} trees, Optuna-tuned)\")\n",
    "print(f\"  Features: {len(features)} ({len(cat_features)} categorical)\")\n",
    "print(f\"  AUC (this batch): {auc:.4f}\")\n",
    "print(f\"  Calibration: {calibrator.__class__.__name__ if calibrator else 'None'}\")\n",
    "\n",
    "# Conformal Prediction\n",
    "print(\"\\n--- CONFORMAL PREDICTION (NB04) ---\")\n",
    "print(\"  Method: Split Conformal (MAPIE)\")\n",
    "print(\"  Coverage target: 90%\")\n",
    "print(f\"  Avg interval width: {interval_width.mean():.4f}\")\n",
    "print(\"  Coverage guarantee: distribution-free, finite-sample\")\n",
    "\n",
    "# IFRS 9\n",
    "s1 = (stages_enhanced == 1).sum()\n",
    "s2 = (stages_enhanced == 2).sum()\n",
    "s3 = (stages_enhanced == 3).sum()\n",
    "print(\"\\n--- IFRS 9 ECL (NB06 + NB09) ---\")\n",
    "print(\n",
    "    f\"  Staging: S1={s1:,} ({s1 / len(stages_enhanced):.1%}), S2={s2:,} ({s2 / len(stages_enhanced):.1%}), S3={s3:,}\"\n",
    ")\n",
    "print(f\"  ECL (expected):     ${ecl_range['ecl_point'].sum():>12,.0f}\")\n",
    "print(f\"  ECL (conservative): ${ecl_range['ecl_high'].sum():>12,.0f}\")\n",
    "print(f\"  Provision range:    ${ecl_range['ecl_high'].sum() - ecl_range['ecl_low'].sum():>12,.0f}\")\n",
    "\n",
    "# Portfolio Optimization\n",
    "print(\"\\n--- PORTFOLIO OPTIMIZATION (NB08 + NB09) ---\")\n",
    "print(f\"  Budget: ${BUDGET:,.0f}\")\n",
    "print(\n",
    "    f\"  Robust LP:     ${sol_robust['objective_value']:>12,.0f} return, {sol_robust['n_funded']:,} funded\"\n",
    ")\n",
    "print(\n",
    "    f\"  Non-Robust LP: ${sol_nonrobust['objective_value']:>12,.0f} return, {sol_nonrobust['n_funded']:,} funded\"\n",
    ")\n",
    "price_robust = sol_nonrobust[\"objective_value\"] - sol_robust[\"objective_value\"]\n",
    "price_pct = (\n",
    "    (price_robust / sol_nonrobust[\"objective_value\"] * 100)\n",
    "    if sol_nonrobust[\"objective_value\"]\n",
    "    else 0.0\n",
    ")\n",
    "print(f\"  Price of robustness: ${price_robust:>12,.0f} ({price_pct:.1f}%)\")\n",
    "print(\n",
    "    f\"  Non-robust worst-case PD: {wpd_nr_worst:.4f} {'(VIOLATES ' + str(MAX_PD) + ' cap!)' if wpd_nr_worst > MAX_PD else '(within cap)'}\"\n",
    ")\n",
    "print(\n",
    "    f\"  Robust worst-case PD:     {wpd_r_worst:.4f} {'(within cap)' if wpd_r_worst <= MAX_PD else '(VIOLATES!)'}\"\n",
    ")\n",
    "\n",
    "# Pipeline timing\n",
    "total_time = pd_time + cp_time + ecl_time + opt_time\n",
    "print(\"\\n--- PIPELINE TIMING ---\")\n",
    "print(f\"  PD Prediction:        {pd_time:.2f}s\")\n",
    "print(f\"  Conformal Prediction: {cp_time:.2f}s\")\n",
    "print(f\"  IFRS 9 ECL:           {ecl_time:.2f}s\")\n",
    "print(f\"  Optimization:         {opt_time:.2f}s\")\n",
    "print(f\"  TOTAL:                {total_time:.2f}s ({total_time / 60:.1f}min)\")\n",
    "print(\"=\" * 85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Thesis Contribution — Conformal Predict-then-Optimize\n",
    "\n",
    "### The Innovation\n",
    "\n",
    "This project demonstrates a **Conformal Predict-then-Optimize** pipeline for credit risk:\n",
    "\n",
    "1. **Predict**: CatBoost PD model with Optuna HPO and Isotonic calibration\n",
    "2. **Quantify Uncertainty**: MAPIE conformal prediction for distribution-free PD intervals\n",
    "3. **Optimize**: Pyomo + HiGHS robust optimization using CP intervals as box uncertainty sets\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "| Approach | Uncertainty | Guarantees | Tractability |\n",
    "|----------|------------|------------|--------------|\n",
    "| Point estimates | None | None | Trivial |\n",
    "| Bootstrap | Empirical | Asymptotic only | Moderate |\n",
    "| Bayesian | Prior-dependent | With correct prior | Complex |\n",
    "| **Conformal (ours)** | **Distribution-free** | **Finite-sample** | **Tractable LP** |\n",
    "\n",
    "### Key Results\n",
    "\n",
    "- **PD Model**: AUC ~0.69 with 44 features, out-of-time validated\n",
    "- **Conformal Intervals**: 90% coverage guarantee, avg width ~0.12\n",
    "- **Robust Portfolio**: Sacrifices ~72% return for worst-case safety\n",
    "- **ECL Range**: Conformal intervals propagate to provision ranges\n",
    "- **SICR Enhancement**: CP interval width as additional Stage 2 migration signal\n",
    "- **Causal Insights**: Naive rate-default correlation overestimates causal effect by 3.8x\n",
    "\n",
    "### Notebooks Summary\n",
    "\n",
    "| NB | Topic | Key Output |\n",
    "|----|-------|------------|\n",
    "| 01 | EDA | Data quality, distributions, target analysis |\n",
    "| 02 | Feature Engineering | 44 CatBoost features, WOE/IV, interactions |\n",
    "| 03 | PD Modeling | CatBoost AUC ~0.69, SHAP, calibration |\n",
    "| 04 | Conformal Prediction | [PD_low, PD_high] with 90% coverage |\n",
    "| 05 | Time Series | AutoARIMA forecasts, IFRS 9 scenarios |\n",
    "| 06 | Survival Analysis | Cox PH, RSF, lifetime PD curves |\n",
    "| 07 | Causal Inference | DML ATE, CausalForest CATE, verification paradox |\n",
    "| 08 | Portfolio Optimization | LP/MILP, efficient frontier, ECL minimization |\n",
    "| 09 | **End-to-End** | **Full pipeline: data -> PD -> CP -> ECL -> optimization** |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pipeline results\n",
    "pipeline_results = {\n",
    "    \"batch_size\": len(df_apps),\n",
    "    \"pd_mean\": float(pd_point.mean()),\n",
    "    \"pd_auc\": float(auc),\n",
    "    \"interval_width_mean\": float(interval_width.mean()),\n",
    "    \"stages\": {\"S1\": int(s1), \"S2\": int(s2), \"S3\": int(s3)},\n",
    "    \"ecl_expected\": float(ecl_range[\"ecl_point\"].sum()),\n",
    "    \"ecl_conservative\": float(ecl_range[\"ecl_high\"].sum()),\n",
    "    \"ecl_range\": float(ecl_range[\"ecl_high\"].sum() - ecl_range[\"ecl_low\"].sum()),\n",
    "    \"robust_return\": float(sol_robust[\"objective_value\"]),\n",
    "    \"robust_funded\": int(sol_robust[\"n_funded\"]),\n",
    "    \"nonrobust_return\": float(sol_nonrobust[\"objective_value\"]),\n",
    "    \"nonrobust_funded\": int(sol_nonrobust[\"n_funded\"]),\n",
    "    \"price_of_robustness\": float(price_robust),\n",
    "    \"pipeline_time_s\": float(total_time),\n",
    "}\n",
    "\n",
    "with open(MODEL_DIR / \"pipeline_results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(pipeline_results, f)\n",
    "\n",
    "print(f\"Pipeline results saved to {MODEL_DIR / 'pipeline_results.pkl'}\")\n",
    "print(\"\\nNB09 End-to-End Pipeline complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Final Conclusions: End-to-End Integration\n",
    "\n",
    "### Key Findings\n",
    "- The pipeline is operational from scoring to uncertainty, IFRS9 staging, and optimization.\n",
    "- Decision quality is dominated by two factors: PD calibration quality and conformal interval efficiency.\n",
    "- Robust and non-robust portfolios expose an explicit trade-off between safety and return.\n",
    "\n",
    "### Financial Risk Interpretation\n",
    "- Uncertainty-aware decisions provide clearer downside control for provisioning and allocation.\n",
    "- Conservative ECL scenarios quantify reserve sensitivity to predictive uncertainty.\n",
    "- End-to-end traceability supports model risk governance and audit readiness.\n",
    "\n",
    "### Contribution to End-to-End Pipeline\n",
    "- Integrates all modules into a coherent decision stack with measurable outputs.\n",
    "- Demonstrates how each analytical layer contributes to financial risk control.\n",
    "- Provides the implementation baseline for production hardening (API, dashboard, CI)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lending-club-risk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
