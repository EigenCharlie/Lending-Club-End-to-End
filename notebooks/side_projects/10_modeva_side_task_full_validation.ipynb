{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Side Task: Evaluacion Integral con Modeva (Lending Club Risk)\n",
    "\n",
    "Este notebook es una tarea lateral (`side task`) para evaluar metodos y tecnicas de `modeva`\n",
    "sobre el caso de riesgo de credito del proyecto.\n",
    "\n",
    "Alcance:\n",
    "- Ejecutar metodos utiles/relevantes para datos tabulares de default.\n",
    "- Comparar resultados con el baseline core del proyecto.\n",
    "- Producir insights tecnicos y recomendacion de adopcion.\n",
    "\n",
    "Nota:\n",
    "- No reemplaza el pipeline canonico actual (PD + calibracion + conformal + optimizacion).\n",
    "- Debe ejecutarse en el entorno principal del proyecto (no en `Legacy`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import modeva\n",
    "import modeva.auth as modeva_auth\n",
    "from modeva import models\n",
    "\n",
    "# Evita prompt interactivo de auth en ejecucion batch.\n",
    "modeva_auth.Authenticator.run = lambda self, auth_code=None: None\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / \"data\").exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "REPORT_DIR = PROJECT_ROOT / \"reports\"\n",
    "\n",
    "print(\"modeva version:\", modeva.__version__)\n",
    "print(\"project root:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1) Contexto baseline del proyecto core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(MODEL_DIR / \"pd_training_record.pkl\", \"rb\") as f:\n",
    "    pd_record = pickle.load(f)\n",
    "\n",
    "conformal_status = json.loads((MODEL_DIR / \"conformal_policy_status.json\").read_text())\n",
    "causal_rule = json.loads((MODEL_DIR / \"causal_policy_rule.json\").read_text())\n",
    "ifrs9_summary = pd.read_parquet(DATA_DIR / \"ifrs9_scenario_summary.parquet\")\n",
    "\n",
    "core_snapshot = pd.DataFrame(\n",
    "    {\n",
    "        \"metric\": [\n",
    "            \"core_pd_auc_test\",\n",
    "            \"core_pd_brier_test\",\n",
    "            \"core_conformal_cov90\",\n",
    "            \"core_conformal_cov95\",\n",
    "            \"core_conformal_min_group_cov90\",\n",
    "            \"core_causal_selected_rule\",\n",
    "            \"core_ifrs9_baseline_ecl\",\n",
    "            \"core_ifrs9_severe_ecl\",\n",
    "        ],\n",
    "        \"value\": [\n",
    "            pd_record[\"final_test_metrics\"][\"auc_roc\"],\n",
    "            pd_record[\"final_test_metrics\"][\"brier_score\"],\n",
    "            conformal_status[\"coverage_90\"],\n",
    "            conformal_status[\"coverage_95\"],\n",
    "            conformal_status[\"min_group_coverage_90\"],\n",
    "            causal_rule[\"selected_rule\"],\n",
    "            float(ifrs9_summary.loc[ifrs9_summary[\"scenario\"] == \"baseline\", \"total_ecl\"].iloc[0]),\n",
    "            float(ifrs9_summary.loc[ifrs9_summary[\"scenario\"] == \"severe\", \"total_ecl\"].iloc[0]),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "core_snapshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2) Carga de datos y construccion de datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features = [\n",
    "    \"loan_amnt\",\n",
    "    \"annual_inc\",\n",
    "    \"dti\",\n",
    "    \"int_rate\",\n",
    "    \"installment\",\n",
    "    \"loan_to_income\",\n",
    "    \"rev_utilization\",\n",
    "    \"open_acc\",\n",
    "    \"total_acc\",\n",
    "    \"fico_score\",\n",
    "    \"grade\",\n",
    "    \"home_ownership\",\n",
    "    \"purpose\",\n",
    "    \"verification_status\",\n",
    "    \"term\",\n",
    "]\n",
    "target_col = \"default_flag\"\n",
    "\n",
    "train_raw_full = pd.read_parquet(DATA_DIR / \"train_fe.parquet\")\n",
    "test_raw_full = pd.read_parquet(DATA_DIR / \"test_fe.parquet\")\n",
    "\n",
    "raw_cols = base_features + [target_col]\n",
    "train_raw = (\n",
    "    train_raw_full.sample(6000, random_state=RANDOM_STATE).reset_index(drop=True)[raw_cols].copy()\n",
    ")\n",
    "test_raw = (\n",
    "    test_raw_full.sample(3000, random_state=RANDOM_STATE).reset_index(drop=True)[raw_cols].copy()\n",
    ")\n",
    "\n",
    "protected_data = pd.concat(\n",
    "    [train_raw[[\"home_ownership\"]], test_raw[[\"home_ownership\"]]],\n",
    "    axis=0,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "combined = pd.concat([train_raw, test_raw], axis=0, ignore_index=True)\n",
    "combined_num = pd.get_dummies(\n",
    "    combined,\n",
    "    columns=[\"grade\", \"home_ownership\", \"purpose\", \"verification_status\"],\n",
    "    drop_first=True,\n",
    ")\n",
    "combined_num = combined_num.fillna(combined_num.median(numeric_only=True))\n",
    "\n",
    "train_num = combined_num.iloc[: len(train_raw)].reset_index(drop=True)\n",
    "test_num = combined_num.iloc[len(train_raw) :].reset_index(drop=True)\n",
    "\n",
    "print(\"train_raw:\", train_raw.shape, \"test_raw:\", test_raw.shape)\n",
    "print(\"train_num:\", train_num.shape, \"test_num:\", test_num.shape)\n",
    "print(\"train default rate:\", round(train_raw[target_col].mean(), 6))\n",
    "print(\"test default rate:\", round(test_raw[target_col].mean(), 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3) Setup Modeva + helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ds_raw = modeva.DataSet(name=\"lc_modeva_raw_side_task\")\n",
    "ds_raw.load_dataframe_train_test(train_raw, test_raw)\n",
    "ds_raw.set_target(target_col)\n",
    "\n",
    "ds = modeva.DataSet(name=\"lc_modeva_numeric_side_task\")\n",
    "ds.load_dataframe_train_test(train_num, test_num)\n",
    "ds.set_target(target_col)\n",
    "ds.set_protected_data(protected_data)\n",
    "\n",
    "results = {}\n",
    "run_log = []\n",
    "\n",
    "\n",
    "def run_test(name, fn):\n",
    "    try:\n",
    "        out = fn()\n",
    "        results[name] = out\n",
    "        run_log.append({\"test\": name, \"status\": \"ok\", \"error\": \"\"})\n",
    "        table = getattr(out, \"table\", None)\n",
    "        if isinstance(table, pd.DataFrame):\n",
    "            print(f\"[OK] {name}: table_shape={table.shape}\")\n",
    "        elif isinstance(table, dict):\n",
    "            print(f\"[OK] {name}: table_keys={list(table.keys())}\")\n",
    "        else:\n",
    "            print(f\"[OK] {name}: no_table\")\n",
    "        return out\n",
    "    except Exception as e:\n",
    "        run_log.append({\"test\": name, \"status\": \"error\", \"error\": repr(e)})\n",
    "        print(f\"[FAIL] {name}: {repr(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def result_table(name):\n",
    "    out = results.get(name)\n",
    "    if out is None:\n",
    "        return None\n",
    "    table = getattr(out, \"table\", None)\n",
    "    return table if isinstance(table, pd.DataFrame) else None\n",
    "\n",
    "\n",
    "def result_value(name):\n",
    "    out = results.get(name)\n",
    "    if out is None:\n",
    "        return None\n",
    "    return getattr(out, \"value\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 4) Preprocessing nativo de Modeva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = ds_raw.impute_missing(add_indicators=True)\n",
    "_ = ds_raw.encode_categorical(method=\"onehot\")\n",
    "_ = ds_raw.scale_numerical(method=\"standardize\")\n",
    "_ = ds_raw.bin_numerical(features=(\"int_rate\", \"dti\"), method=\"quantile\", bins=5)\n",
    "ds_raw.preprocess()\n",
    "\n",
    "preprocess_summary = {\n",
    "    \"raw_shape\": tuple(ds_raw.to_df(raw_data=True).shape),\n",
    "    \"preprocessed_shape\": tuple(ds_raw.to_df(raw_data=False).shape),\n",
    "}\n",
    "\n",
    "x_small = ds_raw.get_data(\"train\")[:10, :-1]\n",
    "try:\n",
    "    _ = ds_raw.transform(x_small)\n",
    "    preprocess_summary[\"transform_status\"] = \"ok\"\n",
    "except Exception as e:\n",
    "    preprocess_summary[\"transform_status\"] = f\"error: {repr(e)}\"\n",
    "\n",
    "try:\n",
    "    _ = ds_raw.inverse_transform(x_small)\n",
    "    preprocess_summary[\"inverse_transform_status\"] = \"ok\"\n",
    "except Exception as e:\n",
    "    preprocess_summary[\"inverse_transform_status\"] = f\"error: {repr(e)}\"\n",
    "\n",
    "pd.DataFrame([preprocess_summary])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 5) Data diagnostics: EDA, drift, outliers, feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\"summary\", lambda: ds.summary())\n",
    "run_test(\"eda_1d\", lambda: ds.eda_1d(feature=\"int_rate\", dataset=\"train\", sample_size=2000))\n",
    "run_test(\n",
    "    \"eda_2d\",\n",
    "    lambda: ds.eda_2d(\n",
    "        feature_x=\"int_rate\",\n",
    "        feature_y=\"loan_to_income\",\n",
    "        feature_color=target_col,\n",
    "        dataset=\"train\",\n",
    "        sample_size=2000,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"eda_3d\",\n",
    "    lambda: ds.eda_3d(\n",
    "        feature_x=\"int_rate\",\n",
    "        feature_y=\"loan_to_income\",\n",
    "        feature_z=\"fico_score\",\n",
    "        feature_color=target_col,\n",
    "        dataset=\"train\",\n",
    "        sample_size=1000,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"eda_correlation\",\n",
    "    lambda: ds.eda_correlation(\n",
    "        features=(\"int_rate\", \"loan_to_income\", \"fico_score\", \"dti\", target_col),\n",
    "        dataset=\"train\",\n",
    "    ),\n",
    ")\n",
    "run_test(\"eda_pca\", lambda: ds.eda_pca(dataset=\"train\", sample_size=2000))\n",
    "run_test(\n",
    "    \"eda_umap\",\n",
    "    lambda: ds.eda_umap(\n",
    "        dataset=\"train\", sample_size=1000, n_neighbors=15, random_state=RANDOM_STATE\n",
    "    ),\n",
    ")\n",
    "\n",
    "run_test(\n",
    "    \"drift_psi\",\n",
    "    lambda: ds.data_drift_test(\n",
    "        dataset1=\"train\", dataset2=\"test\", distance_metric=\"PSI\", psi_bins=10\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"drift_wd1\",\n",
    "    lambda: ds.data_drift_test(dataset1=\"train\", dataset2=\"test\", distance_metric=\"WD1\"),\n",
    ")\n",
    "run_test(\n",
    "    \"drift_ks\", lambda: ds.data_drift_test(dataset1=\"train\", dataset2=\"test\", distance_metric=\"KS\")\n",
    ")\n",
    "\n",
    "run_test(\n",
    "    \"outlier_iforest\",\n",
    "    lambda: ds.detect_outlier_isolation_forest(dataset=\"train\", threshold=0.995, n_estimators=200),\n",
    ")\n",
    "run_test(\n",
    "    \"outlier_pca\",\n",
    "    lambda: ds.detect_outlier_pca(dataset=\"train\", threshold=0.995, method=\"mahalanobis\"),\n",
    ")\n",
    "run_test(\n",
    "    \"outlier_cblof\",\n",
    "    lambda: ds.detect_outlier_cblof(\n",
    "        dataset=\"train\", threshold=0.995, n_clusters=12, method=\"kmeans\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "run_test(\"fs_corr\", lambda: ds.feature_select_corr(dataset=\"train\", threshold=0.80))\n",
    "run_test(\"fs_xgbpfi\", lambda: ds.feature_select_xgbpfi(dataset=\"train\", threshold=0.01))\n",
    "run_test(\n",
    "    \"fs_rcit\",\n",
    "    lambda: ds.feature_select_rcit(\n",
    "        dataset=\"train\",\n",
    "        threshold=1e-5,\n",
    "        n_fourier=10,\n",
    "        n_fourier2=3,\n",
    "        n_forwards=1,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"subsample_random\",\n",
    "    lambda: ds.subsample_random(\n",
    "        dataset=\"train\", sample_size=0.25, stratify=target_col, random_state=RANDOM_STATE\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Top drift (PSI)\"))\n",
    "psi_table = result_table(\"drift_psi\")\n",
    "if psi_table is not None:\n",
    "    display(psi_table.sort_values(\"Distance_Scores\", ascending=False).head(10))\n",
    "\n",
    "display(Markdown(\"### Feature selection (XGB-PFI top)\"))\n",
    "fs_xgb = result_table(\"fs_xgbpfi\")\n",
    "if fs_xgb is not None:\n",
    "    display(fs_xgb.sort_values(\"Importance\", ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 6) ModelZoo: entrenamiento y tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mz = modeva.ModelZoo(dataset=ds, name=\"lc_modeva_side_task_zoo\", random_state=RANDOM_STATE)\n",
    "mz.add_model(models.MoLogisticRegression(max_iter=500), name=\"MoLogReg\", replace=True)\n",
    "mz.add_model(\n",
    "    models.MoLGBMClassifier(n_estimators=300, max_depth=6, verbose=-1), name=\"MoLGBM\", replace=True\n",
    ")\n",
    "mz.add_model(\n",
    "    models.MoXGBClassifier(n_estimators=300, max_depth=5, learning_rate=0.05, verbosity=0),\n",
    "    name=\"MoXGB\",\n",
    "    replace=True,\n",
    ")\n",
    "mz.train_all(silent=True)\n",
    "leaderboard = mz.leaderboard(order_by=\"test AUC\", ascending=False)\n",
    "leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = models.ModelTuneRandomSearch(ds, models.MoLogisticRegression(max_iter=500))\n",
    "run_test(\n",
    "    \"tune_random_logreg\",\n",
    "    lambda: tuner.run(\n",
    "        param_distributions={\"C\": (0.01, 5.0)},\n",
    "        dataset=\"train\",\n",
    "        n_iter=5,\n",
    "        metric=\"AUC\",\n",
    "        cv=3,\n",
    "        random_state=RANDOM_STATE,\n",
    "    ),\n",
    ")\n",
    "result_table(\"tune_random_logreg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 7) TestSuite: calidad, robustez, reliability, fairness y slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_primary = mz.get_model(\"MoLGBM\")\n",
    "model_list = [mz.get_model(n) for n in mz.list_model_names()]\n",
    "ts = modeva.TestSuite(\n",
    "    dataset=ds, model=model_primary, models=model_list, name=\"lc_modeva_side_task_suite\"\n",
    ")\n",
    "\n",
    "group_cfg = {\n",
    "    \"rent_vs_mortgage\": {\n",
    "        \"feature\": \"home_ownership\",\n",
    "        \"protected\": \"RENT\",\n",
    "        \"reference\": \"MORTGAGE\",\n",
    "    }\n",
    "}\n",
    "\n",
    "run_test(\"diag_accuracy\", lambda: ts.diagnose_accuracy_table())\n",
    "run_test(\"cmp_accuracy\", lambda: ts.compare_accuracy_table())\n",
    "run_test(\n",
    "    \"diag_reliability\",\n",
    "    lambda: ts.diagnose_reliability(\n",
    "        train_dataset=\"train\", test_dataset=\"test\", alpha=0.1, width_threshold=0.15\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_reliability\",\n",
    "    lambda: ts.compare_reliability(train_dataset=\"train\", test_dataset=\"test\", alpha=0.1),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_robustness\",\n",
    "    lambda: ts.diagnose_robustness(\n",
    "        dataset=\"test\",\n",
    "        perturb_features=(\"int_rate\", \"dti\"),\n",
    "        noise_levels=(0.02, 0.05, 0.1),\n",
    "        n_repeats=5,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_robustness\",\n",
    "    lambda: ts.compare_robustness(\n",
    "        dataset=\"test\",\n",
    "        perturb_features=(\"int_rate\", \"dti\"),\n",
    "        noise_levels=(0.02, 0.05, 0.1),\n",
    "        n_repeats=5,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_resilience\",\n",
    "    lambda: ts.diagnose_resilience(dataset=\"test\", method=\"worst-sample\", alphas=(0.1, 0.2, 0.3)),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_resilience\",\n",
    "    lambda: ts.compare_resilience(dataset=\"test\", method=\"worst-sample\", alphas=(0.1, 0.2, 0.3)),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_residual_analysis\",\n",
    "    lambda: ts.diagnose_residual_analysis(features=\"int_rate\", dataset=\"test\", sample_size=1500),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_residual_cluster\",\n",
    "    lambda: ts.diagnose_residual_cluster(dataset=\"test\", n_clusters=8, sample_size=1500),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_residual_cluster\",\n",
    "    lambda: ts.compare_residual_cluster(dataset=\"test\", n_clusters=8, sample_size=1500),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_residual_interpret\",\n",
    "    lambda: ts.diagnose_residual_interpret(dataset=\"test\", n_estimators=80, max_depth=2),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_fairness\",\n",
    "    lambda: ts.diagnose_fairness(group_config=group_cfg, dataset=\"test\", metric=\"AIR\"),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_fairness\",\n",
    "    lambda: ts.compare_fairness(group_config=group_cfg, dataset=\"test\", metric=\"AIR\"),\n",
    ")\n",
    "\n",
    "run_test(\n",
    "    \"diag_slicing_accuracy\",\n",
    "    lambda: ts.diagnose_slicing_accuracy(\n",
    "        features=(\"int_rate\", \"fico_score\"), dataset=\"test\", bins=5\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_slicing_accuracy\",\n",
    "    lambda: ts.compare_slicing_accuracy(features=\"int_rate\", dataset=\"test\", bins=5),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_slicing_overfit\",\n",
    "    lambda: ts.diagnose_slicing_overfit(\n",
    "        features=(\"int_rate\", \"fico_score\"), train_dataset=\"train\", test_dataset=\"test\", bins=5\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_slicing_overfit\",\n",
    "    lambda: ts.compare_slicing_overfit(\n",
    "        features=\"int_rate\", train_dataset=\"train\", test_dataset=\"test\", bins=5\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_slicing_robustness\",\n",
    "    lambda: ts.diagnose_slicing_robustness(\n",
    "        features=(\"int_rate\", \"fico_score\"),\n",
    "        dataset=\"test\",\n",
    "        bins=5,\n",
    "        perturb_features=(\"int_rate\", \"dti\"),\n",
    "        noise_levels=0.05,\n",
    "        n_repeats=3,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_slicing_robustness\",\n",
    "    lambda: ts.compare_slicing_robustness(\n",
    "        features=\"int_rate\",\n",
    "        dataset=\"test\",\n",
    "        bins=5,\n",
    "        perturb_features=(\"int_rate\", \"dti\"),\n",
    "        noise_levels=0.05,\n",
    "        n_repeats=3,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_slicing_reliability\",\n",
    "    lambda: ts.diagnose_slicing_reliability(\n",
    "        features=(\"int_rate\", \"fico_score\"),\n",
    "        train_dataset=\"train\",\n",
    "        test_dataset=\"test\",\n",
    "        bins=5,\n",
    "        alpha=0.1,\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_slicing_reliability\",\n",
    "    lambda: ts.compare_slicing_reliability(\n",
    "        features=\"int_rate\", train_dataset=\"train\", test_dataset=\"test\", bins=5, alpha=0.1\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_slicing_fairness\",\n",
    "    lambda: ts.diagnose_slicing_fairness(\n",
    "        group_config=group_cfg,\n",
    "        features=(\"int_rate\", \"fico_score\"),\n",
    "        dataset=\"test\",\n",
    "        bins=5,\n",
    "        metric=\"AIR\",\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"cmp_slicing_fairness\",\n",
    "    lambda: ts.compare_slicing_fairness(\n",
    "        group_config=group_cfg, features=\"int_rate\", dataset=\"test\", bins=5, metric=\"AIR\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "run_test(\n",
    "    \"diag_mitigate_unfair_thresholding\",\n",
    "    lambda: ts.diagnose_mitigate_unfair_thresholding(\n",
    "        group_config=group_cfg,\n",
    "        dataset=\"test\",\n",
    "        metric=\"AIR\",\n",
    "        performance_metric=\"AUC\",\n",
    "        proba_cutoff=(0.30, 0.35, 0.40, 0.45),\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"diag_mitigate_unfair_binning\",\n",
    "    lambda: ts.diagnose_mitigate_unfair_binning(\n",
    "        group_config=group_cfg,\n",
    "        dataset=\"test\",\n",
    "        metric=\"AIR\",\n",
    "        performance_metric=\"AUC\",\n",
    "        binning_features=(\"int_rate\", \"fico_score\"),\n",
    "        bins=5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### Compare Accuracy\"))\n",
    "display(result_table(\"cmp_accuracy\"))\n",
    "display(Markdown(\"### Compare Reliability\"))\n",
    "display(result_table(\"cmp_reliability\"))\n",
    "display(Markdown(\"### Compare Fairness (AIR)\"))\n",
    "display(result_table(\"cmp_fairness\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 8) Explainability e interpretabilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_test(\"explain_pfi\", lambda: ts.explain_pfi(dataset=\"test\", sample_size=2000, n_repeats=5))\n",
    "run_test(\n",
    "    \"explain_shap\",\n",
    "    lambda: ts.explain_shap(dataset=\"test\", sample_index=25, baseline_sample_size=300),\n",
    ")\n",
    "run_test(\"explain_lime\", lambda: ts.explain_lime(dataset=\"test\", sample_index=25))\n",
    "run_test(\n",
    "    \"explain_pdp\",\n",
    "    lambda: ts.explain_pdp(\n",
    "        features=(\"int_rate\", \"loan_to_income\"), dataset=\"test\", sample_size=2000\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"explain_ale\",\n",
    "    lambda: ts.explain_ale(\n",
    "        features=(\"int_rate\", \"loan_to_income\"), dataset=\"test\", sample_size=2000\n",
    "    ),\n",
    ")\n",
    "run_test(\n",
    "    \"explain_hstat\",\n",
    "    lambda: ts.explain_hstatistic(\n",
    "        features=(\"int_rate\", \"fico_score\", \"loan_to_income\"), dataset=\"test\", sample_size=1500\n",
    "    ),\n",
    ")\n",
    "\n",
    "ts_logreg = modeva.TestSuite(\n",
    "    dataset=ds,\n",
    "    model=mz.get_model(\"MoLogReg\"),\n",
    "    models=model_list,\n",
    "    name=\"lc_modeva_side_task_suite_logreg\",\n",
    ")\n",
    "run_test(\"interpret_coef\", lambda: ts_logreg.interpret_coef())\n",
    "run_test(\"interpret_effects\", lambda: ts_logreg.interpret_effects())\n",
    "run_test(\"interpret_ei\", lambda: ts_logreg.interpret_ei(dataset=\"test\"))\n",
    "run_test(\n",
    "    \"interpret_local_ei\", lambda: ts_logreg.interpret_local_ei(dataset=\"test\", sample_index=25)\n",
    ")\n",
    "run_test(\"interpret_fi\", lambda: ts_logreg.interpret_fi(dataset=\"test\"))\n",
    "run_test(\n",
    "    \"interpret_local_fi\", lambda: ts_logreg.interpret_local_fi(dataset=\"test\", sample_index=25)\n",
    ")\n",
    "run_test(\n",
    "    \"interpret_local_linear_fi\",\n",
    "    lambda: ts_logreg.interpret_local_linear_fi(dataset=\"test\", sample_index=25),\n",
    ")\n",
    "\n",
    "mz_tree = modeva.ModelZoo(dataset=ds, name=\"lc_modeva_side_task_tree\", random_state=RANDOM_STATE)\n",
    "mz_tree.add_model(models.MoDecisionTreeClassifier(max_depth=4), name=\"MoTree\", replace=True)\n",
    "mz_tree.train(\"MoTree\")\n",
    "ts_tree = modeva.TestSuite(\n",
    "    dataset=ds, model=mz_tree.get_model(\"MoTree\"), name=\"lc_modeva_side_task_suite_tree\"\n",
    ")\n",
    "run_test(\"interpret_global_tree\", lambda: ts_tree.interpret_global_tree())\n",
    "run_test(\n",
    "    \"interpret_local_tree\", lambda: ts_tree.interpret_local_tree(dataset=\"test\", sample_index=25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(\"### PFI top\"))\n",
    "pfi_table = result_table(\"explain_pfi\")\n",
    "if pfi_table is not None:\n",
    "    display(pfi_table.sort_values(\"Importance\", ascending=False).head(15))\n",
    "\n",
    "display(Markdown(\"### SHAP local (sample 25) top |effect|\"))\n",
    "shap_table = result_table(\"explain_shap\")\n",
    "if shap_table is not None:\n",
    "    display(\n",
    "        shap_table.assign(abs_effect=shap_table[\"Effect\"].abs())\n",
    "        .sort_values(\"abs_effect\", ascending=False)\n",
    "        .head(15)\n",
    "    )\n",
    "\n",
    "display(Markdown(\"### Coeficientes MoLogReg\"))\n",
    "coef_table = result_table(\"interpret_coef\")\n",
    "if coef_table is not None:\n",
    "    display(coef_table.sort_values(\"Coefficients\", ascending=False).head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 9) Export reporte Modeva + auditoria de ejecucion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_path = REPORT_DIR / \"modeva_side_task_report.html\"\n",
    "run_test(\"export_report\", lambda: ts.export_report(path=str(report_path)))\n",
    "\n",
    "run_df = pd.DataFrame(run_log)\n",
    "print(\"status counts\")\n",
    "display(run_df[\"status\"].value_counts())\n",
    "print(\"report path:\", report_path)\n",
    "run_df.sort_values([\"status\", \"test\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 10) Insights de resultados y aporte potencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance por modelo en test\n",
    "cmp_accuracy = result_table(\"cmp_accuracy\")\n",
    "model_perf_rows = []\n",
    "if cmp_accuracy is not None:\n",
    "    model_names = cmp_accuracy.columns.get_level_values(0).unique()\n",
    "    for model_name in model_names:\n",
    "        model_perf_rows.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"test_auc\": float(cmp_accuracy.loc[\"test\", (model_name, \"AUC\")]),\n",
    "                \"test_brier\": float(cmp_accuracy.loc[\"test\", (model_name, \"Brier\")]),\n",
    "                \"gap_auc_test_minus_train\": float(cmp_accuracy.loc[\"GAP\", (model_name, \"AUC\")]),\n",
    "            }\n",
    "        )\n",
    "model_perf = pd.DataFrame(model_perf_rows).sort_values(\"test_auc\", ascending=False)\n",
    "display(Markdown(\"### Performance comparada (Modeva sample)\"))\n",
    "display(model_perf)\n",
    "\n",
    "# Drift top\n",
    "drift_top = None\n",
    "psi_table = result_table(\"drift_psi\")\n",
    "if psi_table is not None:\n",
    "    drift_top = psi_table.sort_values(\"Distance_Scores\", ascending=False).head(10)\n",
    "display(Markdown(\"### Top drift features (PSI)\"))\n",
    "display(drift_top)\n",
    "\n",
    "\n",
    "# Outlier rates\n",
    "def outlier_rate(result_name):\n",
    "    out = results.get(result_name)\n",
    "    if out is None:\n",
    "        return np.nan\n",
    "    table = getattr(out, \"table\", None)\n",
    "    if not isinstance(table, dict):\n",
    "        return np.nan\n",
    "    n_out = len(table.get(\"outliers\", []))\n",
    "    n_non = len(table.get(\"non-outliers\", []))\n",
    "    return n_out / (n_out + n_non) if (n_out + n_non) > 0 else np.nan\n",
    "\n",
    "\n",
    "outlier_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"method\": [\"IsolationForest\", \"PCA-Mahalanobis\", \"CBLOF\"],\n",
    "        \"outlier_rate\": [\n",
    "            outlier_rate(\"outlier_iforest\"),\n",
    "            outlier_rate(\"outlier_pca\"),\n",
    "            outlier_rate(\"outlier_cblof\"),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "display(Markdown(\"### Outlier rate por metodo\"))\n",
    "display(outlier_summary)\n",
    "\n",
    "# Feature selection consensus\n",
    "fs_corr = result_table(\"fs_corr\")\n",
    "fs_xgb = result_table(\"fs_xgbpfi\")\n",
    "fs_rcit_value = result_value(\"fs_rcit\")\n",
    "fs_corr_sel = (\n",
    "    set(fs_corr.loc[fs_corr[\"Selected\"] == True, \"Name\"].tolist()) if fs_corr is not None else set()\n",
    ")\n",
    "fs_xgb_sel = (\n",
    "    set(fs_xgb.loc[fs_xgb[\"Selected\"] == True, \"Name\"].tolist()) if fs_xgb is not None else set()\n",
    ")\n",
    "fs_rcit_sel = set(fs_rcit_value.get(\"selected\", [])) if isinstance(fs_rcit_value, dict) else set()\n",
    "fs_consensus = sorted((fs_corr_sel & fs_xgb_sel) | fs_rcit_sel)\n",
    "\n",
    "fs_summary = pd.DataFrame(\n",
    "    {\n",
    "        \"selected_by_corr\": [len(fs_corr_sel)],\n",
    "        \"selected_by_xgbpfi\": [len(fs_xgb_sel)],\n",
    "        \"selected_by_rcit\": [len(fs_rcit_sel)],\n",
    "        \"consensus_plus_rcit\": [\", \".join(fs_consensus[:20])],\n",
    "    }\n",
    ")\n",
    "display(Markdown(\"### Feature selection resumen\"))\n",
    "display(fs_summary)\n",
    "\n",
    "# Reliability / fairness / robustness summaries\n",
    "reliability_table = result_table(\"cmp_reliability\")\n",
    "reliability_rows = []\n",
    "if reliability_table is not None:\n",
    "    for model_name in reliability_table.columns.get_level_values(0).unique():\n",
    "        reliability_rows.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"avg_width\": float(reliability_table.loc[0, (model_name, \"Avg.Width\")]),\n",
    "                \"avg_coverage\": float(reliability_table.loc[0, (model_name, \"Avg.Coverage\")]),\n",
    "            }\n",
    "        )\n",
    "reliability_summary = pd.DataFrame(reliability_rows)\n",
    "display(Markdown(\"### Reliability resumen\"))\n",
    "display(reliability_summary)\n",
    "\n",
    "fairness_table = result_table(\"cmp_fairness\")\n",
    "fairness_rows = []\n",
    "if fairness_table is not None:\n",
    "    for model_name in fairness_table.columns.get_level_values(0).unique():\n",
    "        fairness_rows.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"AIR_rent_vs_mortgage\": float(\n",
    "                    fairness_table.loc[\"AIR\", (model_name, \"rent_vs_mortgage\")]\n",
    "                ),\n",
    "            }\n",
    "        )\n",
    "fairness_summary = pd.DataFrame(fairness_rows)\n",
    "display(Markdown(\"### Fairness resumen (AIR)\"))\n",
    "display(fairness_summary)\n",
    "\n",
    "robustness_table = result_table(\"cmp_robustness\")\n",
    "robustness_rows = []\n",
    "if robustness_table is not None:\n",
    "    robust_grouped = robustness_table.groupby(level=[0, 1], axis=1).mean()\n",
    "    noise_levels = sorted(set(robust_grouped.columns.get_level_values(1).tolist()))\n",
    "    max_noise = max(noise_levels)\n",
    "    for model_name in robust_grouped.columns.get_level_values(0).unique():\n",
    "        base = float(robust_grouped[(model_name, 0.0)].mean())\n",
    "        high_noise = float(robust_grouped[(model_name, max_noise)].mean())\n",
    "        robustness_rows.append(\n",
    "            {\n",
    "                \"model\": model_name,\n",
    "                \"metric_at_noise_0.0\": base,\n",
    "                f\"metric_at_noise_{max_noise}\": high_noise,\n",
    "                \"delta_high_noise_minus_base\": high_noise - base,\n",
    "            }\n",
    "        )\n",
    "robustness_summary = pd.DataFrame(robustness_rows)\n",
    "display(Markdown(\"### Robustness resumen\"))\n",
    "display(robustness_summary)\n",
    "\n",
    "# Compare best Modeva AUC vs core PD AUC\n",
    "core_auc = float(pd_record[\"final_test_metrics\"][\"auc_roc\"])\n",
    "best_modeva_auc = float(model_perf[\"test_auc\"].max()) if not model_perf.empty else np.nan\n",
    "auc_gap_vs_core = best_modeva_auc - core_auc if not np.isnan(best_modeva_auc) else np.nan\n",
    "\n",
    "comparison_vs_core = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"core_pd_auc\": core_auc,\n",
    "            \"best_modeva_auc_sample\": best_modeva_auc,\n",
    "            \"modeva_minus_core_auc\": auc_gap_vs_core,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "display(Markdown(\"### Comparacion AUC vs baseline core\"))\n",
    "display(comparison_vs_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = run_df.loc[run_df[\"status\"] == \"error\", [\"test\", \"error\"]].copy()\n",
    "\n",
    "lines = []\n",
    "lines.append(\"## Conclusiones de la side task (Modeva)\")\n",
    "lines.append(\"\")\n",
    "lines.append(f\"- Metodos OK: {int((run_df['status'] == 'ok').sum())}\")\n",
    "lines.append(f\"- Metodos con error controlado: {int((run_df['status'] == 'error').sum())}\")\n",
    "if not model_perf.empty:\n",
    "    best = model_perf.iloc[0]\n",
    "    lines.append(f\"- Mejor AUC en muestra Modeva: {best['model']} = {best['test_auc']:.4f}\")\n",
    "    lines.append(f\"- Brecha vs AUC core ({core_auc:.4f}): {auc_gap_vs_core:+.4f}\")\n",
    "if drift_top is not None and not drift_top.empty:\n",
    "    lines.append(\n",
    "        f\"- Mayor drift PSI: {drift_top.index[0]} ({float(drift_top.iloc[0]['Distance_Scores']):.4f})\"\n",
    "    )\n",
    "if not fairness_summary.empty:\n",
    "    lines.append(\n",
    "        f\"- AIR rent_vs_mortgage (min/max): {fairness_summary['AIR_rent_vs_mortgage'].min():.4f} / {fairness_summary['AIR_rent_vs_mortgage'].max():.4f}\"\n",
    "    )\n",
    "if not failures.empty:\n",
    "    lines.append(\"- Limites observados en modeva:\")\n",
    "    for _, row in failures.iterrows():\n",
    "        lines.append(f\"  - {row['test']}: {row['error']}\")\n",
    "\n",
    "lines.append(\"\")\n",
    "lines.append(\"Aporte potencial al proyecto:\")\n",
    "lines.append(\n",
    "    \"- Modeva aporta una bateria muy amplia de validaciones de gobernanza (drift, slicing, fairness, robustez, reliability, interpretabilidad).\"\n",
    ")\n",
    "lines.append(\n",
    "    \"- En esta muestra no supera el baseline de AUC del modelo canonico; no se recomienda reemplazo del core en este estado.\"\n",
    ")\n",
    "lines.append(\"- Se recomienda uso complementario para auditoria/monitoring del modelo core.\")\n",
    "lines.append(\"\")\n",
    "lines.append(\n",
    "    \"Esta entrega se considera side task y no parte del core, salvo que en futuras pruebas muestre mejoras materiales.\"\n",
    ")\n",
    "\n",
    "display(Markdown(\"\\\\n\".join(lines)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 11) Recomendacion explicita: que llevar al core vs que descartar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "method_decisions = [\n",
    "    {\n",
    "        \"method_group\": \"data_drift_test (PSI/KS)\",\n",
    "        \"status\": \"RECOMENDADO\",\n",
    "        \"decision\": \"Incluir como monitoreo periodico\",\n",
    "        \"justification\": \"Entrega alerta temprana de cambio distribucional en variables de riesgo.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"diagnose_fairness / diagnose_slicing_fairness\",\n",
    "        \"status\": \"RECOMENDADO\",\n",
    "        \"decision\": \"Incluir como control de gobernanza\",\n",
    "        \"justification\": \"Detecta sesgos por subgrupo y soporta discusiones de politica de credito.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"diagnose_robustness / diagnose_slicing_robustness\",\n",
    "        \"status\": \"RECOMENDADO\",\n",
    "        \"decision\": \"Incluir como prueba de estres\",\n",
    "        \"justification\": \"Cuantifica degradacion ante ruido en variables sensibles.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"diagnose_reliability / slicing_reliability\",\n",
    "        \"status\": \"RECOMENDADO\",\n",
    "        \"decision\": \"Incluir como control complementario\",\n",
    "        \"justification\": \"Aporta una vista adicional de estabilidad de prediccion por segmentos.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"diagnose_slicing_accuracy / overfit\",\n",
    "        \"status\": \"RECOMENDADO\",\n",
    "        \"decision\": \"Incluir en reporte de validacion\",\n",
    "        \"justification\": \"Hace visible donde el modelo rinde peor por cortes de negocio.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"detect_outlier_isolation_forest\",\n",
    "        \"status\": \"RECOMENDADO\",\n",
    "        \"decision\": \"Usar como indicador de calidad de entrada\",\n",
    "        \"justification\": \"Ayuda a monitorear estabilidad de poblacion y datos anomales.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"explain_pfi / explain_shap / explain_lime\",\n",
    "        \"status\": \"RECOMENDADO\",\n",
    "        \"decision\": \"Mantener para auditoria y explicabilidad\",\n",
    "        \"justification\": \"Facilita interpretacion de drivers globales y casos individuales.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"ModelZoo para reentrenar modelos alternos\",\n",
    "        \"status\": \"DESCARTAR (por ahora)\",\n",
    "        \"decision\": \"No usar para reemplazar pipeline core\",\n",
    "        \"justification\": \"En este benchmark no supero al modelo canonico en AUC.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"interpret_local_ei (logistic wrapper)\",\n",
    "        \"status\": \"DESCARTAR (bug de libreria)\",\n",
    "        \"decision\": \"No usar hasta correccion upstream\",\n",
    "        \"justification\": \"Falla por atributo faltante (`modeva_intercept_`).\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"transform / inverse_transform con binning\",\n",
    "        \"status\": \"DESCARTAR (inestable)\",\n",
    "        \"decision\": \"Evitar en flujo productivo\",\n",
    "        \"justification\": \"Se observaron errores de shape tras pipeline de binning.\",\n",
    "    },\n",
    "    {\n",
    "        \"method_group\": \"eda_umap / visualizaciones pesadas\",\n",
    "        \"status\": \"DESCARTAR (core)\",\n",
    "        \"decision\": \"Usar solo para analisis exploratorio\",\n",
    "        \"justification\": \"Valor operativo bajo para gate de produccion.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "decision_df = pd.DataFrame(method_decisions)\n",
    "display(decision_df)\n",
    "\n",
    "core_recommended = decision_df[decision_df[\"status\"].str.contains(\"RECOMENDADO\")]\n",
    "core_discarded = decision_df[~decision_df[\"status\"].str.contains(\"RECOMENDADO\")]\n",
    "\n",
    "display(Markdown(\"### Lista corta para el proyecto principal\"))\n",
    "display(core_recommended[[\"method_group\", \"decision\"]])\n",
    "\n",
    "display(Markdown(\"### Lista a descartar/no priorizar en el core\"))\n",
    "display(core_discarded[[\"method_group\", \"decision\"]])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
