"""Anexo: RAPIDS GPU benchmark â€” aceleraciÃ³n GPU para riesgo de crÃ©dito."""

from __future__ import annotations

import sys
from pathlib import Path

_REPO_ROOT = Path(__file__).resolve().parents[2]
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

import numpy as np
import pandas as pd
import plotly.express as px
import streamlit as st

from streamlit_app.components.metric_cards import kpi_row
from streamlit_app.components.narrative import storytelling_intro
from streamlit_app.theme import PLOTLY_TEMPLATE
from streamlit_app.utils import download_table

# ---------------------------------------------------------------------------
# Data loading helpers â€” reads from reports/gpu_benchmark/
# Supports both parquet (new) and CSV (legacy) artifacts.
# ---------------------------------------------------------------------------

GPU_BENCH_DIR = Path(__file__).resolve().parents[2] / "reports" / "gpu_benchmark"

# Consistent color palette
COLORS = {
    "pandas_cpu": "#E45756",
    "pandas_cudf": "#F58518",
    "polars_cpu": "#4C78A8",
    "polars_cudf": "#72B7B2",
    "duckdb": "#54A24B",
    "sklearn_cpu": "#A8B5C4",
    "cuml_gpu": "#0B5ED7",
    "networkx_cpu": "#A8B5C4",
    "cugraph_gpu": "#0B5ED7",
    "networkx_cugraph_backend": "#198754",
    "scipy_highs_cpu": "#A8B5C4",
    "scipy_milp_cpu": "#6F42C1",
    "cuopt_gpu": "#0B5ED7",
    "numpy_cpu": "#A8B5C4",
    "scipy_cpu": "#A8B5C4",
    "cupy_gpu": "#0B5ED7",
}


@st.cache_data(ttl=300)
def _load_bench(name: str) -> pd.DataFrame:
    """Load benchmark artifact (try parquet first, then CSV)."""
    pq = GPU_BENCH_DIR / f"{name}.parquet"
    if pq.exists():
        return pd.read_parquet(pq)
    csv = GPU_BENCH_DIR / f"{name}.csv"
    if csv.exists():
        return pd.read_csv(csv)
    return pd.DataFrame()


@st.cache_data(ttl=300)
def _load_meta() -> dict:
    """Load benchmark metadata JSON."""
    import json

    path = GPU_BENCH_DIR / "gpu_bench_meta.json"
    if path.exists():
        return json.loads(path.read_text())
    return {}


def _safe_float(val: object, default: float = 0.0) -> float:
    """Safely convert to float, returning default for NaN/None."""
    try:
        v = float(val)
        return v if np.isfinite(v) else default
    except (TypeError, ValueError):
        return default


# ---------------------------------------------------------------------------
# Page layout
# ---------------------------------------------------------------------------

st.title("âš¡ Benchmark RAPIDS GPU")
st.caption(
    "EvaluaciÃ³n exhaustiva de aceleraciÃ³n GPU (NVIDIA RAPIDS 26.02) sobre los "
    "datos reales del proyecto Lending Club â€” Side project independiente."
)

storytelling_intro(
    page_goal=(
        "Medir el valor prÃ¡ctico de aceleraciÃ³n GPU (NVIDIA RAPIDS) sobre los datos reales "
        "del proyecto Lending Club (1.86M prÃ©stamos, 110 columnas)."
    ),
    business_value=(
        "Identificar quÃ© componentes del pipeline de riesgo se benefician de GPU "
        "y cuÃ¡les no justifican la complejidad, para decisiones de infraestructura."
    ),
    key_decision=(
        "Decidir quÃ© librerÃ­as usar para cada tarea: procesamiento de datos, "
        "machine learning, grafos, optimizaciÃ³n y cÃ³mputo numÃ©rico."
    ),
    how_to_read=[
        "Cada secciÃ³n compara backends CPU vs GPU para una tarea especÃ­fica.",
        "Los tiempos son medianas de mÃºltiples ejecuciones con warmup.",
        "Las barras muestran tiempos absolutos â€” menor es mejor.",
        "Los quality gates validan que GPU produce resultados correctos.",
    ],
)

# â”€â”€ Load all artifacts â”€â”€
cudf_bench = _load_bench("cudf_polars_benchmark")
cuml_bench = _load_bench("cuml_benchmark")
cugraph_bench = _load_bench("cugraph_benchmark")
cuopt_bench = _load_bench("cuopt_benchmark")
cupy_bench = _load_bench("cupy_benchmark")
scaling = _load_bench("gpu_bench_scaling")
meta = _load_meta()

# â”€â”€ Compute KPIs from available data â”€â”€
has_any_data = not cudf_bench.empty or not cuml_bench.empty

sections_tested = sum(1 for df in [cudf_bench, cuml_bench, cugraph_bench, cuopt_bench, cupy_bench] if not df.empty)

# Best speedup across all sections
all_speedups: list[float] = []
if not cudf_bench.empty and "speedup_vs_pandas_cpu" in cudf_bench.columns:
    ok = cudf_bench[cudf_bench["status"] == "ok"] if "status" in cudf_bench.columns else cudf_bench
    all_speedups.extend(ok["speedup_vs_pandas_cpu"].dropna().tolist())
if not cuml_bench.empty and "fit_speedup_vs_cpu" in cuml_bench.columns:
    all_speedups.extend(cuml_bench["fit_speedup_vs_cpu"].dropna().tolist())

best_speedup = f"{max(all_speedups):.1f}x" if all_speedups else "N/D"

# Total algorithms benchmarked
n_algos = 0
for df, col in [(cudf_bench, "mode"), (cuml_bench, "task"), (cugraph_bench, "task"), (cuopt_bench, "task"), (cupy_bench, "task")]:
    if not df.empty and col in df.columns:
        n_algos += df[col].nunique()

kpi_row(
    [
        {"label": "Mejor speedup", "value": best_speedup},
        {"label": "Secciones", "value": f"{sections_tested}/5"},
        {"label": "MÃ©todos evaluados", "value": str(n_algos)},
        {
            "label": "Hardware",
            "value": meta.get("hardware", {}).get("gpu", "RTX 3080") if meta else "RTX 3080",
        },
    ],
    n_cols=4,
)

if not has_any_data:
    st.warning(
        "No se encontraron artefactos de benchmark en `reports/gpu_benchmark/`. "
        "Ejecuta el notebook RAPIDS para generar los resultados."
    )
    st.stop()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 1: DataFrame Processing â€” pandas vs cuDF vs Polars vs DuckDB
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("1) Procesamiento de DataFrames")

st.markdown(
    """
**Workload**: lectura de parquet (1.35M filas, 110 columnas) â†’ selecciÃ³n de columnas â†’
parsing de strings (int_rate, term) â†’ filtrado multi-condiciÃ³n â†’ groupby multi-clave â†’
join â†’ sort â†’ head(5000). Este pipeline replica operaciones tÃ­picas de
`build_datasets.py` y `prepare_dataset.py` del proyecto.

**Backends evaluados**:
| Backend | DescripciÃ³n | Tipo |
|---------|-------------|------|
| `pandas_cpu` | pandas vanilla (baseline) | CPU |
| `pandas_cudf` | pandas via `python -m cudf.pandas` (zero-code-change) | GPU |
| `polars_cpu` | Polars lazy collect | CPU |
| `polars_cudf` | Polars lazy con `pl.GPUEngine()` | GPU |
| `duckdb` | SQL in-process (motor analÃ­tico columnar) | CPU |
"""
)

if not cudf_bench.empty:
    ok = cudf_bench[cudf_bench["status"] == "ok"].copy() if "status" in cudf_bench.columns else cudf_bench.copy()

    if "mode" in ok.columns and "median_seconds" in ok.columns:
        chart_df = ok[["mode", "median_seconds"]].dropna().copy()
        chart_df = chart_df.sort_values("median_seconds", ascending=False)

        # Compute speedup vs each backend for the comparison table
        backends = chart_df.set_index("mode")["median_seconds"].to_dict()
        pandas_time = backends.get("pandas_cpu", 1.0)

        # â”€â”€ Chart 1: Absolute times (all backends) â”€â”€
        chart_df["color"] = chart_df["mode"].map(COLORS).fillna("#999999")

        fig = px.bar(
            chart_df,
            y="mode",
            x="median_seconds",
            orientation="h",
            labels={"mode": "Backend", "median_seconds": "Tiempo mediano (s)"},
            title="Tiempo de ejecuciÃ³n por backend (menor = mejor)",
            color="mode",
            color_discrete_map=COLORS,
        )
        for _, row in chart_df.iterrows():
            speedup = pandas_time / max(row["median_seconds"], 1e-12)
            fig.add_annotation(
                x=row["median_seconds"],
                y=row["mode"],
                text=f" {row['median_seconds']:.3f}s ({speedup:.1f}x vs pandas)",
                showarrow=False,
                xanchor="left",
                font={"size": 11},
            )
        fig.update_layout(**PLOTLY_TEMPLATE["layout"], height=max(280, len(chart_df) * 55), showlegend=False)
        st.plotly_chart(fig, use_container_width=True)

        # â”€â”€ Cross-comparison matrix â”€â”€
        st.subheader("Matriz de speedup cruzado")
        st.markdown(
            "Cada celda muestra cuÃ¡ntas veces mÃ¡s rÃ¡pido es el backend de la **columna** "
            "respecto al backend de la **fila**. Valores > 1 indican ventaja."
        )
        modes = sorted(backends.keys())
        matrix_rows = []
        for row_mode in modes:
            row_data = {"Backend": row_mode}
            for col_mode in modes:
                if row_mode == col_mode:
                    row_data[col_mode] = "â€”"
                else:
                    ratio = backends[row_mode] / max(backends[col_mode], 1e-12)
                    row_data[col_mode] = f"{ratio:.1f}x"
            matrix_rows.append(row_data)
        matrix_df = pd.DataFrame(matrix_rows).set_index("Backend")
        st.dataframe(matrix_df, use_container_width=True)

        # â”€â”€ Detailed results table â”€â”€
        with st.expander("Datos detallados de benchmark"):
            display_cols = ["mode", "median_seconds", "mean_seconds", "std_seconds"]
            if "speedup_vs_pandas_cpu" in ok.columns:
                display_cols.append("speedup_vs_pandas_cpu")
            if "rows_out" in ok.columns:
                display_cols.append("rows_out")
            if "consistency_pass" in ok.columns:
                display_cols.append("consistency_pass")
            show = [c for c in display_cols if c in ok.columns]
            st.dataframe(ok[show].sort_values("median_seconds"), use_container_width=True, hide_index=True)

        # â”€â”€ Analysis text â”€â”€
        st.markdown(
            """
**AnÃ¡lisis detallado:**

- **Polars GPU** (`pl.GPUEngine()`) lidera gracias a que compila el plan lazy completo
  a operaciones cuDF en GPU, eliminando transferencias intermedias CPUâ†”GPU. El plan
  incluye scan, filter, groupby, join y sort en una sola pasada GPU.

- **Polars CPU** ya es significativamente mÃ¡s rÃ¡pido que pandas porque usa un motor
  lazy columnar con ejecuciÃ³n paralela multi-hilo y predicados pushdown.

- **cuDF pandas accelerator** (`python -m cudf.pandas`) logra ~18x sobre pandas sin
  cambiar una sola lÃ­nea de cÃ³digo. Intercepta llamadas a la API de pandas y las
  despacha a kernels CUDA. El overhead viene de la traducciÃ³n de operaciones.

- **DuckDB** es un motor SQL analÃ­tico columnar in-process. Su ventaja principal es
  el bajo consumo de memoria (spill-to-disk) y la capacidad de procesar datasets mÃ¡s
  grandes que la RAM disponible â€” algo que ni pandas ni cuDF pueden hacer nativamente.
  [Referencia: codecentric.de benchmark](https://www.codecentric.de/en/knowledge-hub/blog/duckdb-vs-polars-performance-and-memory-with-massive-parquet-data)
  encontrÃ³ que DuckDB usa solo 1.3 GB de RAM para 140 GB de datos, vs 17 GB de Polars.

- **pandas CPU** es el baseline mÃ¡s lento porque opera fila-por-fila en muchas
  operaciones y no paraleliza automÃ¡ticamente.

**Operaciones representadas del pipeline real** (`build_datasets.py`):
- Parsing de `int_rate` (string con '%' â†’ float)
- Parsing de `term` (regex extract â†’ int)
- Filtrado multi-condiciÃ³n (loan_amnt, annual_inc, term)
- GroupBy multi-clave con agregaciones (count, sum, mean)
- Joins (left merge en grade y purpose)
"""
        )

        download_table(chart_df, "gpu_bench_dataframe_comparison.csv", "Descargar resultados")
else:
    st.info("No hay datos de benchmark cuDF/Polars/DuckDB disponibles.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 2: Machine Learning â€” scikit-learn CPU vs cuML GPU
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("2) Machine Learning (scikit-learn vs cuML)")

st.markdown(
    """
**Dataset**: `train_fe.parquet` (250K sample) con 27 features numÃ©ricos del pipeline de
feature engineering. Incluye ratios financieros (`loan_to_income`, `rev_utilization`),
scores crediticios (`fico_score`), e interacciones (`fico_x_dti`).

**Protocolo**: Cada algoritmo se entrena y predice mÃºltiples veces con warmup.
Se reporta el **fit time** mediano y la **mÃ©trica de calidad** para validar que la
aceleraciÃ³n GPU no degrada los resultados.
"""
)

if not cuml_bench.empty and "task" in cuml_bench.columns:
    tasks = sorted(cuml_bench["task"].unique())

    # â”€â”€ Chart per algorithm (solves the scale problem) â”€â”€
    for task_name in tasks:
        task_data = cuml_bench[cuml_bench["task"] == task_name].copy()
        if task_data.empty:
            continue

        col1, col2 = st.columns([2, 1])

        with col1:
            fig = px.bar(
                task_data,
                x="backend",
                y="fit_seconds",
                color="backend",
                title=f"{task_name.replace('_', ' ').title()}: Tiempo de entrenamiento",
                labels={"backend": "Backend", "fit_seconds": "Fit time (s)"},
                color_discrete_map=COLORS,
                text_auto=".3f",
            )
            fig.update_layout(**PLOTLY_TEMPLATE["layout"], height=320, showlegend=False)
            fig.update_traces(textposition="outside")
            st.plotly_chart(fig, use_container_width=True)

        with col2:
            for _, r in task_data.iterrows():
                backend = str(r.get("backend", ""))
                fit_s = _safe_float(r.get("fit_seconds"))
                metric = str(r.get("metric", ""))
                metric_val = _safe_float(r.get("metric_value"))
                speedup = _safe_float(r.get("fit_speedup_vs_cpu"))

                if "gpu" in backend:
                    if speedup > 1.0:
                        st.metric("Speedup GPU", f"{speedup:.1f}x", f"{(speedup - 1) * 100:.0f}% mÃ¡s rÃ¡pido")
                    elif speedup > 0:
                        st.metric("Speedup GPU", f"{speedup:.2f}x", f"{(1 - speedup) * 100:.0f}% mÃ¡s lento", delta_color="inverse")
                    st.caption(f"**{metric}**: CPU={_safe_float(cuml_bench[(cuml_bench['task'] == task_name) & (cuml_bench['backend'] == 'sklearn_cpu')]['metric_value'].values[0]) if len(cuml_bench[(cuml_bench['task'] == task_name) & (cuml_bench['backend'] == 'sklearn_cpu')]) else 'N/D':.4f} vs GPU={metric_val:.4f}")

    # â”€â”€ Analysis per algorithm â”€â”€
    st.markdown(
        """
**AnÃ¡lisis por algoritmo:**

- **Random Forest**: El mayor beneficio de GPU. La construcciÃ³n de Ã¡rboles en
  paralelo escala linealmente con CUDA cores. cuML paraleliza la evaluaciÃ³n de
  splits y la construcciÃ³n simultÃ¡nea de todos los Ã¡rboles (250 estimators).

- **KMeans**: Speedup moderado. Las iteraciones de Lloyd's algorithm (asignaciÃ³n
  de centroides + recÃ¡lculo) se paralelizan bien en GPU. El cuello de botella es
  el cÃ³mputo de distancias `O(nÃ—kÃ—d)` â€” ideal para GPU.

- **PCA**: La descomposiciÃ³n SVD subyacente es una operaciÃ³n de Ã¡lgebra lineal
  densa que aprovecha las unidades tensoriales de GPU. `cuML.PCA` usa
  `cuSOLVER` internamente.

- **KNN**: El cÃ³mputo de distancias `O(nÃ—mÃ—d)` para k-nearest neighbors se
  paraleliza masivamente en GPU. cuML usa `FAISS`-like indices internos.

- **Logistic Regression**: Puede ser **mÃ¡s lento en GPU** para datasets pequeÃ±os.
  El algoritmo LBFGS requiere operaciones secuenciales (line search) y el
  overhead de transferencia CPUâ†”GPU domina cuando el cÃ³mputo por iteraciÃ³n
  es rÃ¡pido. Este es un caso donde GPU **no** es la mejor opciÃ³n.

**LecciÃ³n clave**: No todo se beneficia de GPU. Algoritmos con iteraciones
secuenciales o datasets pequeÃ±os (< 100K filas) pueden ser mÃ¡s lentos en GPU
por el overhead de transferencia y lanzamiento de kernels CUDA.
"""
    )

    # â”€â”€ Quality parity table â”€â”€
    quality = _load_bench("cuml_quality_checks")
    if quality.empty:
        quality = _load_bench("benchmark_quality_checks_all_sections")
        if not quality.empty and "section" in quality.columns:
            quality = quality[quality["section"] == "cuml"]

    if not quality.empty:
        with st.expander("Paridad de mÃ©tricas CPU vs GPU (quality gates)"):
            st.dataframe(quality, use_container_width=True, hide_index=True)
            st.markdown(
                """
Los **quality gates** validan que la aceleraciÃ³n GPU no degrada la calidad del modelo.
Para cada algoritmo se define una tolerancia mÃ¡xima de diferencia:
- ClasificaciÃ³n (AUC): tolerancia 0.010â€“0.025
- Clustering (silhouette): tolerancia 0.040
- PCA (explained variance): tolerancia 0.050

Si `quality_pass = True`, la implementaciÃ³n GPU es funcionalmente equivalente a CPU.
"""
            )

    download_table(cuml_bench, "gpu_bench_ml_comparison.csv", "Descargar resultados ML")
else:
    st.info("No hay datos de benchmark cuML disponibles.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 3: Graph Analytics â€” NetworkX vs cuGraph
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("3) AnÃ¡lisis de Grafos (NetworkX vs cuGraph)")

st.markdown(
    """
**Grafo**: Bipartito loanâ†’atributo. Cada prÃ©stamo se conecta a nodos de
`grade`, `purpose`, `sub_grade` y `verification_status`. Esto produce un grafo
denso que modela relaciones implÃ­citas entre prÃ©stamos.

**AplicaciÃ³n en riesgo de crÃ©dito**: Detectar comunidades de alto riesgo,
nodos centrales (prÃ©stamos que comparten muchos atributos de riesgo), y
concentraciÃ³n de exposiciÃ³n por subgrafo.

**Backends**:
- `networkx_cpu`: NetworkX puro (Python, single-threaded)
- `cugraph_gpu`: cuGraph nativo (CUDA, paralelo)
- `networkx_cugraph_backend`: NetworkX con dispatch automÃ¡tico a cuGraph via `backend="cugraph"`
"""
)

if not cugraph_bench.empty and "task" in cugraph_bench.columns:
    tasks = sorted(cugraph_bench["task"].unique())

    # Check if we actually have GPU data
    has_gpu_data = any(
        "gpu" in str(b) or "cugraph" in str(b)
        for b in cugraph_bench["backend"].unique()
        if b != "networkx_cpu"
    )

    # â”€â”€ Chart: grouped bar per task â”€â”€
    fig = px.bar(
        cugraph_bench,
        x="task",
        y="seconds",
        color="backend",
        barmode="group",
        title="Tiempo por tarea de grafo y backend",
        labels={"task": "Tarea", "seconds": "Tiempo mediano (s)", "backend": "Backend"},
        color_discrete_map=COLORS,
        text_auto=".3f",
    )
    fig.update_layout(**PLOTLY_TEMPLATE["layout"], height=420)
    fig.update_traces(textposition="outside")
    st.plotly_chart(fig, use_container_width=True)

    # â”€â”€ Speedup table â”€â”€
    if "speedup_vs_cpu" in cugraph_bench.columns:
        gpu_rows = cugraph_bench[cugraph_bench["backend"] != "networkx_cpu"].copy()
        if not gpu_rows.empty:
            st.subheader("Speedup por tarea")
            for _, r in gpu_rows.iterrows():
                speedup = _safe_float(r.get("speedup_vs_cpu"))
                task = str(r.get("task", ""))
                backend = str(r.get("backend", ""))
                if speedup > 0:
                    icon = "ğŸŸ¢" if speedup > 1.0 else "ğŸ”´"
                    st.caption(f"{icon} **{task}** ({backend}): {speedup:.1f}x speedup vs NetworkX CPU")

    if not has_gpu_data:
        st.warning(
            "Solo hay datos de CPU disponibles. Los benchmarks GPU de cuGraph "
            "requieren un entorno RAPIDS con cuGraph instalado. Ejecuta el notebook "
            "en un entorno RAPIDS para ver las comparaciones completas."
        )

    with st.expander("Datos detallados de benchmark de grafos"):
        st.dataframe(cugraph_bench, use_container_width=True, hide_index=True)

    st.markdown(
        """
**Tareas evaluadas:**

- **graph_build**: ConstrucciÃ³n del grafo desde edgelist. En CPU esto es O(E) con
  overhead de Python dict. cuGraph usa CSR/CSC nativo en GPU memory.

- **connected_components**: BFS/Union-Find para encontrar componentes conexas.
  Altamente paralelizable â€” cada nodo puede explorarse independientemente.

- **pagerank**: Iteraciones de power method (multiplicaciÃ³n matriz-vector repetida).
  Natural para GPU: cada iteraciÃ³n es una SpMV (sparse matrix-vector multiply).
  Nota: verificar `sum_pagerank â‰ˆ 1.0` y `converged_rate = 1.0`.

- **louvain**: DetecciÃ³n de comunidades por optimizaciÃ³n de modularidad.
  cuGraph implementa el algoritmo de Louvain con coarsening multi-nivel en GPU.

**nx-cugraph dispatch**: Permite usar la API de NetworkX (`nx.pagerank(G, backend="cugraph")`)
con aceleraciÃ³n transparente. El grafo se transfiere a GPU automÃ¡ticamente.
"""
    )
else:
    st.info("No hay datos de benchmark cuGraph disponibles.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 4: Optimization â€” HiGHS vs cuOpt
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("4) OptimizaciÃ³n de Portafolio (HiGHS vs cuOpt)")

st.markdown(
    """
**Problema**: LP de selecciÃ³n de portafolio con restricciones de presupuesto
(20% del total), riesgo (PD-weighted â‰¤ 10% del presupuesto), y concentraciÃ³n
por `purpose` (â‰¤ 35% por categorÃ­a). Este es el mismo tipo de problema que
resuelve `scripts/optimize_portfolio.py` con Pyomo+HiGHS.

**Backends**:
- `scipy_highs_cpu`: SciPy `linprog(method="highs")` â€” solver LP open-source
- `scipy_milp_cpu`: SciPy `milp()` â€” solver MILP (referencia)
- `cuopt_gpu`: NVIDIA cuOpt `DataModel` + `Solve` API
"""
)

if not cuopt_bench.empty and "task" in cuopt_bench.columns:
    # Separate LP results from smoke tests/errors
    lp_data = cuopt_bench[cuopt_bench["task"] == "portfolio_lp"].copy()
    milp_data = cuopt_bench[cuopt_bench["task"] == "portfolio_milp_reference"].copy()
    other_data = cuopt_bench[~cuopt_bench["task"].isin(["portfolio_lp", "portfolio_milp_reference"])].copy()

    # â”€â”€ LP comparison chart â”€â”€
    if not lp_data.empty:
        # Only show backends with valid times
        valid_lp = lp_data[lp_data["seconds"].notna() & (lp_data["seconds"] > 0)].copy()

        if not valid_lp.empty:
            fig = px.bar(
                valid_lp,
                x="backend",
                y="seconds",
                color="backend",
                title=f"Portfolio LP: Tiempo de resoluciÃ³n ({int(valid_lp['n_variables'].iloc[0]):,} variables)",
                labels={"backend": "Solver", "seconds": "Tiempo mediano (s)"},
                color_discrete_map=COLORS,
                text_auto=".4f",
            )
            fig.update_layout(**PLOTLY_TEMPLATE["layout"], height=350, showlegend=False)
            fig.update_traces(textposition="outside")
            st.plotly_chart(fig, use_container_width=True)

        # Show LP results with objective comparison
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**Resultados LP:**")
            for _, r in lp_data.iterrows():
                backend = str(r.get("backend", ""))
                seconds = _safe_float(r.get("seconds"))
                status = str(r.get("status", ""))
                objective = _safe_float(r.get("objective"))
                n_vars = int(_safe_float(r.get("n_variables")))

                if "error" in status.lower() or seconds == 0:
                    st.error(f"**{backend}**: Error â€” {status}")
                else:
                    st.success(f"**{backend}**: {seconds:.4f}s, objetivo={objective:,.0f}, {n_vars:,} variables")

        with col2:
            if not milp_data.empty:
                st.markdown("**Referencia MILP:**")
                for _, r in milp_data.iterrows():
                    seconds = _safe_float(r.get("seconds"))
                    objective = _safe_float(r.get("objective"))
                    n_vars = int(_safe_float(r.get("n_variables")))
                    st.info(f"MILP ({n_vars:,} vars): {seconds:.4f}s, objetivo={objective:,.0f}")

    # â”€â”€ Error details â”€â”€
    error_rows = cuopt_bench[cuopt_bench["status"].astype(str).str.contains("error", case=False, na=False)]
    if not error_rows.empty:
        with st.expander("Errores detectados en benchmarks de optimizaciÃ³n"):
            for _, r in error_rows.iterrows():
                st.warning(f"**{r.get('task', '')}** ({r.get('backend', '')}): {r.get('status', '')}")
            st.markdown(
                """
**Nota sobre cuOpt**: La API de cuOpt ha cambiado significativamente entre versiones.
El error `'Problem' object has no attribute 'set_objective_data'` indica que la versiÃ³n
instalada no soporta la API legacy `Problem`. El notebook usa la API actual
`DataModel` + `Solve` que es la correcta para RAPIDS 26.02.

Si ves errores, verifica:
1. Que cuOpt estÃ¡ instalado: `pip install cuopt`
2. Que la versiÃ³n es compatible con RAPIDS 26.02
3. Que la GPU tiene suficiente VRAM para el problema
"""
            )

    with st.expander("Datos completos de optimizaciÃ³n"):
        st.dataframe(cuopt_bench, use_container_width=True, hide_index=True)

    st.markdown(
        """
**Contexto del proyecto**: El pipeline de optimizaciÃ³n de portafolio usa
**Pyomo + HiGHS** como solver principal (ver `src/optimization/portfolio_model.py`).
cuOpt es una alternativa GPU que puede acelerar problemas grandes (>10K variables).

**ConexiÃ³n con la tesis**: Los **uncertainty sets conformales** (PD_low, PD_high)
del predictor conformal Mondrian se usan como restricciones de incertidumbre en
la optimizaciÃ³n robusta. Un solver mÃ¡s rÃ¡pido permite evaluar mÃ¡s escenarios
de robustez en menos tiempo.

**Scaling behavior**: LP solvers GPU son mÃ¡s eficientes a escala:
- < 5K variables: CPU (HiGHS) generalmente gana por menor overhead
- 5Kâ€“20K variables: GPU empieza a ser competitivo
- &gt; 20K variables: GPU tiene ventaja significativa (paralelismo en simplex/IPM)
"""
    )
else:
    st.info("No hay datos de benchmark cuOpt disponibles.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 5: Numerical Computing â€” NumPy/SciPy vs CuPy
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("5) CÃ³mputo NumÃ©rico (NumPy/SciPy vs CuPy)")

st.markdown(
    """
**CuPy** es un reemplazo drop-in de NumPy/SciPy que ejecuta operaciones en GPU.
Los arrays CuPy viven en VRAM y las operaciones usan cuBLAS, cuSOLVER, cuSPARSE.

**Tareas evaluadas:**
- **Monte Carlo ECL**: SimulaciÃ³n de 100K escenarios Ã— 10K prÃ©stamos
  (PD Ã— LGD Ã— EAD) â€” operaciÃ³n central en provisioning IFRS9
- **SVD**: DescomposiciÃ³n en valores singulares de la matriz de features
  (100K Ã— 27) â€” usado en PCA y reducciÃ³n dimensional
- **Sparse MatMul**: MultiplicaciÃ³n de matrices dispersas CSR (50K Ã— 50K,
  densidad 0.1%) â€” relevante para grafos y regularizaciÃ³n
"""
)

if not cupy_bench.empty and "task" in cupy_bench.columns:
    tasks = sorted(cupy_bench["task"].unique())

    for task_name in tasks:
        task_data = cupy_bench[cupy_bench["task"] == task_name].copy()
        if task_data.empty or len(task_data) < 1:
            continue

        col1, col2 = st.columns([2, 1])

        with col1:
            valid_data = task_data[task_data["seconds"].notna() & (task_data["seconds"] > 0)]
            if not valid_data.empty:
                fig = px.bar(
                    valid_data,
                    x="backend",
                    y="seconds",
                    color="backend",
                    title=f"{task_name.replace('_', ' ').title()}: CPU vs GPU",
                    labels={"backend": "Backend", "seconds": "Tiempo mediano (s)"},
                    color_discrete_map=COLORS,
                    text_auto=".4f",
                )
                fig.update_layout(**PLOTLY_TEMPLATE["layout"], height=320, showlegend=False)
                fig.update_traces(textposition="outside")
                st.plotly_chart(fig, use_container_width=True)

        with col2:
            cpu_row = task_data[task_data["backend"].str.contains("cpu", case=False, na=False)]
            gpu_row = task_data[task_data["backend"].str.contains("gpu", case=False, na=False)]
            if not cpu_row.empty and not gpu_row.empty:
                cpu_s = _safe_float(cpu_row.iloc[0]["seconds"])
                gpu_s = _safe_float(gpu_row.iloc[0]["seconds"])
                if gpu_s > 0 and cpu_s > 0:
                    speedup = cpu_s / gpu_s
                    if speedup > 1.0:
                        st.metric("Speedup GPU", f"{speedup:.1f}x", f"{(speedup - 1) * 100:.0f}% mÃ¡s rÃ¡pido")
                    else:
                        st.metric("Speedup GPU", f"{speedup:.2f}x", f"{(1 - speedup) * 100:.0f}% mÃ¡s lento", delta_color="inverse")
            elif not cpu_row.empty:
                st.info("Solo CPU disponible")

    st.markdown(
        """
**Por quÃ© Monte Carlo ECL es ideal para GPU:**
La simulaciÃ³n `ECL = PD Ã— LGD Ã— EAD` es embarrassingly parallel: cada escenario
es independiente. Con 100K escenarios Ã— 10K prÃ©stamos = 1 billÃ³n de multiplicaciones
flotantes â€” esto es exactamente lo que las miles de CUDA cores de una GPU hacen bien.

**ConexiÃ³n con IFRS9**: El cÃ¡lculo de Expected Credit Loss bajo mÃºltiples escenarios
macroeconÃ³micos (base, stress, severe) es una operaciÃ³n Monte Carlo. Acelerar esto
permite evaluar mÃ¡s escenarios y obtener distribuciones de ECL mÃ¡s robustas.

**SVD y reducciÃ³n dimensional**: La descomposiciÃ³n SVD es una operaciÃ³n de Ã¡lgebra
lineal densa (LAPACK â†’ cuSOLVER) que escala como O(min(m,n)Â² Ã— max(m,n)). Para
matrices grandes, GPU tiene ventaja significativa.
"""
    )

    with st.expander("Datos detallados de benchmark CuPy"):
        st.dataframe(cupy_bench, use_container_width=True, hide_index=True)
else:
    st.info(
        "No hay datos de benchmark CuPy disponibles. "
        "Ejecuta el notebook RAPIDS para generar benchmarks de Monte Carlo ECL, SVD y sparse matmul."
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 6: Scaling Analysis
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("6) Curva de Escalamiento: GPU vs TamaÃ±o del Dataset")

st.markdown(
    """
Un factor clave en la decisiÃ³n CPU vs GPU es el **tamaÃ±o del dataset**.
Para datos pequeÃ±os, el overhead de transferencia CPUâ†’GPU y lanzamiento de
kernels CUDA puede superar el beneficio del paralelismo. El **punto de cruce**
indica el tamaÃ±o mÃ­nimo donde GPU empieza a ser ventajoso.
"""
)

if not scaling.empty:
    x_col = "pct_data" if "pct_data" in scaling.columns else scaling.columns[0]
    y_col = "speedup_x" if "speedup_x" in scaling.columns else scaling.columns[-1]
    color_col = "method" if "method" in scaling.columns else None

    fig = px.line(
        scaling,
        x=x_col,
        y=y_col,
        color=color_col,
        markers=True,
        title="Speedup GPU vs % del dataset",
        labels={x_col: "FracciÃ³n del dataset", y_col: "Speedup (x)"},
    )
    fig.add_hline(y=1.0, line_dash="dash", line_color="#5F6B7A", annotation_text="1x = paridad CPU/GPU")
    fig.update_layout(**PLOTLY_TEMPLATE["layout"], height=420)
    st.plotly_chart(fig, use_container_width=True)

    # Show the data
    with st.expander("Datos de escalamiento"):
        st.dataframe(scaling, use_container_width=True, hide_index=True)

    st.markdown(
        """
**InterpretaciÃ³n:**
- El overhead fijo de GPU (transferencia de datos, compilaciÃ³n de kernels) es
  constante ~10-50ms independiente del tamaÃ±o.
- Con datos pequeÃ±os (< 10K filas), este overhead domina â†’ GPU es mÃ¡s lento.
- Con datos grandes (> 100K filas), el paralelismo masivo (miles de CUDA cores
  ejecutando simultÃ¡neamente) amortiza el overhead.
- El **punto de cruce** varÃ­a por operaciÃ³n: operaciones element-wise (cuDF)
  cruzan antes que operaciones con dependencias (cuML iterativo).
"""
    )
else:
    st.info(
        "No hay datos de escalamiento disponibles. "
        "Ejecuta el notebook RAPIDS con la secciÃ³n de scaling analysis para ver "
        "cÃ³mo varÃ­a el speedup con el tamaÃ±o del dataset."
    )

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 7: Resumen Comparativo por LibrerÃ­a
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("7) Resumen Comparativo por LibrerÃ­a")

st.markdown(
    """
Cada librerÃ­a RAPIDS tiene un perfil diferente de rendimiento. No existe una
soluciÃ³n universal â€” la elecciÃ³n depende de la tarea, el tamaÃ±o del dataset,
y las restricciones de infraestructura.
"""
)

# Build per-library summary cards
summaries: list[dict] = []

# cuDF
if not cudf_bench.empty and "speedup_vs_pandas_cpu" in cudf_bench.columns:
    ok = cudf_bench[cudf_bench["status"] == "ok"] if "status" in cudf_bench.columns else cudf_bench
    speeds = ok["speedup_vs_pandas_cpu"].dropna()
    if len(speeds):
        summaries.append({
            "LibrerÃ­a": "cuDF / Polars GPU",
            "Mejor speedup": f"{speeds.max():.1f}x",
            "Mediana speedup": f"{speeds.median():.1f}x",
            "Caso de uso": "ETL, feature engineering, data wrangling",
            "Fortaleza": "Zero-code-change acceleration",
            "Debilidad": "Requiere datos en VRAM (10 GB limit en RTX 3080)",
            "RecomendaciÃ³n": "Usar para datasets > 100K filas con operaciones tabulares",
        })

# cuML
if not cuml_bench.empty and "fit_speedup_vs_cpu" in cuml_bench.columns:
    gpu = cuml_bench[cuml_bench["backend"] == "cuml_gpu"]
    speeds = gpu["fit_speedup_vs_cpu"].dropna()
    if len(speeds):
        best_task = gpu.loc[gpu["fit_speedup_vs_cpu"].idxmax(), "task"] if len(gpu) else "N/D"
        worst_task = gpu.loc[gpu["fit_speedup_vs_cpu"].idxmin(), "task"] if len(gpu) else "N/D"
        summaries.append({
            "LibrerÃ­a": "cuML",
            "Mejor speedup": f"{speeds.max():.1f}x ({best_task})",
            "Mediana speedup": f"{speeds.median():.1f}x",
            "Caso de uso": "Entrenamiento e inferencia ML",
            "Fortaleza": "Algoritmos paralelos (RF, KMeans, PCA)",
            "Debilidad": f"Overhead en algoritmos iterativos ({worst_task})",
            "RecomendaciÃ³n": "Usar para RF, KMeans, KNN; evaluar caso por caso para LR",
        })

# cuGraph
if not cugraph_bench.empty and "speedup_vs_cpu" in cugraph_bench.columns:
    speeds = cugraph_bench["speedup_vs_cpu"].dropna()
    if len(speeds):
        summaries.append({
            "LibrerÃ­a": "cuGraph",
            "Mejor speedup": f"{speeds.max():.1f}x",
            "Mediana speedup": f"{speeds.median():.1f}x",
            "Caso de uso": "AnÃ¡lisis de grafos de crÃ©dito",
            "Fortaleza": "PageRank, Louvain, componentes conexas",
            "Debilidad": "Overhead de transferencia para grafos pequeÃ±os",
            "RecomendaciÃ³n": "Usar para grafos con > 100K aristas",
        })

# cuOpt
if not cuopt_bench.empty:
    valid = cuopt_bench[cuopt_bench["seconds"].notna() & (cuopt_bench["seconds"] > 0)]
    if not valid.empty:
        summaries.append({
            "LibrerÃ­a": "cuOpt",
            "Mejor speedup": "Depende de escala",
            "Mediana speedup": "Depende de escala",
            "Caso de uso": "OptimizaciÃ³n LP/MILP de portafolio",
            "Fortaleza": "Problemas grandes (>10K variables)",
            "Debilidad": "API en evoluciÃ³n, overhead para problemas pequeÃ±os",
            "RecomendaciÃ³n": "Usar para problemas de optimizaciÃ³n a escala",
        })

# CuPy
if not cupy_bench.empty and "speedup_vs_cpu" in cupy_bench.columns:
    speeds = cupy_bench["speedup_vs_cpu"].dropna()
    if len(speeds):
        summaries.append({
            "LibrerÃ­a": "CuPy",
            "Mejor speedup": f"{speeds.max():.1f}x",
            "Mediana speedup": f"{speeds.median():.1f}x",
            "Caso de uso": "Monte Carlo, SVD, Ã¡lgebra lineal",
            "Fortaleza": "Drop-in NumPy/SciPy, embarrassingly parallel ops",
            "Debilidad": "Requiere refactoring para operaciones no element-wise",
            "RecomendaciÃ³n": "Usar para simulaciones Monte Carlo y SVD",
        })

if summaries:
    summary_df = pd.DataFrame(summaries)
    st.dataframe(summary_df, use_container_width=True, hide_index=True)

    # â”€â”€ Recommendation matrix â”€â”€
    st.subheader("Matriz de decisiÃ³n: Â¿CuÃ¡ndo usar GPU?")
    st.markdown(
        """
| Escenario | RecomendaciÃ³n | LibrerÃ­a |
|-----------|--------------|----------|
| ETL/Feature engineering > 100K filas | **GPU** | Polars GPU o cudf.pandas |
| ETL/Feature engineering < 100K filas | **CPU** | Polars CPU o DuckDB |
| Datasets mÃ¡s grandes que RAM | **CPU** | DuckDB (spill-to-disk) |
| Random Forest / KMeans / KNN training | **GPU** | cuML |
| Logistic Regression / SVM training | **CPU** | scikit-learn |
| Monte Carlo simulation (> 10K escenarios) | **GPU** | CuPy |
| Ãlgebra lineal densa (SVD, eigenvalues) | **GPU** | CuPy |
| Grafos grandes (> 100K aristas) | **GPU** | cuGraph |
| Grafos pequeÃ±os | **CPU** | NetworkX |
| LP/MILP > 10K variables | **GPU** | cuOpt |
| LP/MILP < 10K variables | **CPU** | HiGHS (Pyomo) |
| Portabilidad de cÃ³digo | **CPU** | Narwhals (write-once API) |
"""
    )
else:
    st.info("No hay datos suficientes para generar el resumen comparativo.")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Section 8: Hardware y MetodologÃ­a
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

st.header("8) Hardware y MetodologÃ­a")

col_hw, col_method = st.columns(2)

with col_hw:
    st.subheader("Especificaciones")
    if meta and "hardware" in meta:
        hw = meta["hardware"]
        st.markdown(
            f"""
| Componente | EspecificaciÃ³n |
|------------|---------------|
| **GPU** | {hw.get('gpu', 'NVIDIA GeForce RTX 3080')} |
| **VRAM** | {hw.get('vram', '10 GB GDDR6X')} |
| **CPU** | {hw.get('cpu', 'AMD Ryzen 5 5600X')} |
| **RAM** | {hw.get('ram', '24 GB DDR4')} |
| **OS** | {hw.get('os', 'WSL2 / Linux')} |
| **CUDA** | Driver 591.86, CUDA 13.1 |
"""
        )
    else:
        st.markdown(
            """
| Componente | EspecificaciÃ³n |
|------------|---------------|
| **GPU** | NVIDIA GeForce RTX 3080 |
| **VRAM** | 10 GB GDDR6X |
| **CPU** | AMD Ryzen 5 5600X (6-core, 12-thread) |
| **RAM** | 24 GB DDR4 |
| **OS** | WSL2 / Linux |
| **CUDA** | Driver 591.86, CUDA 13.1 |
"""
        )

with col_method:
    st.subheader("Protocolo")
    st.markdown(
        """
**MediciÃ³n:**
- Cada benchmark ejecuta mÃºltiples repeticiones con warmup
- Se reporta la **mediana** (robusta a outliers) con IQR
- SincronizaciÃ³n GPU explÃ­cita (`cp.cuda.Stream.null.synchronize()`)

**RMM (RAPIDS Memory Manager):**
- Pool allocator con 6 GB iniciales
- RTX 3080 safe (10 GB VRAM total, 4 GB para OS/driver)
- CuPy integrado via `rmm_cupy_allocator`

**Quality gates:**
- Row-count parity (cuDF)
- Checksum relative error â‰¤ 0.5% (cuDF)
- Metric tolerance per algorithm (cuML)
- Convergence verification (cuGraph PageRank)
- Objective parity (cuOpt LP)
"""
    )

with st.expander("Versiones de librerÃ­as"):
    if meta and "library_versions" in meta:
        versions = meta["library_versions"]
        ver_df = pd.DataFrame(
            [{"LibrerÃ­a": k, "VersiÃ³n": v} for k, v in sorted(versions.items())]
        )
        st.dataframe(ver_df, use_container_width=True, hide_index=True)
    else:
        st.markdown(
            """
Ejecuta el notebook RAPIDS para registrar las versiones exactas en `gpu_bench_meta.json`.

**Versiones objetivo (RAPIDS 26.02):**
- cuDF, cuML, cuGraph, cuOpt: 26.02
- CuPy: 13.x
- Polars: 1.x (con GPUEngine)
- DuckDB: 1.x
- Narwhals: 1.x
"""
        )

with st.expander("Sobre DuckDB vs Polars (referencia externa)"):
    st.markdown(
        """
SegÃºn el [benchmark de codecentric.de](https://www.codecentric.de/en/knowledge-hub/blog/duckdb-vs-polars-performance-and-memory-with-massive-parquet-data)
con datos masivos (2 GB â†’ 2 TB):

| ConfiguraciÃ³n | Memoria (140 GB dataset) | Velocidad | Mejor para |
|---------------|-------------------------|-----------|-----------|
| **DuckDB** | 1.3 GB | MÃ¡s rÃ¡pido | Entornos con restricciÃ³n de memoria |
| **Polars (default)** | 17 GB | Competitivo | Datos particionados con RAM disponible |
| **Polars (async)** | 750 MB | MÃ¡s lento | Archivos grandes no particionados |

**Hallazgo clave**: Particionar los datos reduce dramÃ¡ticamente el uso de memoria
para ambos motores (8x DuckDB, 4x Polars). DuckDB mantiene un consumo de memoria
constante independiente del tamaÃ±o del dataset gracias a su motor streaming.

En nuestro caso (190 MB parquet), ambos motores operan cÃ³modamente en RAM, por lo que
la diferencia principal es la velocidad de ejecuciÃ³n de la query analÃ­tica.
"""
    )

# â”€â”€ Footer â”€â”€
st.markdown("---")
st.caption(
    "Este benchmark es un **side project independiente** del pipeline principal de tesis. "
    "Los resultados provienen de `reports/gpu_benchmark/` y se generan ejecutando "
    "`notebooks/side_projects/10_rapids_gpu_benchmark_lending_club.ipynb` en un entorno RAPIDS."
)
